module PARALLEL

#ifdef MPI
!  Note the capital USE, below which prevents the 
!  preprocessor from making the module a dependent

!  MPI datatypes  
   USE mpi, only: MPI_CHARACTER
   USE mpi, only: MPI_LOGICAL
   USE mpi, only: MPI_INTEGER
   USE mpi, only: MPI_ADDRESS_KIND
   USE mpi, only: MPI_DOUBLE_PRECISION
   USE mpi, only: MPI_2DOUBLE_PRECISION
   USE mpi, only: MPI_2INTEGER
  


!  MPI communicator routines  
   USE mpi, only: MPI_COMM_WORLD
   USE mpi, only: MPI_COMM_DUP
   USE mpi, only: MPI_COMM_SIZE
   USE mpi, only: MPI_COMM_RANK

!  MPI gather functions   
   USE mpi, only: MPI_MAX
   USE mpi, only: MPI_MIN
   USE mpi, only: MPI_SUM
   USE mpi, only: MPI_PROD
   USE mpi, only: MPI_LAND
   USE mpi, only: MPI_LOR
   USE mpi, only: MPI_LXOR
   USE mpi, only: MPI_MAXLOC
   USE mpi, only: MPI_MINLOC
   USE mpi, only: MPI_SCAN
   USE mpi, only: MPI_REDUCE
   USE mpi, only: MPI_ALLREDUCE
   USE mpi, only: MPI_BCAST
   USE mpi, only: MPI_IBCAST
   USE mpi, only: MPI_GATHER
   USE mpi, only: MPI_IGATHER
   USE mpi, only: MPI_ALLGATHER
   USE mpi, only: MPI_IALLGATHER
   USE mpi, only: MPI_ALLTOALL
   USE mpi, only: MPI_IALLTOALL

!  MPI scatter functions   
   USE mpi, only: MPI_SCATTER
   USE mpi, only: MPI_ISCATTER

!  MPI status   
   USE mpi, only: MPI_STATUS_SIZE
   USE mpi, only: MPI_STATUS_IGNORE
   USE mpi, only: MPI_STATUSES_IGNORE
   USE mpi, only: MPI_PROBE
   USE mpi, only: MPI_IPROBE
   USE mpi, only: MPI_TEST
   USE mpi, only: MPI_TESTANY
   USE mpi, only: MPI_TESTALL
   USE mpi, only: MPI_TEST_CANCELLED

!  MPI initialise and finalise   
   USE mpi, only: MPI_INIT
   USE mpi, only: MPI_FINALIZE

!  MPI Barrier   
   USE mpi, only: MPI_BARRIER
   USE mpi, only: MPI_IBARRIER
   USE mpi, only: MPI_WAIT
   USE mpi, only: MPI_WAITANY
   USE mpi, only: MPI_WAITSOME
   USE mpi, only: MPI_WAITALL

!  MPI Characteristics  
   USE mpi, only: MPI_WTICK
   USE mpi, only: MPI_WTIME

!  MPI Send and receives   
   USE mpi, only: MPI_SEND
   USE mpi, only: MPI_RECV
   USE mpi, only: MPI_ISEND
   USE mpi, only: MPI_IRECV
   USE mpi, only: MPI_SSEND
   USE mpi, only: MPI_ISSEND
   USE mpi, only: MPI_SENDRECV
   USE mpi, only: MPI_WIN_CREATE
   USE mpi, only: MPI_WIN_CREATE_DYNAMIC
   USE mpi, only: MPI_WIN_ALLOCATE
   USE mpi, only: MPI_WIN_ALLOCATE_SHARED
   USE mpi, only: MPI_WIN_FREE
   USE mpi, only: MPI_WIN_LOCK
   USE mpi, only: MPI_WIN_UNLOCK
   USE mpi, only: MPI_LOCK_EXCLUSIVE
   USE mpi, only: MPI_LOCK_SHARED
   USE mpi, only: MPI_PUT
   USE mpi, only: MPI_GET
   USE mpi, only: MPI_ACCUMULATE
   USE mpi, only: MPI_FETCH_AND_OP
   USE mpi, only: MPI_GET_ACCUMULATE
#endif

  implicit none

contains

   initialize
   ! Initialise the parallel environment.
#ifdef MPI
      .is_parallel = TRUE
      .do_parallel_lock = " "
      .master_rank = 0
      call MPI_INIT(.mpi_error)
      call MPI_COMM_DUP(MPI_COMM_WORLD,.mpi_comm,.mpi_error)
      call MPI_COMM_SIZE(.mpi_comm,.n_processors,.mpi_error)
      call MPI_COMM_RANK(.mpi_comm,.processor_rank,.mpi_error)
#else
      .is_parallel = FALSE
      .do_parallel_lock = " "
      .master_rank = 0
      .n_processors = 1
      .processor_rank = 0
#endif
   end

   finalize
   ! Finalise the parallel environment.
      self :: INOUT

      .is_parallel = FALSE
#ifdef MPI
      call MPI_FINALIZE(.mpi_error)
#endif

   end

! ================
! Inquiry routines
! ================

   is_master_processor result (res) ::: pure
   ! Return TRUE if this is the master processor. The index of the master
   ! processor is normally 0.
      self :: IN
      res :: BIN

      res = .processor_rank == .master_rank

   end

   master_processor result (res) ::: pure
   ! Return the index of the master processor, which is normally 0.
      self :: IN
      res :: INT

      res = .master_rank

   end

   this_processor result (res) ::: pure
   ! Return the index of this processor.  First index is zero!
      self :: IN
      res :: INT

      if (.is_parallel) then; res = .processor_rank
      else;                   res = 0
      end

   end

! ============================================================================
! Parallel do loops: simple parallel loops are implemented in the preprocessor
! ============================================================================

   parallel_do_start(first,stride) result (res) ::: pure
   ! Return the starting index to be used in a parallel do statement. The
   ! "first" index and the loop "stride" can optionally be supplied, if they are
   ! not equal to 1.  In this model, each processor skips through the whole
   ! length of the loop.  This should be load balanced --- assuming there is no
   ! systematic dependence in the work for each element of the loop which
   ! depends on a multiple of the number of processors. See the "do_stride"
   ! routine.
      self :: IN
      first,stride :: INT, IN, optional
      res :: INT

      f,s :: INT

      f = 1
      if (present(first)) f = first

      s = 1
      if (present(stride)) s = stride

      if (.do_in_parallel) then; res = f + s*.processor_rank
      else;                      res = f
      end

   end

   parallel_do_stride(stride) result (res) ::: pure
   ! Return the stride to be used in a parallel do statement.  The "stride"
   ! length can be optionally supplied, if it is not 1.
      self :: IN
      stride :: INT, IN, optional
      res :: INT

      s :: INT

      s = 1
      if (present(stride))       s = stride

      if (.do_in_parallel) then; res = s*.n_processors
      else;                      res = s
      end

   end

   do_in_parallel result (res) ::: pure
   ! Return TRUE if we are allowed to start to implement a given do-loop in
   ! parallel. NOTE: Once you have parallelised a loop, remember to prevent
   ! further parallelisation, using the ".lock_parallel_do" method. This is
   ! typically used as the first statement after the parallel do loop is
   ! entered.
      self :: IN
      res :: BIN

      res = .is_parallel AND .do_parallel_lock==" "

   end

   lock_parallel_do(name) ::: pure
   ! Sets the parallel do-loop lock to the "name" of the locking routine.
   ! Only the routine with the same "name" may unlock the loop. WARNING: This
   ! assumes that the names of the routines are all distinct. NOTE: It is
   ! currently an error if the routine is recursive. Nested parallel loops are
   ! OK but disabled.
      self :: INOUT
      name :: STR, IN

 !   ENSURE(name/=.do_parallel_lock,"recursive parallel routines not allowed")

      if (.do_parallel_lock==name) return
      if (.do_parallel_lock==" ") .do_parallel_lock = name

   end

   unlock_parallel_do(name) ::: pure
   ! Allow parallelisation again, if the name matches the original.
      self :: INOUT
      name :: STR, IN

      if (.do_parallel_lock==name) .do_parallel_lock = " "

   end

! =======================
! Characteristic routines
! =======================

   init ::: template
   ! Initialize the MPI execution environment 
#ifdef MPI
   call MPI_INIT(.mpi_error)
#endif
   end

   comm_size(size) ::: template
   ! Determines the size of the group associated with a communicator
   size :: INT, OUT
#ifdef MPI
   call MPI_COMM_SIZE(MPI_COMM_WORLD,size,.mpi_error)
#else
   size = 0
#endif
   end

   comm_rank(rank) ::: template
   ! Determines the rank of the calling process in the communicator 
   rank :: INT, OUT
#ifdef MPI
   call MPI_COMM_RANK(MPI_COMM_WORLD,rank,.mpi_error)
#else
   rank = 0
#endif
   end

! ==================
! Summation routines
! ==================

   parallel_vector_sum(vec,dim) ::: template
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
      dim :: INT, IN
      vec :: VEC{X}(dim), INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: VEC{X}*

      vec = vec

      allocate(tmp(dim))
#ifdef MPI
      call MPI_ALLREDUCE(vec,tmp,dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      vec = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>INT, Y=>MPI_INTEGER)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val,sum) ::: template
   ! This routine adds the versions of value "val" from all processors, and gives the
   ! result to all processors.
      val :: XX, INOUT
      sum :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

#ifdef MPI
      tmp = val
      call MPI_ALLREDUCE(val,tmp,LEN,YY,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      if (present(sum)) then
         sum = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif

   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine adds the versions of value "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine adds the versions of value "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine adds the versions of value "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_sum(val,sum) ::: template
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
      val :: XX, INOUT
      sum :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX@

#ifdef MPI
      tmp = val
      call MPI_ALLREDUCE(val,tmp,LEN,YY,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      if (present(sum)) then
         sum = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif
      val = tmp

   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   
   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_symmetric_sum(mat)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors. The matrix "mat" is assumed to be symmetric, and
   ! only the lower half of "mat" is summed; the upper triangle is forced to be
   ! the same as the lower triangle.
      mat :: MAT{REAL}, INOUT

   ENSURE(.is_parallel,"must be in a parallel environment")
   ENSURE(mat.dim1==mat.dim2,"mat must be square")

      invec,outvec :: VEC{REAL}*
      tri_size,dim :: INT

      dim = mat.dim1
      tri_size = dim*(dim+1)/2

      allocate(invec(tri_size))
      allocate(outvec(tri_size))
#ifdef MPI
      .compress_to_triangle(invec,mat)
      call MPI_ALLREDUCE(invec,outvec,tri_size,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      .uncompress_from_triangle(outvec,mat)
#else
      DIE("wtf?")
#endif
      deallocate(outvec)
      deallocate(invec)

   end

! ================
! Summation reduce
! ================

   reduce_sum(val,root,sum) ::: template
   ! This routine adds the versions of value "val" from all processors, and gives the
   ! result to the root processor.
      val :: XX, INOUT
      root :: INT, IN
      sum :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

#ifdef MPI
      tmp = val
      call MPI_REDUCE(val,tmp,LEN,YY,MPI_SUM,root,MPI_COMM_WORLD,.mpi_error)
      if (present(sum)) then
         sum = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif

   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine adds the versions of value "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine adds the versions of value "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine adds the versions of value "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_sum(val,root,sum) ::: template
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
      val :: XX, INOUT

      root :: INT, IN
      sum :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX@

#ifdef MPI
      tmp = val
      call MPI_REDUCE(val,tmp,LEN,YY,MPI_SUM,root,MPI_COMM_WORLD,.mpi_error)
      if (present(sum)) then
         sum = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif

   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   
   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_sum(val,root,sum) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

! ==============
! Summation scan
! ==============

   scan_sum(val,sum) ::: template
   ! This routine adds the versions of value "val" up to this processor, and gives the
   ! result.
      val :: XX, INOUT
      sum :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

#ifdef MPI
      tmp = val
      call MPI_SCAN(val,tmp,LEN,YY,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      if (present(sum)) then
         sum = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif
      val = tmp

   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine adds the versions of value "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine adds the versions of value "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine adds the versions of value "val" up to this processor, and gives the
   ! result.
   end


   scan_sum(val,sum) ::: template
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
      val :: XX, INOUT
      sum :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX@

#ifdef MPI
      tmp = val
      call MPI_SCAN(val,tmp,LEN,YY,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      if (present(sum)) then
         sum = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif
      val = tmp

   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   
   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end


   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end


   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end


   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_sum(val,sum) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine adds the versions of array "val" up to this processor, and gives the
   ! result.
   end

! =======================
! Multiplication routines
! =======================

   parallel_product(val,prod) ::: template
   ! This routine multiplies the versions of value "val" from all processors, and gives the
   ! result to all processors.
      val :: XX, INOUT
      prod :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

#ifdef MPI
      tmp = val
      call MPI_ALLREDUCE(val,tmp,LEN,YY,MPI_PROD,MPI_COMM_WORLD,.mpi_error)
      if(present(prod)) then
         prod = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif

   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine multiplies the versions of value "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine multiplies the versions of value "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine multiplies the versions of value "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_product(val,prod) ::: template
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
      val :: XX, INOUT
      prod :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX@

#ifdef MPI
      tmp = val
      call MPI_ALLREDUCE(val,tmp,LEN,YY,MPI_PROD,MPI_COMM_WORLD,.mpi_error)
      if(present(prod)) then
         prod = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif

   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   
   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_product(val,prod) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to all processors.
   end

! =====================
! Multiplication reduce
! =====================

   reduce_prod(val,root,prod) ::: template
   ! This routine multiplies the versions of value "val" from all processors, and gives the
   ! result to the root processor.
      val :: XX, INOUT
      root :: INT, IN
      prod :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

#ifdef MPI
      tmp = val
      call MPI_REDUCE(val,tmp,LEN,YY,MPI_PROD,root,MPI_COMM_WORLD,.mpi_error)
      if(present(prod)) then
         prod = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif

   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine multiplies the versions of value "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine multiplies the versions of value "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine multiplies the versions of value "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_prod(val,root,prod) ::: template
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
      val :: XX, INOUT
      root :: INT, IN
      prod :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX@

#ifdef MPI
      tmp = val
      call MPI_REDUCE(val,tmp,LEN,YY,MPI_PROD,root,MPI_COMM_WORLD,.mpi_error)
      if(present(prod)) then
         prod = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif

   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   
   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_prod(val,root,prod) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" from all processors, and gives the
   ! result to the root processor.
   end

! ===================
! Multiplication scan
! ===================

   scan_product(val,prod) ::: template
   ! This routine multiplies the versions of value "val" up to this processor, and gives the
   ! result.
      val :: XX, INOUT
      prod :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

#ifdef MPI
      tmp = val
      call MPI_SCAN(val,tmp,LEN,YY,MPI_PROD,MPI_COMM_WORLD,.mpi_error)
      if(present(prod)) then
         prod = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif

   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine multiplies the versions of value "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine multiplies the versions of value "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine multiplies the versions of value "val" up to this processor, and gives the
   ! result.
   end


   scan_product(val,prod) ::: template
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
      val :: XX, INOUT
      prod :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX@

#ifdef MPI
      tmp = val
      call MPI_SCAN(val,tmp,LEN,YY,MPI_PROD,MPI_COMM_WORLD,.mpi_error)
      if(present(prod)) then
         prod = tmp
      else
         val = tmp
      end if
#else
      DIE("wtf?")
#endif

   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   
   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end


   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end


   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end


   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

   scan_product(val,prod) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine multiplies the versions of array "val" up to this processor, and gives the
   ! result.
   end

! These should be inherited, but aren't, to decouple this
! module from any others.

   compress_to_triangle(tr,mat)
   ! Converts the lower triangle of matrix "mat" to the triangle "tr".
   ! using row order.
      mat :: MAT{REAL}
      tr :: VEC{REAL}

   ENSURE(mat.dim1==mat.dim2,"mat must be square")

      ij,i,j :: INT

      if (FALSE) self = self

      ij = 0
      do i = 1,mat.dim1
        do j = 1,i
           tr(ij+j) = mat(i,j)
        end
        ij = ij+i
      end

   end

   uncompress_from_triangle(tr,mat)
   ! Converts the triangle "tr" into the symmetric matrix "mat".
      tr :: VEC{REAL}
      mat :: MAT{REAL}

   ENSURE(mat.dim1==mat.dim2,"mat must be square")

      tmp :: REAL
      ij,i,j :: INT

      if (FALSE) self = self

      ij = 0
      do i = 1,size(mat,1)
        do j = 1,i
           tmp = tr(ij+j)
           mat(j,i) = tmp
           mat(i,j) = tmp
        end
        ij = ij+i
      end

   end

! ================
! Maximum routines
! ================

   parallel_maximum(val,max) ::: template
   ! This routine finds the maximum of value "val" from all processors, and gives the
   ! result to all processors.
      val :: XX, INOUT
      max :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

      tmp = val
#ifdef MPI
      call MPI_ALLREDUCE(val,tmp,LEN,YY,MPI_MAX,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      if(present(max)) then
         max = tmp
      else 
         val = tmp
      end


   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine finds the maximum of value "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine finds the maximum of value "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine finds the maximum of value "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_maximum(val,max) ::: template
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
      val :: XX, INOUT
      max :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")
   ENSURE(size(max)==size(val),"output must match size of input")

      tmp :: XX@

      tmp = val

#ifdef MPI
      call MPI_ALLREDUCE(val,tmp,LEN,YY,MPI_MAX,MPI_COMM_WORLD,.mpi_error)
      if(present(max)) then
         max = tmp
      else
         val = tmp
      end 

#else
      DIE("wtf?")
#endif
   
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   
   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_maximum_location(val,maxloc) ::: template
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
      val :: XX, INOUT
      maxloc :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")
   ENSURE(size(maxloc)==size(val),"output must match size of input")

      tmp :: XX@

      tmp = val

#ifdef MPI
      call MPI_ALLREDUCE(val,tmp,LEN,YY,MPI_MAXLOC,MPI_COMM_WORLD,.mpi_error)
      if(present(maxloc)) then
         maxloc = tmp
      else
         val = tmp
      end 

#else
      DIE("wtf?")
#endif
   
   end

   parallel_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   
   parallel_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" from all processors, and gives the
   ! result to all processors.
   end

! ==============
! Maximum reduce
! ==============

   reduce_max(val,root,max) ::: template
   ! This routine finds the maximum of value "val" from all processors, and gives the
   ! result to the root processor.
      val :: XX, INOUT
      root :: INT, IN
      max :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

      tmp = val
#ifdef MPI
      call MPI_REDUCE(val,tmp,LEN,YY,MPI_MAX,root,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      if(present(max)) then
         max = tmp
      else
         val = tmp
      end


   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine finds the maximum of value "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine finds the maximum of value "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine finds the maximum of value "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_max(val,root,max) ::: template
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
      val :: XX, INOUT

      root :: INT, IN
      max :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX@

      tmp = val
#ifdef MPI
      call MPI_REDUCE(val,tmp,LEN,YY,MPI_MAX,root,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      if(present(max)) then
         max = tmp
      else
         val = tmp
      end

   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   
   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_max(val,root,max) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   

   reduce_maxloc(val,root,maxloc) ::: template
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
      val :: XX, INOUT

      root :: INT, IN
      maxloc :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX@

      tmp = val
      tmp = 0
#ifdef MPI
      call MPI_REDUCE(val,tmp,LEN,YY,MPI_MAXLOC,root,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      if(present(maxloc)) then
         maxloc = tmp
      else
         val = tmp
      end

   end

   reduce_maxloc(val,root,maxloc) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_maxloc(val,root,maxloc) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   
   reduce_maxloc(val,root,maxloc) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_maxloc(val,root,maxloc) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_maxloc(val,root,maxloc) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_maxloc(val,root,maxloc) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_maxloc(val,root,maxloc) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_maxloc(val,root,maxloc) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_maxloc(val,root,maxloc) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_maxloc(val,root,maxloc) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

! ============
! Maximum scan
! ============

   scan_maximum(val,max) ::: template
   ! This routine finds the maximum of value "val" up to this processor, and gives the
   ! result.
      val :: XX, INOUT
      max :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

      tmp = val
#ifdef MPI
      call MPI_SCAN(val,tmp,LEN,YY,MPI_MAX,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      if(present(max)) then
         max = tmp
      else 
         val = tmp
      end


   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine finds the maximum of value "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine finds the maximum of value "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine finds the maximum of value "val" up to this processor, and gives the
   ! result.
   end


   scan_maximum(val,max) ::: template
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
      val :: XX, INOUT
      max :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")
   ENSURE(size(max)==size(val),"output must match size of input")

      tmp :: XX@

      tmp = val

#ifdef MPI
      call MPI_SCAN(val,tmp,LEN,YY,MPI_MAX,MPI_COMM_WORLD,.mpi_error)
      if(present(max)) then
         max = tmp
      else
         val = tmp
      end 

#else
      DIE("wtf?")
#endif
   
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   
   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum(val,max) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the maximum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_maximum_location(val,maxloc) ::: template
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
      val :: XX, INOUT
      maxloc :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")
   ENSURE(size(maxloc)==size(val),"output must match size of input")

      tmp :: XX@

      tmp = val

#ifdef MPI
      call MPI_SCAN(val,tmp,LEN,YY,MPI_MAXLOC,MPI_COMM_WORLD,.mpi_error)
      if(present(maxloc)) then
         maxloc = tmp
      else
         val = tmp
      end 

#else
      DIE("wtf?")
#endif
   
   end

   scan_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   
   scan_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_maximum_location(val,maxloc) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the maximum of array "val" up to this processor, and gives the
   ! result.
   end

! ================
! Minimum routines
! ================

   parallel_minimum(val,min) ::: template
   ! This routine finds the minimum of value "val" from all processors, and gives the
   ! result to all processors.
      val :: XX, INOUT
      min :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

      tmp = val
#ifdef MPI
      call MPI_ALLREDUCE(val,tmp,LEN,YY,MPI_MIN,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      if(present(min)) then
         min = tmp
      else 
         val = tmp
      end


   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine finds the minimum of value "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine finds the minimum of value "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine finds the minimum of value "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_minimum(val,min) ::: template
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
      val :: XX, INOUT
      min :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")
   ENSURE(size(min)==size(val),"output must match size of input")

      tmp :: XX@

      tmp = val

#ifdef MPI
      call MPI_ALLREDUCE(val,tmp,LEN,YY,MPI_MIN,MPI_COMM_WORLD,.mpi_error)
      if(present(min)) then
         min = tmp
      else
         val = tmp
      end 

#else
      DIE("wtf?")
#endif
   
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   
   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum_location(val,minloc) ::: template
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
      val :: XX, INOUT
      minloc :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")
   ENSURE(size(minloc)==size(val),"output must match size of input")

      tmp :: XX@

      tmp = val

#ifdef MPI
      call MPI_ALLREDUCE(val,tmp,LEN,YY,MPI_MINLOC,MPI_COMM_WORLD,.mpi_error)
      if(present(minloc)) then
         minloc = tmp
      else
         val = tmp
      end 

#else
      DIE("wtf?")
#endif
   
   end

   parallel_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   
   parallel_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end


   parallel_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

   parallel_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" from all processors, and gives the
   ! result to all processors.
   end

! ===============
! Minimum reduce
! ==============

   reduce_min(val,root,min) ::: template
   ! This routine finds the minimum of value "val" from all processors, and gives the
   ! result to the root processor.
      val :: XX, INOUT
      root :: INT, IN
      min :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

      tmp = val
#ifdef MPI
      call MPI_REDUCE(val,tmp,LEN,YY,MPI_MIN,root,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      if(present(min)) then
         min = tmp
      else
         val = tmp
      end


   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine finds the minimum of value "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine finds the minimum of value "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine finds the minimum of value "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_min(val,root,min) ::: template
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
      val :: XX, INOUT

      root :: INT, IN
      min :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX@

      tmp = val
#ifdef MPI
      call MPI_REDUCE(val,tmp,LEN,YY,MPI_MIN,root,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      if(present(min)) then
         min = tmp
      else
         val = tmp
      end

   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   
   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_min(val,root,min) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   

   reduce_minloc(val,root,minloc) ::: template
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
      val :: XX, INOUT

      root :: INT, IN
      minloc :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX@

      tmp = val
#ifdef MPI
      call MPI_REDUCE(val,tmp,LEN,YY,MPI_MINLOC,root,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      if(present(minloc)) then
         minloc = tmp
      else
         val = tmp
      end

   end

   reduce_minloc(val,root,minloc) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_minloc(val,root,minloc) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   
   reduce_minloc(val,root,minloc) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_minloc(val,root,minloc) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_minloc(val,root,minloc) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_minloc(val,root,minloc) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_minloc(val,root,minloc) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_minloc(val,root,minloc) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end


   reduce_minloc(val,root,minloc) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

   reduce_minloc(val,root,minloc) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum and its location of array "val" from all processors, and gives the
   ! result to the root processor.
   end

! ================
! Minimum routines
! ================

   scan_minimum(val,min) ::: template
   ! This routine finds the minimum of value "val" up to this processor, and gives the
   ! result.
      val :: XX, INOUT
      min :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

      tmp = val
#ifdef MPI
      call MPI_SCAN(val,tmp,LEN,YY,MPI_MIN,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      if(present(min)) then
         min = tmp
      else 
         val = tmp
      end


   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER, LEN=>1)
   ! This routine finds the minimum of value "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! This routine finds the minimum of value "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! This routine finds the minimum of value "val" up to this processor, and gives the
   ! result.
   end


   scan_minimum(val,min) ::: template
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
      val :: XX, INOUT
      min :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")
   ENSURE(size(min)==size(val),"output must match size of input")

      tmp :: XX@

      tmp = val

#ifdef MPI
      call MPI_SCAN(val,tmp,LEN,YY,MPI_MIN,MPI_COMM_WORLD,.mpi_error)
      if(present(min)) then
         min = tmp
      else
         val = tmp
      end 

#else
      DIE("wtf?")
#endif
   
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>VEC{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   
   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT3{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT4{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_INTEGER, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum(val,min) ::: get_from(PARALLEL, XX=>MAT5{CPX}, YY=>MPI_DOUBLE_COMPLEX, LEN=>size(val))
   ! This routine finds the minimum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_minimum_location(val,minloc) ::: template
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
      val :: XX, INOUT
      minloc :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")
   ENSURE(size(minloc)==size(val),"output must match size of input")

      tmp :: XX@

      tmp = val

#ifdef MPI
      call MPI_SCAN(val,tmp,LEN,YY,MPI_MINLOC,MPI_COMM_WORLD,.mpi_error)
      if(present(minloc)) then
         minloc = tmp
      else
         val = tmp
      end 

#else
      DIE("wtf?")
#endif
   
   end

   scan_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>VEC{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>VEC{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   
   scan_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT3{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT3{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT4{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT4{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
   end


   scan_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT5{INT}, YY=>MPI_2INTEGER, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
   end

   scan_minimum_location(val,minloc) ::: get_from(PARALLEL, XX=>MAT5{REAL}, YY=>MPI_2DOUBLE_PRECISION, LEN=>size(val))
   ! This routine finds the location and value of the minimum of array "val" up to this processor, and gives the
   ! result.
   end

! =================
! Logical Allreduce
! =================
   
   parallel_or(bin,lor) ::: template
   ! This routine collects binary statements, and returns true if one
   ! binary statements is true.
      bin :: BIN, INOUT
      lor :: BIN, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_ALLREDUCE(bin,tmp,1,MPI_LOGICAL,MPI_LOR,MPI_COMM_WORLD,.mpi_error)
      if(present(lor)) then
         lor = tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   parallel_or(bin,lor) ::: template
   ! This routine collects multiple statements, and returns true if one
   ! binary statements is true.
      bin :: XX, INOUT
      lor :: XX, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: XX@

#ifdef MPI
      tmp = bin
      call MPI_ALLREDUCE(bin,tmp,LEN,MPI_LOGICAL,MPI_LOR,MPI_COMM_WORLD,.mpi_error)
      if(present(lor)) then
         lor = tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   parallel_or(bin,lor) ::: get_from(PARALLEL, XX=>VEC{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if one
   ! binary statements is true.
   end

   parallel_or(bin,lor) ::: get_from(PARALLEL, XX=>MAT{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if one
   ! binary statements is true.
   end

   parallel_or(bin,lor) ::: get_from(PARALLEL, XX=>MAT3{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if one
   ! binary statements is true.
   end

   parallel_or(bin,lor) ::: get_from(PARALLEL, XX=>MAT4{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if one
   ! binary statements is true.
   end

   parallel_xor(bin,lxor) ::: template
   ! This routine collects binary statements, and returns true if exactly one
   ! binary statements is true.
      bin :: BIN, INOUT
      lxor :: BIN, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_ALLREDUCE(bin,tmp,1,MPI_LOGICAL,MPI_LXOR,MPI_COMM_WORLD,.mpi_error)
      if(present(lxor)) then
         lxor= tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   parallel_xor(bin,lxor) ::: template
   ! This routine collects multiple statements, and returns true if exactly one
   ! binary statements is true.
      bin :: XX, INOUT
      lxor :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: XX@

#ifdef MPI
      tmp = bin
      call MPI_ALLREDUCE(bin,tmp,LEN,MPI_LOGICAL,MPI_LXOR,MPI_COMM_WORLD,.mpi_error)
      if(present(lxor)) then
         lxor= tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   parallel_xor(bin,lxor) ::: get_from(PARALLEL, XX=>VEC{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if exactly one
   ! binary statements is true.
   end

   parallel_xor(bin,lxor) ::: get_from(PARALLEL, XX=>MAT{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if exactly one
   ! binary statements is true.
   end

   parallel_xor(bin,lxor) ::: get_from(PARALLEL, XX=>MAT3{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if exactly one
   ! binary statements is true.
   end

   parallel_xor(bin,lxor) ::: get_from(PARALLEL, XX=>MAT4{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if exactly one
   ! binary statements is true.
   end

   parallel_and(bin,land) ::: template
   ! This routine collects binary statements, and returns true if all
   ! binary statements are true.
      bin :: BIN, INOUT
      land :: BIN, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_ALLREDUCE(bin,tmp,1,MPI_LOGICAL,MPI_LAND,MPI_COMM_WORLD,.mpi_error)
      if(present(land)) then
         land= tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   parallel_and(bin,land) ::: template
   ! This routine collects multiple statements, and returns true if all
   ! binary statements are true.
      bin :: XX, INOUT
      land :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: XX@

#ifdef MPI
      tmp = bin
      call MPI_ALLREDUCE(bin,tmp,LEN,MPI_LOGICAL,MPI_LAND,MPI_COMM_WORLD,.mpi_error)
      if(present(land)) then
         land= tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   parallel_and(bin,land) ::: get_from(PARALLEL, XX=>VEC{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if all
   ! binary statements are true.
   end

   parallel_and(bin,land) ::: get_from(PARALLEL, XX=>MAT{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if all
   ! binary statements are true.
   end

   parallel_and(bin,land) ::: get_from(PARALLEL, XX=>MAT3{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if all
   ! binary statements are true.
   end

   parallel_and(bin,land) ::: get_from(PARALLEL, XX=>MAT4{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true if all
   ! binary statements are true.
   end

! ==============
! Logical Reduce
! ==============
   
   reduce_lor(bin,root,lor)::: template
   ! This routine collects binary statements, and returns true to root if one
   ! binary statements is true.
      bin :: BIN, INOUT
      root :: INT, IN
      lor :: BIN, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_REDUCE(bin,tmp,1,MPI_LOGICAL,MPI_LOR,root,MPI_COMM_WORLD,.mpi_error)
      if(present(lor)) then
         lor = tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   reduce_lor(bin,root,lor)::: template
   ! This routine collects multiple statements, and returns true to root if one
   ! binary statements is true.
      bin :: XX, INOUT
      root :: INT, IN
      lor :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: XX@

#ifdef MPI
      tmp = bin
      call MPI_REDUCE(bin,tmp,LEN,MPI_LOGICAL,MPI_LOR,root,MPI_COMM_WORLD,.mpi_error)
      if(present(lor)) then
         lor = tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   reduce_lor(bin,root,lor)::: get_from(PARALLEL, XX=>VEC{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if one
   ! binary statements is true.
   end

   reduce_lor(bin,root,lor)::: get_from(PARALLEL, XX=>MAT{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if one
   ! binary statements is true.
   end

   reduce_lor(bin,root,lor)::: get_from(PARALLEL, XX=>MAT3{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if one
   ! binary statements is true.
   end

   reduce_lor(bin,root,lor)::: get_from(PARALLEL, XX=>MAT4{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if one
   ! binary statements is true.
   end

   reduce_lxor(bin,root,lxor)::: template
   ! This routine collects binary statements, and returns true to root if exactly one
   ! binary statements is true.
      bin :: BIN, INOUT
      root :: INT, IN
      lxor :: BIN, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_REDUCE(bin,tmp,1,MPI_LOGICAL,MPI_LXOR,root,MPI_COMM_WORLD,.mpi_error)
      if(present(lxor)) then
         lxor= tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   reduce_lxor(bin,root,lxor)::: template
   ! This routine collects multiple statements, and returns true to root if exactly one
   ! binary statements is true.
      bin :: XX, INOUT
      root :: INT, IN
      lxor :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: XX@

#ifdef MPI
      tmp = bin
      call MPI_REDUCE(bin,tmp,LEN,MPI_LOGICAL,MPI_LXOR,root,MPI_COMM_WORLD,.mpi_error)
      if(present(lxor)) then
         lxor= tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   reduce_lxor(bin,root,lxor)::: get_from(PARALLEL, XX=>VEC{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if exactly one
   ! binary statements is true.
   end

   reduce_lxor(bin,root,lxor)::: get_from(PARALLEL, XX=>MAT{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if exactly one
   ! binary statements is true.
   end

   reduce_lxor(bin,root,lxor)::: get_from(PARALLEL, XX=>MAT3{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if exactly one
   ! binary statements is true.
   end

   reduce_lxor(bin,root,lxor)::: get_from(PARALLEL, XX=>MAT4{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if exactly one
   ! binary statements is true.
   end

   reduce_land(bin,root,land)::: template
   ! This routine collects binary statements, and returns true to root if all
   ! binary statements are true.
      bin :: BIN, INOUT
      root :: INT, IN
      land :: BIN, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_REDUCE(bin,tmp,1,MPI_LOGICAL,MPI_LAND,root,MPI_COMM_WORLD,.mpi_error)
      if(present(land)) then
         land= tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   reduce_land(bin,root,land)::: template
   ! This routine collects multiple statements, and returns true to root if all
   ! binary statements are true.
      bin :: XX, INOUT
      root :: INT, IN
      land :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: XX@

#ifdef MPI
      tmp = bin
      call MPI_REDUCE(bin,tmp,LEN,MPI_LOGICAL,MPI_LAND,root,MPI_COMM_WORLD,.mpi_error)
      if(present(land)) then
         land= tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   reduce_land(bin,root,land)::: get_from(PARALLEL, XX=>VEC{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if all
   ! binary statements are true.
   end

   reduce_land(bin,root,land)::: get_from(PARALLEL, XX=>MAT{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if all
   ! binary statements are true.
   end

   reduce_land(bin,root,land)::: get_from(PARALLEL, XX=>MAT3{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if all
   ! binary statements are true.
   end

   reduce_land(bin,root,land)::: get_from(PARALLEL, XX=>MAT4{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements, and returns true to root if all
   ! binary statements are true.
   end

! ============
! Logical Scan
! ============
   
   scan_lor(bin,lor) ::: template
   ! This routine collects binary statements up to this processor, and returns true if one
   ! binary statements is true.
      bin :: BIN, INOUT
      lor :: BIN, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_SCAN(bin,tmp,1,MPI_LOGICAL,MPI_LOR,MPI_COMM_WORLD,.mpi_error)
      if(present(lor)) then
         lor = tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   scan_lor(bin,lor) ::: template
   ! This routine collects multiple statements up to this processor, and returns true if one
   ! binary statements is true.
      bin :: XX, INOUT
      lor :: XX, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: XX@

#ifdef MPI
      tmp = bin
      call MPI_SCAN(bin,tmp,LEN,MPI_LOGICAL,MPI_LOR,MPI_COMM_WORLD,.mpi_error)
      if(present(lor)) then
         lor = tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   scan_lor(bin,lor) ::: get_from(PARALLEL, XX=>VEC{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if one
   ! binary statements is true.
   end

   scan_lor(bin,lor) ::: get_from(PARALLEL, XX=>MAT{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if one
   ! binary statements is true.
   end

   scan_lor(bin,lor) ::: get_from(PARALLEL, XX=>MAT3{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if one
   ! binary statements is true.
   end

   scan_lor(bin,lor) ::: get_from(PARALLEL, XX=>MAT4{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if one
   ! binary statements is true.
   end

   scan_lxor(bin,lxor) ::: template
   ! This routine collects binary statements up to this processor, and returns true if exactly one
   ! binary statements is true.
      bin :: BIN, INOUT
      lxor :: BIN, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_SCAN(bin,tmp,1,MPI_LOGICAL,MPI_LXOR,MPI_COMM_WORLD,.mpi_error)
      if(present(lxor)) then
         lxor = tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   scan_lxor(bin,lxor) ::: template
   ! This routine collects multiple statements up to this processor, and returns true if exactly one
   ! binary statements is true.
      bin :: XX, INOUT
      lxor :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: XX@

#ifdef MPI
      tmp = bin
      call MPI_SCAN(bin,tmp,LEN,MPI_LOGICAL,MPI_LXOR,MPI_COMM_WORLD,.mpi_error)
      if(present(lxor)) then
         lxor = tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   scan_lxor(bin,lxor) ::: get_from(PARALLEL, XX=>VEC{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if exactly one
   ! binary statements is true.
   end

   scan_lxor(bin,lxor) ::: get_from(PARALLEL, XX=>MAT{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if exactly one
   ! binary statements is true.
   end

   scan_lxor(bin,lxor) ::: get_from(PARALLEL, XX=>MAT3{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if exactly one
   ! binary statements is true.
   end

   scan_lxor(bin,lxor) ::: get_from(PARALLEL, XX=>MAT4{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if exactly one
   ! binary statements is true.
   end

   scan_land(bin,land) ::: template
   ! This routine collects binary statements up to this processor, and returns true if all
   ! binary statements are true.
      bin :: BIN, INOUT
      land :: BIN, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_SCAN(bin,tmp,1,MPI_LOGICAL,MPI_LAND,MPI_COMM_WORLD,.mpi_error)
      if(present(land)) then
         land = tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   scan_land(bin,land) ::: template
   ! This routine collects multiple statements up to this processor, and returns true if all
   ! binary statements are true.
      bin :: XX, INOUT
      land :: XX, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: XX@

#ifdef MPI
      tmp = bin
      call MPI_SCAN(bin,tmp,LEN,MPI_LOGICAL,MPI_LAND,MPI_COMM_WORLD,.mpi_error)
      if(present(land)) then
         land = tmp
      else
         bin = tmp
      end if
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

   scan_land(bin,land) ::: get_from(PARALLEL, XX=>VEC{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if all
   ! binary statements are true.
   end

   scan_land(bin,land) ::: get_from(PARALLEL, XX=>MAT{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if all
   ! binary statements are true.
   end

   scan_land(bin,land) ::: get_from(PARALLEL, XX=>MAT3{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if all
   ! binary statements are true.
   end

   scan_land(bin,land) ::: get_from(PARALLEL, XX=>MAT4{BIN}, LEN=>size(bin))
   ! This routine collects multiple statements up to this processor, and returns true if all
   ! binary statements are true.
   end

! =======
! Barrier
! =======

   barrier ::: template 
   ! Blocks until all processes in the communicator have reached this routine.
   self :: INOUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
      
#ifdef MPI
      call MPI_BARRIER(MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
      self = self
#endif

   end

   ibarrier(request) ::: template 
   ! Notifies the process that it has reached the barrier and returns immediately
   self :: INOUT
   request :: INT, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")
   req :: INT   
      
#ifdef MPI
#ifndef _WIN32
      call MPI_IBARRIER(MPI_COMM_WORLD,req,.mpi_error)
      if(present(request)) request = req
#endif
#else
      req = 0
      if(present(request)) request = req
      DIE("wtf?")
      self = self
#endif

   end

! ====
! Wait
! ====

   parallel_wait(request) ::: template 
   ! Waits for an MPI request to complete
      request :: INT, INOUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      status :: VEC{INT}(6)
#endif      
      
#ifdef MPI
      call MPI_WAIT(request,MPI_STATUS_IGNORE,.mpi_error)
#else
      DIE("wtf?")
      self = self
      request = request
#endif
   end

   waitall(array_of_requests) ::: template 
   ! Waits for all given MPI Requests to complete
      array_of_requests :: VEC{INT}, INOUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
     ! status :: VEC{INT}*
      i :: INT
      statuses :: MAT{INT}@ 
      ALLOCATE(statuses(array_of_requests.dim,MPI_STATUS_SIZE))

#endif      
      
#ifdef MPI
      print*,size(array_of_requests)
      print*,array_of_requests
      print*, "Beginning wait"
      call MPI_WAITALL(size(array_of_requests),array_of_requests,statuses,.mpi_error)
      print*, "Finished wait"
      DEALLOCATE(statuses)
#else
      array_of_requests(1) = 0
      DIE("wtf?")
      self = self
#endif

   end

   waitany(array_of_requests,index) ::: template 
   ! Waits for any specified MPI Request to complete
      array_of_requests :: VEC{INT}, INOUT
      index :: INT, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")
#ifdef MPI
      indx :: INT
      status :: VEC{INT}*
      ALLOCATE(status(MPI_STATUS_SIZE))
#endif      
      
#ifdef MPI
#ifndef _WIN32   
      call MPI_WAITANY(size(array_of_requests),array_of_requests,indx,status,.mpi_error)
      if(present(index)) index = indx
      DEALLOCATE(status)
#endif
#else
      array_of_requests(1) = 0
      if(present(index)) index = 0
      DIE("wtf?")
      self = self
#endif

   end

   waitsome(array_of_requests,outcount,indices) ::: template 
   ! Waits for some given MPI Requests to complete
      array_of_requests :: VEC{INT}, INOUT
      outcount :: INT, OUT, optional
      indices :: VEC{INT}, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
     ! status :: VEC{INT}*
      i, ocount :: INT
      ind :: VEC{INT}@
      statuses :: MAT{INT}@ 
      ALLOCATE(statuses(array_of_requests.dim,MPI_STATUS_SIZE))
      ALLOCATE(ind(array_of_requests.dim))

#endif      
      
#ifdef MPI
      call MPI_WAITSOME(size(array_of_requests),array_of_requests,ocount,ind,statuses,.mpi_error)
      if(present(indices))   indices = ind
      if(present(outcount)) outcount = ocount
      DEALLOCATE(statuses)
#else
      array_of_requests(1) = outcount
      DIE("wtf?")
      self = self
#endif

   end

! =====
! Probe
! =====

   probe(source,tag) ::: template 
   ! Blocking test for a message
      source :: INT, IN
      tag :: INT, IN, optional
   ENSURE(.is_parallel,"must be in a parallel environment")
#ifdef MPI
      status :: VEC{INT}(MPI_STATUS_SIZE)
#else
      status :: VEC{INT}(6)
#endif
      mtag :: INT
      mtag = 1
      if(present(tag)) mtag = tag
#ifdef MPI
      call MPI_PROBE(source,mtag,MPI_COMM_WORLD,status,.mpi_error)
#else
      status(1)=source
#endif
   end

   iprobe(source,tag,flag) ::: template 
   ! Nonblocking test for a message
      source :: INT, IN
      tag :: INT, IN, optional
      flag :: BIN, OUT
   ENSURE(.is_parallel,"must be in a parallel environment")
#ifdef MPI
      status :: VEC{INT}(MPI_STATUS_SIZE)
#else
      status :: VEC{INT}(6)
#endif
      mtag :: INT
      mtag = 1
      if(present(tag)) mtag = tag
#ifdef MPI
      call MPI_IPROBE(source,mtag,MPI_COMM_WORLD,flag,status,.mpi_error)
#else
      flag = TRUE
      status(1)=source
#endif
   end

! ====
! Test
! ====

   test(request,flag) ::: template 
   ! Tests for the completion of a request
      request :: INT, INOUT
      flag :: BIN, OUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      status :: VEC{INT}(MPI_STATUS_SIZE)
#endif      
      
#ifdef MPI
      call MPI_TEST(request,flag,status,.mpi_error)
#else
      DIE("wtf?")
      flag = TRUE
      self = self
      request = request
#endif
   end

   testany(array_of_requests,index,flag) ::: template 
   ! Tests for the completion of any previously initiated array_of_requests
      array_of_requests :: VEC{INT}, INOUT
      index :: INT, OUT
      flag :: BIN, OUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      status :: VEC{INT}(MPI_STATUS_SIZE)
#endif      
      
#ifdef MPI
#ifndef _WIN32
      call MPI_TESTANY(size(array_of_requests),array_of_requests,index,flag,status,.mpi_error)
#endif
#else
      DIE("wtf?")
      flag = TRUE
      self = self
      array_of_requests = array_of_requests
#endif
   end

   testall(array_of_requests,flag) ::: template 
   ! Tests for the completion of all previously initiated requests
      array_of_requests :: VEC{INT}, INOUT
      flag :: BIN, OUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      statuses :: MAT{INT}@ 
      ALLOCATE(statuses(array_of_requests.dim,MPI_STATUS_SIZE))
#endif      
      
#ifdef MPI
#ifndef _WIN32
      call MPI_TESTALL(size(array_of_requests),array_of_requests,flag,statuses,.mpi_error)
#endif
#else
      DIE("wtf?")
      flag = TRUE
      self = self
      array_of_requests = array_of_requests
#endif
   end

   test_cancelled(status,flag) ::: template 
   ! Tests to see if a request was cancelled
      flag :: BIN, OUT
   ENSURE(.is_parallel,"must be in a parallel environment")
#ifdef MPI
      status :: VEC{INT}(MPI_STATUS_SIZE), IN
#else
      status :: VEC{INT}(6), IN
      i :: INT
#endif      
      
#ifdef MPI
      call MPI_TEST_CANCELLED(status,flag,.mpi_error)
#else
      DIE("wtf?")
      flag = TRUE
      self = self
      i = status(1)
#endif
   end

! ===================
! Walltime and Wticks
! ===================

   wtick(res) ::: template 
   ! Returns the resolution of MPI_Wtime
      res :: REAL, OUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      res = MPI_WTICK()
      !DEALLOCATE(status)
#else
      DIE("wtf?")
      self = self
      res=0
#endif
   end

   wtime(time) ::: template 
   ! Returns an elapsed time on the calling processor 
      time :: REAL, OUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      time = MPI_WTIME()
      !DEALLOCATE(status)
#else
      DIE("wtf?")
      self = self
      time=0
#endif
   end

! ======
! Cancel
! ======

   cancel(request) ::: template 
   ! Cancels a communication request
      request :: INT, INOUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      call MPI_CANCEL(request,.mpi_error)
      !DEALLOCATE(status)
#else
      DIE("wtf?")
      self = self
      request = request
#endif
   end

! =============
! Blocking send
! =============

   send(buf,dest,tag) ::: template
   ! Performs a blocking send
      buf :: VAR_TYPE, INOUT
      dest :: INT, IN
      tag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc,mtag :: INT

      if (FALSE) self = self

      mtag = 1
      proc = dest
      

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_SEND(buf,LEN,MPI_TYPE,proc,mtag,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
      proc = proc
      self = self
      mtag = mtag
      if(present(tag)) mtag = tag
      buf = buf
#endif

   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

! ================
! Blocking receive
! ================

   recv(buf,source,tag) ::: template
   ! Blocking receive for a message
      self :: INOUT
      buf :: VAR_TYPE, OUT
      source :: INT, IN
      tag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc,mtag :: INT
#ifdef MPI
      status :: VEC{INT}*
      ALLOCATE(status(MPI_STATUS_SIZE))
#endif      
      if (FALSE) self = self

      mtag = 1
      proc = source

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_RECV(buf,LEN,MPI_TYPE,proc,mtag, &
      MPI_COMM_WORLD,status,.mpi_error)
      DEALLOCATE(status)
#else
      DIE("wtf?")
      buf = buf
      proc = proc
      self = self
      mtag = mtag
      if(present(tag)) mtag = tag
#endif

   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end

! =================
! Non-blocking send
! =================

   isend(buf,dest,tag,request) ::: template
   ! Begins a nonblocking send
      buf :: VAR_TYPE, INOUT
      dest :: INT, IN
      tag :: INT, IN, optional
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      req,proc,mtag :: INT


      mtag = 1
      proc = dest

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_ISEND(buf,LEN,MPI_TYPE,proc,mtag, &
      MPI_COMM_WORLD,req,.mpi_error)
      if (present(request)) request = req
#else
      if (FALSE) self = self
      DIE("wtf?")
      proc = proc
      self = self
      req =1 
      buf = buf
      if(present(tag)) mtag = tag
      if (present(request)) request = req
#endif

   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end

! ====================
! Non-blocking receive
! ====================

   irecv(buf,source,tag,request) ::: template
   ! Begins a nonblocking receive
      buf :: VAR_TYPE, INOUT
      source :: INT, IN
      tag :: INT, IN, optional
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      requestuest,proc,mtag :: INT

      if (FALSE) self = self

      mtag = 1
      proc = source

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_IRECV(buf,LEN,MPI_TYPE,proc,mtag,MPI_COMM_WORLD, &
      requestuest,.mpi_error)
      if(present(request)) request = requestuest
#else
      DIE("wtf?")
      buf = buf
      proc = proc
      request = request
      self = self
      requestuest = 1
      mtag = mtag
      if(present(tag)) mtag = tag
      if (present(request)) request = requestuest
#endif

   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
!   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

! =========================
! Blocking synchronous send
! =========================

   ssend(buf,dest,tag) ::: template
   ! Blocking synchronous send
      buf :: VAR_TYPE, INOUT
      dest :: INT, IN
      tag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc,mtag :: INT

      if (FALSE) self = self

      mtag = 1
      proc = dest
      

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_SSEND(buf,LEN,MPI_TYPE,proc,mtag,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
      proc = proc
      self = self
      mtag = mtag
      if(present(tag)) mtag = tag
      buf = buf
#endif

   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end

! =============================
! Non-blocking synchronous send
! =============================

   issend(buf,dest,tag,request) ::: template
   ! Starts a nonblocking synchronous send
      buf :: VAR_TYPE, INOUT
      dest :: INT, IN
      tag :: INT, IN, optional
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      requestuest,proc,mtag :: INT


      mtag = 1 
      proc = dest

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_ISEND(buf,LEN,MPI_TYPE,proc,mtag, &
      MPI_COMM_WORLD,requestuest,.mpi_error)
      if (present(request)) request = requestuest
#else
      if (FALSE) self = self
      DIE("wtf?")
      proc = proc
      request = request
      self = self
      requestuest =1 
      buf = buf
      if(present(tag)) mtag = tag
      if (present(request)) request = requestuest
#endif

   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

! =====================
! Blocking send/receive
! =====================

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: template
   ! Sends and receives a message
      self :: INOUT
      sendbuf :: VAR_TYPE, IN
      recvbuf :: VAR_TYPE, INOUT
      dest :: INT, IN
      source :: INT, IN, optional
      sendtag,recvtag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc1,proc2,mtag1,mtag2 :: INT
#ifdef MPI
      status :: VEC{INT}*
      ALLOCATE(status(MPI_STATUS_SIZE))
#endif      
      if (FALSE) self = self

      mtag1 = 1
      mtag2 = 1
      proc1 = dest
      proc2 = dest

#ifdef MPI
      if(present(sendtag)) mtag1 = sendtag
      if(present(recvtag)) mtag2 = recvtag
      if(present(source)) proc2 = source
      call MPI_SENDRECV(sendbuf,LEN1,MPI_TYPE,proc1,mtag1,recvbuf,LEN2,MPI_TYPE,proc2,mtag2,MPI_COMM_WORLD,status,.mpi_error)
      DEALLOCATE(status)
#else
      DIE("wtf?")
      self = self
      recvbuf = sendbuf
      if(present(sendtag)) mtag1 = sendtag
      if(present(recvtag)) mtag2 = recvtag
      if(present(source)) proc2 = source
#endif

   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN1=>len(sendbuf)), LEN2=>len(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

! ======
! Gather
! ======

   gather(sendbuf,recvbuf,root) ::: template
   ! Gather together values from a group of processes
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VEC{VAR_TYPE}, OUT, optional
      root :: INT, IN

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      recvtmp :: VEC{VAR_TYPE}@
#ifndef MPI
      dummy :: INT
#endif

#ifdef MPI
      if(present(recvbuf)) recvtmp = recvbuf
      call MPI_GATHER(sendbuf,LEN,MPI_TYPE,recvtmp,LEN,MPI_TYPE,root,MPI_COMM_WORLD,.mpi_error)
      if(present(recvbuf)) recvbuf = recvtmp
#else
      if (FALSE) self = self
      if (FALSE) sendbuf = sendbuf
      if (present(recvbuf)) recvbuf = recvbuf

      DIE("wtf?")
#endif

   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(sendbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gather together values from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gather together values from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gather together values from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gather together values from a group of processes
   end


   gather(sendbuf,recvbuf,root) ::: template
   ! Gather together arrays from a group of processes
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VAR_TYPE, OUT, optional
      root :: INT, IN

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      recvtmp :: VAR_TYPE@
#ifndef MPI
      dummy :: INT
#endif

#ifdef MPI
      if(present(recvbuf)) recvtmp = recvbuf
      call MPI_GATHER(sendbuf,LEN,MPI_TYPE,recvtmp,LEN,MPI_TYPE,root,MPI_COMM_WORLD,.mpi_error)
      if(present(recvbuf)) recvbuf = recvtmp
#else
      if (FALSE) self = self
      if (FALSE) sendbuf = sendbuf
      if (present(recvbuf)) recvbuf = recvbuf

      DIE("wtf?")
#endif

   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(sendbuf)*len(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end


   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(sendbuf)*len(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end


   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end


   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end


   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

   gather(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes
   end

! =======
! Scatter
! =======

   scatter(sendbuf,recvbuf,root) ::: template
   ! Sends data from one process to all other processes in a communicator
      sendbuf :: VEC{VAR_TYPE}, INOUT, optional
      recvbuf :: VAR_TYPE, OUT
      root :: INT, IN

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc :: INT
      sendtmp :: VEC{VAR_TYPE}@

#ifdef MPI
      if(present(sendbuf)) sendtmp = sendbuf
      call MPI_SCATTER(sendtmp,LEN,MPI_TYPE,recvbuf,LEN,MPI_TYPE,root,MPI_COMM_WORLD,.mpi_error)
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      proc = root

      proc = 0

      DIE("wtf?")
      proc = proc
#endif

   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: template
   ! Sends arrays from one process to all other processes in a communicator
      sendbuf :: VAR_TYPE, INOUT, optional
      recvbuf :: VAR_TYPE, OUT
      root :: INT, IN

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc :: INT
      sendtmp :: VAR_TYPE@

#ifdef MPI
      if(present(sendbuf)) sendtmp = sendbuf
      call MPI_SCATTER(sendtmp,LEN,MPI_TYPE,recvbuf,LEN,MPI_TYPE,root,MPI_COMM_WORLD,.mpi_error)
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      proc = root

      proc = 0
      recvbuf = sendbuf

      DIE("wtf?")
      proc = proc
#endif

   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends arrays from one process to all other processes in a communicator
   end

! ====================
! Non-blocking scatter
! ====================

   iscatter(sendbuf,recvbuf,root,request) ::: template
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VAR_TYPE, OUT
      root :: INT, IN
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      req,proc :: INT

#ifdef MPI
      call MPI_ISCATTER(sendbuf,LEN1,MPI_TYPE,recvbuf,LEN2,MPI_TYPE,root,MPI_COMM_WORLD,req,.mpi_error)
      if (present(request)) request = req
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      proc = root
      req = root

      if (present(request)) request = req
      proc = 0
      recvbuf = sendbuf

      DIE("wtf?")
      proc = proc
#endif

   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN1=>len(sendbuf)), LEN2=>len(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

! ===================
! Non-blocking gather
! ===================

   igather(sendbuf,recvbuf,root,request) ::: template
   ! Gather together values from a group of processes in a nonblocking way
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VEC{VAR_TYPE}, OUT, optional
      root :: INT, IN
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      recvtmp :: VEC{VAR_TYPE}@
      req :: INT
#ifndef MPI
      dummy :: INT
#endif

#ifdef MPI
      if(present(recvbuf)) recvtmp = recvbuf
      call MPI_IGATHER(sendbuf,LEN,MPI_TYPE,recvtmp,LEN,MPI_TYPE,root,MPI_COMM_WORLD,req,.mpi_error)
      if(present(recvbuf)) recvbuf = recvtmp
      if(present(request)) request = req
#else
      if (FALSE) self = self
      if (FALSE) sendbuf = sendbuf
      if (present(recvbuf)) recvbuf = recvbuf

      DIE("wtf?")
#endif

   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(sendbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,recvbuf,root,request) ::: template
   ! Gather together arrays from a group of processes in a nonblocking way
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VAR_TYPE, OUT, optional
      root :: INT, IN
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      recvtmp :: VAR_TYPE@
      req :: INT
#ifndef MPI
      dummy :: INT
#endif

#ifdef MPI
      if(present(recvbuf)) recvtmp = recvbuf
      call MPI_IGATHER(sendbuf,LEN,MPI_TYPE,recvtmp,LEN,MPI_TYPE,root,MPI_COMM_WORLD,req,.mpi_error)
      if(present(recvbuf)) recvbuf = recvtmp
      if(present(request)) request = req
#else
      if (FALSE) self = self
      if (FALSE) sendbuf = sendbuf
      if (present(recvbuf)) recvbuf = recvbuf

      DIE("wtf?")
#endif

   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(sendbuf)*len(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end


   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(sendbuf)*len(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end


   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end


   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end


   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

   igather(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gather together arrays from a group of processes in a nonblocking way
   end

! =========
! Allgather
! =========

   allgather(sendbuf,recvbuf) ::: template
   ! Gathers values from all tasks and distribute the combined values to all tasks
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VEC{VAR_TYPE}, OUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      recvtmp :: VEC{VAR_TYPE}@
#ifndef MPI
      dummy :: INT
#endif

#ifdef MPI
      recvtmp = recvbuf
      call MPI_ALLGATHER(sendbuf,LEN,MPI_TYPE,recvtmp,LEN,MPI_TYPE,MPI_COMM_WORLD,.mpi_error)
      recvbuf = recvtmp
#else
      if (FALSE) self = self
      if (FALSE) sendbuf = sendbuf

      DIE("wtf?")
#endif

   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(sendbuf))
   ! Gathers values from all tasks and distribute the combined values to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gathers values from all tasks and distribute the combined values to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gathers values from all tasks and distribute the combined values to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gathers values from all tasks and distribute the combined values to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gathers values from all tasks and distribute the combined values to all tasks
   end


   allgather(sendbuf,recvbuf) ::: template
   ! Gathers data from all tasks and distribute the combined data to all tasks
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VAR_TYPE, OUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      recvtmp :: VAR_TYPE@
#ifndef MPI
      dummy :: INT
#endif

#ifdef MPI
      recvtmp = recvbuf
      call MPI_ALLGATHER(sendbuf,LEN,MPI_TYPE,recvtmp,LEN,MPI_TYPE,MPI_COMM_WORLD,.mpi_error)
      recvbuf = recvtmp
#else
      if (FALSE) self = self
      if (FALSE) sendbuf = sendbuf

      DIE("wtf?")
#endif

   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(sendbuf)*len(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(sendbuf)*len(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

! ======================
! Non-blocking Allgather
! ======================

   iallgather(sendbuf,recvbuf,request) ::: template
   ! Gathers values from all tasks and distribute the combined values to all tasks in a nonblocking way
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VEC{VAR_TYPE}, OUT
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      recvtmp :: VEC{VAR_TYPE}@
      req :: INT
#ifndef MPI
      dummy :: INT
#endif

#ifdef MPI
      recvtmp = recvbuf
      call MPI_IALLGATHER(sendbuf,LEN,MPI_TYPE,recvtmp,LEN,MPI_TYPE,MPI_COMM_WORLD,req,.mpi_error)
      recvbuf = recvtmp
      if(present(request)) request = req
#else
      if (FALSE) self = self
      if (FALSE) sendbuf = sendbuf

      DIE("wtf?")
#endif

   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(sendbuf))
   ! Gathers values from all tasks and distribute the combined values to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gathers values from all tasks and distribute the combined values to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gathers values from all tasks and distribute the combined values to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gathers values from all tasks and distribute the combined values to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gathers values from all tasks and distribute the combined values to all tasks in a nonblocking way
   end


   iallgather(sendbuf,recvbuf,request) ::: template
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VAR_TYPE, OUT
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      recvtmp :: VAR_TYPE@
      req :: INT
#ifndef MPI
      dummy :: INT
#endif

#ifdef MPI
      recvtmp = recvbuf
      call MPI_IALLGATHER(sendbuf,LEN,MPI_TYPE,recvtmp,LEN,MPI_TYPE,MPI_COMM_WORLD,req,.mpi_error)
      recvbuf = recvtmp
      if(present(request)) request = req
#else
      if (FALSE) self = self
      if (FALSE) sendbuf = sendbuf

      DIE("wtf?")
#endif

   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(sendbuf)*len(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end


   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(sendbuf)*len(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end


   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end


   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end


   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

   iallgather(sendbuf,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(sendbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks in a nonblocking way
   end

! ==========
! All-to-all
! ==========

   alltoall(sendbuf,recvbuf,commsize) ::: template
   ! Sends data from all to all processes
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VAR_TYPE, OUT
      commsize :: INT, IN

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc :: INT

#ifdef MPI
      call MPI_ALLTOALL(sendbuf,LEN/commsize,MPI_TYPE,recvbuf,LEN/commsize,MPI_TYPE,MPI_COMM_WORLD,.mpi_error)
#else
      if (FALSE) self = self
      proc = 0

      proc = 0
      recvbuf = sendbuf

      DIE("wtf?")
      proc = proc
#endif

   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,recvbuf,commsize) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

! =======================
! Non-blocking all-to-all
! =======================

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: template
   ! Sends data from all to all processes in a non-blocking way
      sendbuf :: VAR_TYPE, INOUT
      sendcount :: INT, IN
      recvbuf :: VAR_TYPE, OUT
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      req :: INT

#ifdef MPI
      call MPI_IALLTOALL(sendbuf,sendcount,MPI_TYPE,recvbuf,LEN,MPI_TYPE,MPI_COMM_WORLD,req,.mpi_error)
      if(present(request)) request = req
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      req = sendcount
      if(present(request)) request = req

      recvbuf = sendbuf

      DIE("wtf?")
#endif

   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

! =========
! Broadcast
! =========

   broadcast(buffer,root) ::: template
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
      buffer :: VAR_TYPE
      root :: INT, IN, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc :: INT

#ifdef MPI
      proc = 0
      if (present(root)) proc = root
      ! Florian here: Added this Barrier to make sure that not BCAST interferes with a different kind leading to str and Int in asynchronous running MPI process to screw up communication
      !call MPI_BARRIER(MPI_COMM_WORLD,.mpi_error)
      call MPI_BCAST(buffer,LEN,MPI_TYPE,proc,MPI_COMM_WORLD,.mpi_error)
#else
      if (FALSE) self = self
      buffer = buffer

      proc = 0
      if (present(root)) proc = root
      DIE("wtf?")
      proc = proc
#endif

   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

! ======================
! Non-blocking Broadcast
! ======================

   ibroadcast(buffer,root,request) ::: template
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
      buffer :: VAR_TYPE
      root :: INT, IN, optional
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      req,proc :: INT


#ifdef MPI
      proc = 0
      if (present(root)) proc = root
      call MPI_IBCAST(buffer,LEN,MPI_TYPE,proc,MPI_COMM_WORLD,req,.mpi_error)
      if (present(request)) request = req
#else
      if (FALSE) self = self
      buffer = buffer

      proc = 0
      if (present(root)) proc = root
      req = 0
      if (present(request)) request = req
      DIE("wtf?")
      proc = proc
#endif

   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

! =============
! Window p_loop
! =============

   setup_p_loop(w,lb,ub) ::: template
   ! Set up locks and windows, return actual window index "w"
      self :: INOUT
      w :: INT, OUT
      lb,ub :: INT,optional, IN 

      ! Lock inner loops
      .lock_parallel_do("no")

      ! Set loop bounds
      .p_loop_lbound = lb
      .p_loop_ubound = ub

      ! Initialize start
      .p_loop_index = lb
      .win_create(.p_loop_index,1,0,w)

   end

   exit_p_loop result (res) ::: template, pure
   ! Return whether the p_loop should exit; this happens by
   ! convention if the loop index is set below its allowed
   ! lower bound.
      self :: IN
      res :: BIN

      res = .p_loop_index > .p_loop_ubound OR .p_loop_index < .p_loop_lbound 

   end

   next_p_loop_index(w) result (g) ::: template
   ! Return the next loop index in a parallel loop for window "w".
      !self :: INOUT
      w :: INT, IN
      g :: INT
      x :: INT

      x = 1

      ! Master has last say over window "w"
      ! This is a barrier to the first other than master.
      .win_lock_exclusive(.master_rank,w)

      if (NOT .is_master_processor) then
         .get_accumulate_sum(x,.p_loop_index,.master_rank,0,w)
      end

      ! Release window "w" for next processor.
      .win_unlock(.master_rank,w)

      ! Master and salve get the same "g"
      g = .p_loop_list(.p_loop_index)

   end

! =============
! Window Create
! =============

   win_create(base,disp_unit,info,win) ::: template
   ! Create an MPI Window object for one-sided communication
      self :: INOUT
      base :: VAR_TYPE, IN
      disp_unit,info :: INT, IN, optional
      win :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      d,i,w :: INT


#ifdef MPI
      d = 1
      i = 0
      if (present(disp_unit)) d = disp_unit
      if (present(info)) i = info
      call MPI_WIN_CREATE(base,sizeof(base),d,i,MPI_COMM_WORLD,w,.mpi_error)
      if (present(win)) win = w
#else
      w = 0
      if (w == 1) print*, base
      if (present(disp_unit)) d = disp_unit
      if (present(info)) i = info
      if (present(win)) w=0; win = w
      if (FALSE) self = self

      DIE("wtf?")
#endif

   end
   
   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>STR, LEN=>len(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, LEN=>1)
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, LEN=>1)
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, LEN=>1)
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, LEN=>1)
   ! Create an MPI Window object for one-sided communication
   end


   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, LEN=>size(base)*len(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end


   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, LEN=>size(base)*len(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end


   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end


   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end


   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

! =====================
! Window Create Dynamic
! =====================

   win_create_dynamic(info,win) ::: template
   ! Create an MPI Window object for one-sided communication. This
   ! window allows memory to be dynamically exposed and un-exposed
   ! for RMA operations.
      self :: INOUT
      info :: INT, IN, optional
      win :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      i :: INT

#ifdef MPI
      w :: INT
#endif


#ifdef MPI
      i = 0
      if (present(info)) i = info
      call MPI_WIN_CREATE_DYNAMIC(i,MPI_COMM_WORLD,w,.mpi_error)
      if (present(win)) win = w
#else
      if (present(info)) i = info
      if (present(win)) win = 0
      if (FALSE) self = self

      DIE("wtf?")
#endif

   end

! ===============
! Window Allocate
! ===============

   win_allocate(base,siz,disp_unit,info,win) ::: template
   ! Create and allocate an MPI Window object for one-sided communication
      self :: INOUT
#ifdef MPI
      base :: MPI_ADDRESS*, OUT 
      siz :: MPI_ADDRESS, IN, optional
#else
      base :: INT, OUT 
      siz :: INT, IN, optional
#endif
      disp_unit,info :: INT, IN, optional
      win :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

#ifdef MPI
      s :: MPI_ADDRESS 
#else
      s :: INT
#endif
      d,i,w :: INT


#ifdef MPI
      d = 0
      i = 0
      if (present(disp_unit)) d = disp_unit
      if (present(info)) i = info
      call MPI_WIN_ALLOCATE(s,d,i,MPI_COMM_WORLD,base,w,.mpi_error)
      if (present(win)) win = w
#else
      w = 0
      if (w == 1) print*, base
      if (present(siz)) s = siz
      if (present(disp_unit)) d = disp_unit
      if (present(info)) i = info
      if (present(win)) win = w
      if (FALSE) self = self

      DIE("wtf?")
#endif

   end

! ======================
! Window Allocate Shared
! ======================

   win_allocate_shared(base,siz,disp_unit,info,win) ::: template
   ! Create an MPI Window object for one-sided communication and
   ! shared memory access, and allocate memory at each process.
      self :: INOUT
#ifdef MPI
      base :: MPI_ADDRESS*, OUT 
      siz :: MPI_ADDRESS, IN, optional
#else
      base :: INT, OUT 
      siz :: INT, IN, optional
#endif
      disp_unit,info :: INT, IN, optional
      win :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

#ifdef MPI
      s :: MPI_ADDRESS 
#else
      s :: INT
#endif
      d,i,w :: INT


#ifdef MPI
      d = 0
      i = 0
      if (present(siz))  then
         s = siz
      else
         s = 0
      end if
      if (present(disp_unit)) d = disp_unit
      if (present(info)) i = info
      call MPI_WIN_ALLOCATE_SHARED(s,d,i,MPI_COMM_WORLD,base,w,.mpi_error)
      if (present(win)) win = w
#else
      w = 0
      if (w == 1) print*, base
      if (present(siz)) s = siz
      if (present(disp_unit)) d = disp_unit
      if (present(info)) i = info
      if (present(win)) win = w
      if (FALSE) self = self

      DIE("wtf?")
#endif

   end

! ===========
! Window Free
! ===========

   win_free(win) ::: template
   ! Free an MPI RMA window
      self :: INOUT
      win :: INT, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

#ifdef MPI
      call MPI_WIN_FREE(win,.mpi_error)
#else
      self = self
      win = 0
      DIE("wtf?")
#endif

   end

! ==================
! Window Lock/Unlock
! ==================

   win_lock_exclusive(rank,win) ::: template 
   ! Begin an RMA access epoch at the target process. 
      rank,win :: INT, IN
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
#ifndef _WIN32
      call MPI_WIN_LOCK(MPI_LOCK_EXCLUSIVE,rank,0,win,.mpi_error)
#endif
#else
      if (win == rank) DIE("wtf?")
      DIE("wtf?")
      self = self
#endif
   end

   win_lock_shared(rank,win) ::: template 
   ! Begin an RMA access epoch at the target process. 
      rank,win :: INT, IN
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
#ifndef _WIN32
      call MPI_WIN_LOCK(MPI_LOCK_SHARED,rank,0,win,.mpi_error)
#endif
#else
      if (win == rank) DIE("wtf?")
      self = self
#endif
   end

   win_unlock(rank,win) ::: template 
   ! Completes an RMA access epoch at the target process. 
      rank,win :: INT, IN
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      call MPI_WIN_UNLOCK(rank,win,.mpi_error)
#else
      if (win == rank) DIE("wtf?")
      self = self
#endif
   end

! ==========
! Window Put
! ==========

   put(origin_addr,target_rank,target_disp,win) ::: template
   ! Put data into a memory window on a remote process
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_PUT(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Put data into a memory window on a remote process
   end


   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(origin_addr)*len(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end


   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(origin_addr)*len(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end


   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end


   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end


   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

! ==========
! Window Get
! ==========

   get(origin_addr,target_rank,target_disp,win) ::: template
   ! Get data from a memory window on a remote process
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_GET(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Get data from a memory window on a remote process
   end


   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(origin_addr)*len(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end


   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(origin_addr)*len(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end


   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end


   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end


   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

! =================
! Window Accumulate
! =================

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: template
   ! Accumulate data using a maximum into the target process using remote memory access
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_ACCUMULATE(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,MPI_MAX,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: template
   ! Accumulate data using a minimum into the target process using remote memory access
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_ACCUMULATE(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,MPI_MIN,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: template
   ! Accumulate data using a sum into the target process using remote memory access
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_ACCUMULATE(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,MPI_SUM,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: template
   ! Accumulate data using a product into the target process using remote memory access
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_ACCUMULATE(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,MPI_PROD,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_land(origin_addr,target_rank,target_disp,win) ::: template
   ! Accumulate data using a logical and into the target process using remote memory access
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_ACCUMULATE(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,MPI_LAND,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   accumulate_land(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Accumulate data using a logical and into the target process using remote memory access
   end


   accumulate_land(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical and into the target process using remote memory access
   end


   accumulate_land(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical and into the target process using remote memory access
   end


   accumulate_land(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical and into the target process using remote memory access
   end


   accumulate_lor(origin_addr,target_rank,target_disp,win) ::: template
   ! Accumulate data using a logical or into the target process using remote memory access
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_ACCUMULATE(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,MPI_LOR,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   accumulate_lor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Accumulate data using a logical or into the target process using remote memory access
   end


   accumulate_lor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical or into the target process using remote memory access
   end


   accumulate_lor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical or into the target process using remote memory access
   end


   accumulate_lor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical or into the target process using remote memory access
   end


   accumulate_lxor(origin_addr,target_rank,target_disp,win) ::: template
   ! Accumulate data using a logical xor into the target process using remote memory access
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_ACCUMULATE(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,MPI_LXOR,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   accumulate_lxor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Accumulate data using a logical xor into the target process using remote memory access
   end


   accumulate_lxor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical xor into the target process using remote memory access
   end


   accumulate_lxor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical xor into the target process using remote memory access
   end


   accumulate_lxor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical xor into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: template
   ! Accumulate data using a maximum with location into the target process using remote memory access
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_ACCUMULATE(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,MPI_MAXLOC,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: template
   ! Accumulate data using a minimum with location into the target process using remote memory access
      origin_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_ACCUMULATE(origin_addr,LEN,MPI_TYPE,target_rank,d,LEN,MPI_TYPE,MPI_MINLOC,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

! =====================
! Window Get Accumulate
! =====================

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform an atomic, one-sided read-and-accumulate operation.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_GET_ACCUMULATE(origin_addr,LEN1,MPI_TYPE,result_addr,LEN2,MPI_TYPE,target_rank,d,LEN1,MPI_TYPE,MPI_MAX,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform an atomic, one-sided read-and-accumulate operation.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_GET_ACCUMULATE(origin_addr,LEN1,MPI_TYPE,result_addr,LEN2,MPI_TYPE,target_rank,d,LEN1,MPI_TYPE,MPI_MIN,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform an atomic, one-sided read-and-accumulate operation.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: INT, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT

      d = 0
      w = 0

#ifdef MPI

      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_GET_ACCUMULATE(origin_addr,LEN1,MPI_TYPE,result_addr,LEN2,MPI_TYPE,target_rank,d,LEN1,MPI_TYPE,MPI_SUM,w,.mpi_error)

#else

      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")

#endif

   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform an atomic, one-sided read-and-accumulate operation.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_GET_ACCUMULATE(origin_addr,LEN1,MPI_TYPE,result_addr,LEN2,MPI_TYPE,target_rank,d,LEN1,MPI_TYPE,MPI_PROD,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_land(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform an atomic, one-sided read-and-accumulate operation.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_GET_ACCUMULATE(origin_addr,LEN1,MPI_TYPE,result_addr,LEN2,MPI_TYPE,target_rank,d,LEN1,MPI_TYPE,MPI_LAND,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   get_accumulate_land(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_land(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_land(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_land(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_lor(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform an atomic, one-sided read-and-accumulate operation.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_GET_ACCUMULATE(origin_addr,LEN1,MPI_TYPE,result_addr,LEN2,MPI_TYPE,target_rank,d,LEN1,MPI_TYPE,MPI_LOR,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   get_accumulate_lor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_lor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_lor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_lor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_lxor(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform an atomic, one-sided read-and-accumulate operation.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_GET_ACCUMULATE(origin_addr,LEN1,MPI_TYPE,result_addr,LEN2,MPI_TYPE,target_rank,d,LEN1,MPI_TYPE,MPI_LXOR,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   get_accumulate_lxor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_lxor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_lxor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_lxor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform an atomic, one-sided read-and-accumulate operation.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_GET_ACCUMULATE(origin_addr,LEN1,MPI_TYPE,result_addr,LEN2,MPI_TYPE,target_rank,d,LEN1,MPI_TYPE,MPI_MAXLOC,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform an atomic, one-sided read-and-accumulate operation.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: MPI_ADDRESS, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_GET_ACCUMULATE(origin_addr,LEN1,MPI_TYPE,result_addr,LEN2,MPI_TYPE,target_rank,d,LEN1,MPI_TYPE,MPI_MINLOC,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end


   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

   get_accumulate_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>origin_addr.dim , LEN2=>result_addr.dim)
   ! Perform an atomic, one-sided read-and-accumulate operation.
   end

! ===================
! Window Fetch and Op
! ===================

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform one-sided read-modify-write.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: INT, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_FETCH_AND_OP(origin_addr,result_addr,MPI_TYPE,target_rank,d,MPI_MAX,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_max(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform one-sided read-modify-write.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: INT, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_FETCH_AND_OP(origin_addr,result_addr,MPI_TYPE,target_rank,d,MPI_MIN,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_min(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform one-sided read-modify-write.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: INT, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 1


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_FETCH_AND_OP(origin_addr,result_addr,MPI_TYPE,target_rank,d,MPI_SUM,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_sum(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform one-sided read-modify-write.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: INT, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_FETCH_AND_OP(origin_addr,result_addr,MPI_TYPE,target_rank,d,MPI_PROD,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_prod(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_land(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform one-sided read-modify-write.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: INT, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_FETCH_AND_OP(origin_addr,result_addr,MPI_TYPE,target_rank,d,MPI_LAND,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   fetch_and_op_land(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_land(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_land(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_land(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_lor(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform one-sided read-modify-write.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: INT, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_FETCH_AND_OP(origin_addr,result_addr,MPI_TYPE,target_rank,d,MPI_LOR,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   fetch_and_op_lor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_lor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_lor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_lor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_lxor(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform one-sided read-modify-write.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: INT, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_FETCH_AND_OP(origin_addr,result_addr,MPI_TYPE,target_rank,d,MPI_LXOR,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   fetch_and_op_lxor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_lxor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_lxor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_lxor(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform one-sided read-modify-write.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: INT, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_FETCH_AND_OP(origin_addr,result_addr,MPI_TYPE,target_rank,d,MPI_MAXLOC,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_maxloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: template
   ! Perform one-sided read-modify-write.
      origin_addr,result_addr :: VAR_TYPE, INOUT
      target_rank :: INT, IN
#ifdef MPI
      target_disp :: INT, IN, optional
#else
      target_disp :: INT, IN, optional
#endif
      win :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

#ifdef MPI
      d :: MPI_ADDRESS
#else
      d :: INT
#endif
      w :: INT
      d = 0
      w = 0


#ifdef MPI
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      call MPI_FETCH_AND_OP(origin_addr,result_addr,MPI_TYPE,target_rank,d,MPI_MINLOC,w,.mpi_error)
#else
      if (w == 1) print*, origin_addr, result_addr
      w = target_rank
      if(present(target_disp)) d = target_disp
      if(present(win)) w = win
      DIE("wtf?")
#endif

   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end


   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION)
   ! Perform one-sided read-modify-write.
   end

   fetch_and_op_minloc(origin_addr,result_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX)
   ! Perform one-sided read-modify-write.
   end

! Comment: there may be problems implementing higher order array
! broadcasts, because the interfaces may be missing from the MPI
! module implementation; we could work around this by using temporary
! one dimensional arrays, but that would be inefficient.

   IO_is_allowed result (res)
   ! Return whether or not this processor is allowed to do the I/O operation.
   ! Here, only the master processor does I/O in a parallel job.
      res :: BIN
      res = .is_master_processor OR (NOT .is_parallel)
   end

   set_max_n_skip_proc(n)
   ! Set the maximum number of processors that can be skipped when assigning a
   ! processor grid.
      self :: INOUT
      n :: INT, IN
      .max_n_skip_proc = n
   end

   get_2d_processor_grid(nprocx,nprocy,nproc)
   ! This routine returns how to divide up the processors into a two-dimensional
   ! grid so that the number of processors in each dimension is roughly the same.
   ! If the number of processors cannot be factored into two dimensions, then it
   ! starts ignoring cpus.
      self :: IN
      nprocx, nprocy :: INT, OUT
      nproc :: INT, IN, optional
      n_skip,i,nprocs :: INT
      found :: BIN

      if (present(nproc)) then
       nprocs = nproc
      else
       nprocs = .n_processors
      end

      ! Quick case
      if (nprocs < 4) then
       nprocx = nprocs
       nprocy = 1
       return
      end

      ENSURE(.max_n_skip_proc>=0,"max_n_skip_proc must be positive")
      ! How to break up grid of processors.
      ! Must have nprocx * nprocy = nprocs
      ! If they can't be factored nicely then try factoring less cpus.
      nprocs = nprocs + 1
      do n_skip = 0,min(.max_n_skip_proc,nprocs)
       nprocs = nprocs - 1
       nprocy = floor(sqrt(real(nproc,kind=REAL_KIND))) ! Start from the sqrt in
       ! case we have some nice big factors as well as small ones.
       found = FALSE
       do i=nprocy,1,-1
         nprocx = nproc/nprocy
         if (mod(nproc,nprocy)==0) then
           found = TRUE
           exit
         end
         if (nprocy<nprocx) exit ! Otherwise same test with x,y reversed.
       end
       if (found) exit
      end
   end

!  =====================
!  Scalapack/blacs stuff
!  =====================

   init_2d_proc_grid
   ! This routine initializes blacs for a two-dimensional processor grid.
      self :: INOUT

      ! Initialise Scalapack with 2d processor grid.
      .get_2d_processor_grid(.proc_grid_nrow,.proc_grid_ncol)
#ifdef SCALAPACK
      call sl_init(.blacs_2d_context, .proc_grid_nrow, .proc_grid_ncol)
#else
      .blacs_2d_context = 0
      .proc_grid_nrow = 1
      .proc_grid_ncol = 1
#endif

      ! Get my position in the global matrix.
#ifdef SCALAPACK
      call blacs_gridinfo(.blacs_2d_context, .proc_grid_nprow, &
             .proc_grid_npcol, .proc_grid_myrow, .proc_grid_mycol)
#else
      .proc_grid_myrow = 1
      .proc_grid_mycol = 1
#endif
   end

   done_proc_grid
   ! Uninitialize blacs.
      self :: INOUT
#ifdef SCALAPACK
      call blacs_gridexit( .blacs_2d_context )
      call blacs_exit( 1 ) ! 0 if you want it to shut down MPI
#else
      .blacs_2d_context = 0 ! so that the routine is not empty
#endif
   end

   n_this_row(n) result (res)
   ! Number of rows of a matrix of n rows assigned to this processor.
      self :: IN
      n :: INT, IN
      res :: INT
#ifdef SCALAPACK
      res = numroc(n, .proc_grid_bs, .proc_grid_myrow, 0, .proc_grid_nrow)
#else
      res = n
#endif
   end

   n_this_col(n)
   ! Number of columns of a matrix of n columns assigned to this processor.
      self :: IN
      n :: INT, IN
      res :: INT
#ifdef SCALAPACK
      res = numroc(n, .proc_grid_bs, .proc_grid_mycol, 0, .proc_grid_ncol)
#else
      res = n
#endif
   end

end
