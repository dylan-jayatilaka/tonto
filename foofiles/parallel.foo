module PARALLEL

#ifdef MPI
!  Note the capital USE, below which prevents the 
!  preprocessor from making the module a dependent
   USE mpi, only: MPI_CHARACTER
   USE mpi, only: MPI_LOGICAL
   USE mpi, only: MPI_INTEGER
   USE mpi, only: MPI_DOUBLE_PRECISION
   USE mpi, only: MPI_DOUBLE_COMPLEX
   USE mpi, only: MPI_COMM_WORLD
   USE mpi, only: MPI_SUM
   USE mpi, only: MPI_LOR
   USE mpi, only: MPI_STATUS_SIZE
   USE mpi, only: MPI_INIT
   USE mpi, only: MPI_COMM_DUP
   USE mpi, only: MPI_COMM_SIZE
   USE mpi, only: MPI_COMM_RANK
   USE mpi, only: MPI_FINALIZE
   USE mpi, only: MPI_ALLREDUCE
   USE mpi, only: MPI_BARRIER
   USE mpi, only: MPI_BCAST
   USE mpi, only: MPI_SEND
   USE mpi, only: MPI_RECV
   USE mpi, only: MPI_ISEND
   USE mpi, only: MPI_IRECV
#endif

  implicit none

contains

   initialize
   ! Initialise the parallel environment.
#ifdef MPI
      .is_parallel = TRUE
      .do_parallel_lock = " "
      .master_rank = 0
      call MPI_INIT(.mpi_error)
      call MPI_COMM_DUP(MPI_COMM_WORLD,.mpi_comm,.mpi_error)
      call MPI_COMM_SIZE(.mpi_comm,.n_processors,.mpi_error)
      call MPI_COMM_RANK(.mpi_comm,.processor_rank,.mpi_error)
#else
      .is_parallel = FALSE
      .do_parallel_lock = " "
      .master_rank = 0
      .n_processors = 1
      .processor_rank = 0
#endif
   end

   finalize
   ! Finalise the parallel environment.

      .is_parallel = FALSE
#ifdef MPI
      call MPI_FINALIZE(.mpi_error)
#endif

   end

! ================
! Inquiry routines
! ================

   is_master_processor result (res) ::: pure
   ! Return TRUE if this is the master processor. The index of the master
   ! processor is normally 0.
      self :: IN
      res :: BIN

      res = .processor_rank == .master_rank

   end

   master_processor result (res) ::: pure
   ! Return the index of the master processor, which is normally 0.
      self :: IN
      res :: INT

      res = .master_rank

   end

   this_processor result (res) ::: pure
   ! Return the index of this processor.  First index is zero!
      self :: IN
      res :: INT

      if (.is_parallel) then; res = .processor_rank
      else;                   res = 0
      end

   end

! ============================================================================
! Parallel do loops: simple parallel loops are implemented in the preprocessor
! ============================================================================

   parallel_do_start(first,stride) result (res) ::: pure
   ! Return the starting index to be used in a parallel do statement. The
   ! "first" index and the loop "stride" can optionally be supplied, if they are
   ! not equal to 1.  In this model, each processor skips through the whole
   ! length of the loop.  This should be load balanced --- assuming there is no
   ! systematic dependence in the work for each element of the loop which
   ! depends on a multiple of the number of processors. See the "do_stride"
   ! routine.
      self :: IN
      first,stride :: INT, IN, optional
      res :: INT

      f,s :: INT

      f = 1
      if (present(first)) f = first

      s = 1
      if (present(stride)) s = stride

      if (.do_in_parallel) then; res = f + s*.processor_rank
      else;                      res = f
      end

   end

   parallel_do_stride(stride) result (res) ::: pure
   ! Return the stride to be used in a parallel do statement.  The "stride"
   ! length can be optionally supplied, if it is not 1.
      self :: IN
      stride :: INT, IN, optional
      res :: INT

      s :: INT

      s = 1
      if (present(stride))       s = stride

      if (.do_in_parallel) then; res = s*.n_processors
      else;                      res = s
      end

   end

   do_in_parallel result (res) ::: pure
   ! Return TRUE if we are allowed to start to implement a given do-loop in
   ! parallel. NOTE: Once you have parallelised a loop, remember to prevent
   ! further parallelisation, using the ".lock_parallel_do" method. This is
   ! typically used as the first statement after the parallel do loop is
   ! entered.
      self :: IN
      res :: BIN

      res = .is_parallel AND .do_parallel_lock==" "

   end

   lock_parallel_do(name) ::: pure
   ! Sets the parallel do-loop lock to the "name" of the locking routine.
   ! Only the routine with the same "name" may unlock the loop. WARNING: This
   ! assumes that the names of the routines are all distinct. NOTE: It is
   ! currently an error if the routine is recursive. Nested parallel loops are
   ! OK but disabled.
      self :: INOUT
      name :: STR, IN

 !   ENSURE(name/=.do_parallel_lock,"recursive parallel routines not allowed")

      if (.do_parallel_lock==name) return
      if (.do_parallel_lock==" ") .do_parallel_lock = name

   end

   unlock_parallel_do(name) ::: pure
   ! Allow parallelisation again, if the name matches the original.
      self :: INOUT
      name :: STR, IN

      if (.do_parallel_lock==name) .do_parallel_lock = " "

   end

! ==================
! Summation routines
! ==================

   parallel_vector_sum(vec,dim) ::: template
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
      dim :: INT, IN
      vec :: VEC{X}(dim), INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: VEC{X}*

      vec = vec

      allocate(tmp(dim))
#ifdef MPI
      call MPI_ALLREDUCE(vec,tmp,dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      vec = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>BIN, Y=>MPI_LOGICAL)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>INT, Y=>MPI_INTEGER)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: template
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
      val :: XX, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

      tmp = val
#ifdef MPI
      call MPI_ALLREDUCE(val,tmp,1,YY,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      val = tmp

   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>BIN, YY=>MPI_LOGICAL)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(vec) ::: template
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
      vec :: VEC{X}, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: VEC{X}*

      vec = vec

      allocate(tmp(size(vec)))
#ifdef MPI
      call MPI_ALLREDUCE(vec,tmp,size(vec),Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      vec = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>BIN, Y=>MPI_LOGICAL)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>INT, Y=>MPI_INTEGER)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

! These matrix summations may not be implemented, because the fortran
! 90 interfaces might be missing from the MPICH MPI implementation; we
! could work around this by using temporary one dimensional arrays,
! but that would be inefficient.

   parallel_sum(mat) ::: template
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors.
      mat :: MAT{X}, INOUT

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: MAT{X}*

      mat = mat

      allocate(tmp(size(mat,1),size(mat,2)))
#ifdef MPI
      call MPI_ALLREDUCE(mat,tmp,mat.dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      mat = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors.
   end


   parallel_symmetric_sum(mat)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors. The matrix "mat" is assumed to be symmetric, and
   ! only the lower half of "mat" is summed; the upper triangle is forced to be
   ! the same as the lower triangle.
      mat :: MAT{REAL}, INOUT

   ENSURE(.is_parallel,"must be in a parallel environment")
   ENSURE(mat.dim1==mat.dim2,"mat must be square")

      invec,outvec :: VEC{REAL}*
      tri_size,dim :: INT

      dim = mat.dim1
      tri_size = dim*(dim+1)/2

      allocate(invec(tri_size))
      allocate(outvec(tri_size))
#ifdef MPI
      .compress_to_triangle(invec,mat)
      call MPI_ALLREDUCE(invec,outvec,tri_size,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      .uncompress_from_triangle(outvec,mat)
#else
      DIE("wtf?")
#endif
      deallocate(outvec)
      deallocate(invec)

   end


   parallel_sum(mat) ::: template
   ! This routine adds the versions of a 3d "mat" from all processors, and gives
   ! the result to all processors.
      mat :: MAT3{X}, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: MAT3{X}*

      allocate(tmp(mat.dim1,mat.dim2,mat.dim3))
#ifdef MPI
      call MPI_ALLREDUCE(mat,tmp,mat.dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      mat = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 3d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 3d "mat" from all processors, and gives
   ! the result to all processors.
   end


   parallel_sum(mat) ::: template
   ! This routine adds the versions of a 4d "mat" from all processors, and gives
   ! the result to all processors.
      mat :: MAT4{X}, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: MAT4{X}*

      allocate(tmp(mat.dim1,mat.dim2,mat.dim3,mat.dim4))
#ifdef MPI
      call MPI_ALLREDUCE(mat,tmp,mat.dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      mat = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 4d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 4d "mat" from all processors, and gives
   ! the result to all processors.
   end


   parallel_sum(mat) ::: template
   ! This routine adds the versions of a 5d "mat" from all processors, and gives
   ! the result to all processors.
      mat :: MAT5{X}, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: MAT5{X}*

      allocate(tmp(mat.dim1,mat.dim2,mat.dim3,mat.dim4,mat.dim5))
#ifdef MPI
      call MPI_ALLREDUCE(mat,tmp,mat.dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      mat = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 5d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 5d "mat" from all processors, and gives
   ! the result to all processors.
   end


! These should be inherited, but aren't, to decouple this
! module from any others.

   compress_to_triangle(tr,mat)
   ! Converts the lower triangle of matrix "mat" to the triangle "tr".
   ! using row order.
      mat :: MAT{REAL}
      tr :: VEC{REAL}

   ENSURE(mat.dim1==mat.dim2,"mat must be square")

      ij,i,j :: INT

      if (FALSE) self = self

      ij = 0
      do i = 1,mat.dim1
        do j = 1,i
           tr(ij+j) = mat(i,j)
        end
        ij = ij+i
      end

   end

   uncompress_from_triangle(tr,mat)
   ! Converts the triangle "tr" into the symmetric matrix "mat".
      tr :: VEC{REAL}
      mat :: MAT{REAL}

   ENSURE(mat.dim1==mat.dim2,"mat must be square")

      tmp :: REAL
      ij,i,j :: INT

      if (FALSE) self = self

      ij = 0
      do i = 1,size(mat,1)
        do j = 1,i
           tmp = tr(ij+j)
           mat(j,i) = tmp
           mat(i,j) = tmp
        end
        ij = ij+i
      end

   end

! ==============
! Logical Reduce
! ==============
   
   parallel_or(bin) ::: template
   ! This routine collects binary statements, and returns true if one
   ! binary statements is true.
      bin :: BIN, INOUT

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_ALLREDUCE(bin,tmp,1,MPI_LOGICAL,MPI_LOR,MPI_COMM_WORLD,.mpi_error)
      bin = tmp
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

! =======================
! Barrier Synchronization
! =======================

   barrier ::: template 
   ! Sets up a barrier for some communicator.
   self :: INOUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
      
#ifdef MPI
      call MPI_BARRIER(MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
      self = self
#endif

   end

   parallel_wait(request) ::: template 
   ! Stalls a processor until a request is complete.
      request :: INT, INOUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      status :: VEC{INT}*
      ALLOCATE(status(MPI_STATUS_SIZE))
#endif      
      
#ifdef MPI
      call MPI_WAIT(request,status,.mpi_error)
      DEALLOCATE(status)
#else
      DIE("wtf?")
      self = self
      request = request
#endif
   end

   waitall(count,requests) ::: template 
   ! Stalls all processors until all requests are complete.
      count :: INT, IN
      requests :: VEC{INT}, IN
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      status :: VEC{INT}*
      ALLOCATE(status(MPI_STATUS_SIZE))
#endif      
      
#ifdef MPI
      call MPI_WAITALL(count,requests,status,.mpi_error)
      DEALLOCATE(status)
#else
      DIE("wtf?")
      self = self
#endif

   end

! ==============================================
! Non-blocking send variable to other processors
! ==============================================

   isend(var,processor,tag,req) ::: template
   ! Asynchronous send variable "var" from "processor" to some other processor.
      var :: VAR_TYPE, INOUT
      processor :: INT, IN
      tag :: INT, IN, optional
      req :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      request,proc,mtag :: INT

      if (FALSE) self = self

      mtag = processor
      proc = processor

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_ISEND(var,LEN,MPI_TYPE,proc,mtag, &
      MPI_COMM_WORLD,request,.mpi_error)
      if (present(req)) req = request
#else
      DIE("wtf?")
      proc = proc
      req = req
      self = self
      request =1 
      var = var
      if(present(tag)) mtag = tag
      if (present(req)) req = request
#endif

   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end


   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end


   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end


   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end


   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end


   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

! ===================================================
! Non-blocking receive variable from other processors
! ===================================================

   irecv(var,processor,tag,req) ::: template
   ! Asynchronously receive variable "var" from "processor".
      var :: VAR_TYPE, INOUT
      processor :: INT, IN
      tag :: INT, IN, optional
      req :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      request,proc,mtag :: INT

      if (FALSE) self = self

      mtag = processor
      proc = processor

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_IRECV(var,LEN,MPI_TYPE,proc,mtag,MPI_COMM_WORLD, &
      request,.mpi_error)
      if(present(req)) req = request
#else
      DIE("wtf?")
      var = var
      proc = proc
      req = req
      self = self
      request = 1
      mtag = mtag
      if(present(tag)) mtag = tag
      if (present(req)) req = request
#endif

   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Asynchronously receive variable "var" from "processor".
   end


   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end


   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end


   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end


   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
!   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end


   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

! ==========================================
! Blocking send variable to other processors
! ==========================================

   send(var,processor,tag) ::: template
   ! Send "var" from this processor to some other "processor".
      var :: VAR_TYPE, INOUT
      processor :: INT, IN
      tag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc,mtag :: INT

      if (FALSE) self = self

      mtag = 1
      proc = processor
      

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_SSEND(var,LEN,MPI_TYPE,proc,mtag,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
      proc = proc
      self = self
      mtag = mtag
      if(present(tag)) mtag = tag
      var = var
#endif

   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Send "var" from this processor to some other "processor".
   end


   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end


   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end


   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end


   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end


   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

! ===============================================
! Blocking receive variable from other processors
! ===============================================

   recv(var,processor,tag) ::: template
   ! Receive variable "var" from "processor".
      self :: INOUT
      var :: VAR_TYPE, OUT
      processor :: INT, IN
      tag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc,mtag :: INT
#ifdef MPI
      status :: VEC{INT}*
      ALLOCATE(status(MPI_STATUS_SIZE))
#endif      
      if (FALSE) self = self

      mtag = 1
      proc = processor

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_RECV(var,LEN,MPI_TYPE,proc,mtag, &
      MPI_COMM_WORLD,status,.mpi_error)
      DEALLOCATE(status)
#else
      DIE("wtf?")
      var = var
      proc = proc
      self = self
      mtag = mtag
      if(present(tag)) mtag = tag
#endif

   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Receive variable "var" from "processor".
   end


   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end


   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end


   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end


   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end


   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

! =====================
! Blocking send/receive
! =====================

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: template
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
      self :: INOUT
      vars :: VAR_TYPE, IN
      varr :: VAR_TYPE, INOUT
      processor :: INT, IN
      processorr :: INT, IN, optional
      tags,tagr :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc1,proc2,mtag1,mtag2 :: INT
#ifdef MPI
      status :: VEC{INT}*
      ALLOCATE(status(MPI_STATUS_SIZE))
#endif      
      if (FALSE) self = self

      mtag1 = 1
      mtag2 = 1
      proc1 = processor
      proc2 = processor

#ifdef MPI
      if(present(tags)) mtag1 = tags
      if(present(tagr)) mtag2 = tagr
      if(present(processorr)) proc2 = processorr
      call MPI_SENDRECV(vars,LEN1,MPI_TYPE,proc1,mtag1,varr,LEN2,MPI_TYPE,proc2,mtag2,MPI_COMM_WORLD,status,.mpi_error)
      DEALLOCATE(status)
#else
      DIE("wtf?")
      self = self
      varr = vars
      if(present(tags)) mtag1 = tags
      if(present(tagr)) mtag2 = tagr
      if(present(processorr)) proc2 = processorr
#endif

   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN1=>len(vars)), LEN2=>len(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end


   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(vars)*len(vars)), LEN2=>size(varr)*len(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end


   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(vars)*len(vars)), LEN2=>size(varr)*len(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end


   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end


   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end


   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

! =====================================
! Broadcast variables to all processors
! =====================================

   broadcast(var,processor) ::: template
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc :: INT

      if (FALSE) self = self
      var = var

      proc = 0
      if (present(processor)) proc = processor

#ifdef MPI
      call MPI_BCAST(var,LEN,MPI_TYPE,proc,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
      proc = proc
#endif

   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Broadcast variable "var" from "processor" to all the others.
   end


   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end


   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end


   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end


   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end


   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

! Comment: there may be problems implementing higher order array
! broadcasts, because the interfaces may be missing from the MPI
! module implementation; we could work around this by using temporary
! one dimensional arrays, but that would be inefficient.

   IO_is_allowed result (res)
   ! Return whether or not this processor is allowed to do the I/O operation.
   ! Here, only the master processor does I/O in a parallel job.
      res :: BIN
      res = .is_master_processor OR (NOT .is_parallel)
   end

   set_max_n_skip_proc(n)
   ! Set the maximum number of processors that can be skipped when assigning a
   ! processor grid.
      self :: INOUT
      n :: INT, IN
      .max_n_skip_proc = n
   end

   get_2d_processor_grid(nprocx,nprocy,nproc)
   ! This routine returns how to divide up the processors into a two-dimensional
   ! grid so that the number of processors in each dimension is roughly the same.
   ! If the number of processors cannot be factored into two dimensions, then it
   ! starts ignoring cpus.
      self :: IN
      nprocx, nprocy :: INT, OUT
      nproc :: INT, IN, optional
      n_skip,i,nprocs :: INT
      found :: BIN

      if (present(nproc)) then
       nprocs = nproc
      else
       nprocs = .n_processors
      end

      ! Quick case
      if (nprocs < 4) then
       nprocx = nprocs
       nprocy = 1
       return
      end

      ENSURE(.max_n_skip_proc>=0,"max_n_skip_proc must be positive")
      ! How to break up grid of processors.
      ! Must have nprocx * nprocy = nprocs
      ! If they can't be factored nicely then try factoring less cpus.
      nprocs = nprocs + 1
      do n_skip = 0,min(.max_n_skip_proc,nprocs)
       nprocs = nprocs - 1
       nprocy = floor(sqrt(real(nproc,kind=REAL_KIND))) ! Start from the sqrt in
       ! case we have some nice big factors as well as small ones.
       found = FALSE
       do i=nprocy,1,-1
         nprocx = nproc/nprocy
         if (mod(nproc,nprocy)==0) then
           found = TRUE
           exit
         end
         if (nprocy<nprocx) exit ! Otherwise same test with x,y reversed.
       end
       if (found) exit
      end
   end

!  =====================
!  Scalapack/blacs stuff
!  =====================

   init_2d_proc_grid
   ! This routine initializes blacs for a two-dimensional processor grid.
      self :: INOUT

      ! Initialise Scalapack with 2d processor grid.
      .get_2d_processor_grid(.proc_grid_nrow,.proc_grid_ncol)
#ifdef SCALAPACK
      call sl_init(.blacs_2d_context, .proc_grid_nrow, .proc_grid_ncol)
#else
      .blacs_2d_context = 0
      .proc_grid_nrow = 1
      .proc_grid_ncol = 1
#endif

      ! Get my position in the global matrix.
#ifdef SCALAPACK
      call blacs_gridinfo(.blacs_2d_context, .proc_grid_nprow, &
             .proc_grid_npcol, .proc_grid_myrow, .proc_grid_mycol)
#else
      .proc_grid_myrow = 1
      .proc_grid_mycol = 1
#endif
   end

   done_proc_grid
   ! Uninitialize blacs.
      self :: INOUT
#ifdef SCALAPACK
      call blacs_gridexit( .blacs_2d_context )
      call blacs_exit( 1 ) ! 0 if you want it to shut down MPI
#else
      .blacs_2d_context = 0 ! so that the routine is not empty
#endif
   end

   n_this_row(n) result (res)
   ! Number of rows of a matrix of n rows assigned to this processor.
      self :: IN
      n :: INT, IN
      res :: INT
#ifdef SCALAPACK
      res = numroc(n, .proc_grid_bs, .proc_grid_myrow, 0, .proc_grid_nrow)
#else
      res = n
#endif
   end

   n_this_col(n)
   ! Number of columns of a matrix of n columns assigned to this processor.
      self :: IN
      n :: INT, IN
      res :: INT
#ifdef SCALAPACK
      res = numroc(n, .proc_grid_bs, .proc_grid_mycol, 0, .proc_grid_ncol)
#else
      res = n
#endif
   end

end
