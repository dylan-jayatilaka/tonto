module PARALLEL

#ifdef MPI
!  Note the capital USE, below which prevents the 
!  preprocessor from making the module a dependent

!  MPI datatypes  
   USE mpi, only: MPI_CHARACTER
   USE mpi, only: MPI_LOGICAL
   USE mpi, only: MPI_INTEGER
   USE mpi, only: MPI_DOUBLE_PRECISION
   USE mpi, only: MPI_DOUBLE_COMPLEX
! MPI communicator routines  
   USE mpi, only: MPI_COMM_WORLD
   USE mpi, only: MPI_COMM_DUP
   USE mpi, only: MPI_COMM_SIZE
   USE mpi, only: MPI_COMM_RANK
! MPI gather functions   
   USE mpi, only: MPI_SUM
   USE mpi, only: MPI_LOR
   USE mpi, only: MPI_ALLREDUCE
   USE mpi, only: MPI_BCAST
   USE mpi, only: MPI_IBCAST
   USE mpi, only: MPI_GATHER
   USE mpi, only: MPI_IGATHER
   USE mpi, only: MPI_ALLGATHER
   USE mpi, only: MPI_IALLGATHER
   USE mpi, only: MPI_ALLTOALL
   USE mpi, only: MPI_IALLTOALL
! MPI scatter functions   
   USE mpi, only: MPI_SCATTER
   USE mpi, only: MPI_ISCATTER
! MPI status   
   USE mpi, only: MPI_STATUS_SIZE
   USE mpi, only: MPI_PROBE
   USE mpi, only: MPI_IPROBE
   USE mpi, only: MPI_TEST
   USE mpi, only: MPI_TESTANY
   USE mpi, only: MPI_TEST_CANCELLED
! MPI initialise and finalise   
   USE mpi, only: MPI_INIT
   USE mpi, only: MPI_FINALIZE
! MPI Barrier   
   USE mpi, only: MPI_BARRIER
   USE mpi, only: MPI_IBARRIER
   USE mpi, only: MPI_WAIT
   USE mpi, only: MPI_WAITANY
!   USE mpi, only: MPI_WAITALL
! MPI Send and receives   
   USE mpi, only: MPI_SEND
   USE mpi, only: MPI_RECV
   USE mpi, only: MPI_ISEND
   USE mpi, only: MPI_IRECV
   USE mpi, only: MPI_SSEND
   USE mpi, only: MPI_ISSEND
   USE mpi, only: MPI_SENDRECV
#endif

  implicit none

contains

   initialize
   ! Initialise the parallel environment.
#ifdef MPI
      .is_parallel = TRUE
      .do_parallel_lock = " "
      .master_rank = 0
      call MPI_INIT(.mpi_error)
      call MPI_COMM_DUP(MPI_COMM_WORLD,.mpi_comm,.mpi_error)
      call MPI_COMM_SIZE(.mpi_comm,.n_processors,.mpi_error)
      call MPI_COMM_RANK(.mpi_comm,.processor_rank,.mpi_error)
#else
      .is_parallel = FALSE
      .do_parallel_lock = " "
      .master_rank = 0
      .n_processors = 1
      .processor_rank = 0
#endif
   end

   finalize
   ! Finalise the parallel environment.

      .is_parallel = FALSE
#ifdef MPI
      call MPI_FINALIZE(.mpi_error)
#endif

   end

! ================
! Inquiry routines
! ================

   is_master_processor result (res) ::: pure
   ! Return TRUE if this is the master processor. The index of the master
   ! processor is normally 0.
      self :: IN
      res :: BIN

      res = .processor_rank == .master_rank

   end

   master_processor result (res) ::: pure
   ! Return the index of the master processor, which is normally 0.
      self :: IN
      res :: INT

      res = .master_rank

   end

   this_processor result (res) ::: pure
   ! Return the index of this processor.  First index is zero!
      self :: IN
      res :: INT

      if (.is_parallel) then; res = .processor_rank
      else;                   res = 0
      end

   end

! ============================================================================
! Parallel do loops: simple parallel loops are implemented in the preprocessor
! ============================================================================

   parallel_do_start(first,stride) result (res) ::: pure
   ! Return the starting index to be used in a parallel do statement. The
   ! "first" index and the loop "stride" can optionally be supplied, if they are
   ! not equal to 1.  In this model, each processor skips through the whole
   ! length of the loop.  This should be load balanced --- assuming there is no
   ! systematic dependence in the work for each element of the loop which
   ! depends on a multiple of the number of processors. See the "do_stride"
   ! routine.
      self :: IN
      first,stride :: INT, IN, optional
      res :: INT

      f,s :: INT

      f = 1
      if (present(first)) f = first

      s = 1
      if (present(stride)) s = stride

      if (.do_in_parallel) then; res = f + s*.processor_rank
      else;                      res = f
      end

   end

   parallel_do_stride(stride) result (res) ::: pure
   ! Return the stride to be used in a parallel do statement.  The "stride"
   ! length can be optionally supplied, if it is not 1.
      self :: IN
      stride :: INT, IN, optional
      res :: INT

      s :: INT

      s = 1
      if (present(stride))       s = stride

      if (.do_in_parallel) then; res = s*.n_processors
      else;                      res = s
      end

   end

   do_in_parallel result (res) ::: pure
   ! Return TRUE if we are allowed to start to implement a given do-loop in
   ! parallel. NOTE: Once you have parallelised a loop, remember to prevent
   ! further parallelisation, using the ".lock_parallel_do" method. This is
   ! typically used as the first statement after the parallel do loop is
   ! entered.
      self :: IN
      res :: BIN

      res = .is_parallel AND .do_parallel_lock==" "

   end

   lock_parallel_do(name) ::: pure
   ! Sets the parallel do-loop lock to the "name" of the locking routine.
   ! Only the routine with the same "name" may unlock the loop. WARNING: This
   ! assumes that the names of the routines are all distinct. NOTE: It is
   ! currently an error if the routine is recursive. Nested parallel loops are
   ! OK but disabled.
      self :: INOUT
      name :: STR, IN

 !   ENSURE(name/=.do_parallel_lock,"recursive parallel routines not allowed")

      if (.do_parallel_lock==name) return
      if (.do_parallel_lock==" ") .do_parallel_lock = name

   end

   unlock_parallel_do(name) ::: pure
   ! Allow parallelisation again, if the name matches the original.
      self :: INOUT
      name :: STR, IN

      if (.do_parallel_lock==name) .do_parallel_lock = " "

   end

! ==================
! Summation routines
! ==================

   parallel_vector_sum(vec,dim) ::: template
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
      dim :: INT, IN
      vec :: VEC{X}(dim), INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: VEC{X}*

      vec = vec

      allocate(tmp(dim))
#ifdef MPI
      call MPI_ALLREDUCE(vec,tmp,dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      vec = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>BIN, Y=>MPI_LOGICAL)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>INT, Y=>MPI_INTEGER)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: template
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
      val :: XX, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: XX

      tmp = val
#ifdef MPI
      call MPI_ALLREDUCE(val,tmp,1,YY,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
#endif
      val = tmp

   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>BIN, YY=>MPI_LOGICAL)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(vec) ::: template
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
      vec :: VEC{X}, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: VEC{X}*

      vec = vec

      allocate(tmp(size(vec)))
#ifdef MPI
      call MPI_ALLREDUCE(vec,tmp,size(vec),Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      vec = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>BIN, Y=>MPI_LOGICAL)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>INT, Y=>MPI_INTEGER)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

! These matrix summations may not be implemented, because the fortran
! 90 interfaces might be missing from the MPICH MPI implementation; we
! could work around this by using temporary one dimensional arrays,
! but that would be inefficient.

   parallel_sum(mat) ::: template
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors.
      mat :: MAT{X}, INOUT

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: MAT{X}*

      mat = mat

      allocate(tmp(size(mat,1),size(mat,2)))
#ifdef MPI
      call MPI_ALLREDUCE(mat,tmp,mat.dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      mat = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors.
   end


   parallel_symmetric_sum(mat)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors. The matrix "mat" is assumed to be symmetric, and
   ! only the lower half of "mat" is summed; the upper triangle is forced to be
   ! the same as the lower triangle.
      mat :: MAT{REAL}, INOUT

   ENSURE(.is_parallel,"must be in a parallel environment")
   ENSURE(mat.dim1==mat.dim2,"mat must be square")

      invec,outvec :: VEC{REAL}*
      tri_size,dim :: INT

      dim = mat.dim1
      tri_size = dim*(dim+1)/2

      allocate(invec(tri_size))
      allocate(outvec(tri_size))
#ifdef MPI
      .compress_to_triangle(invec,mat)
      call MPI_ALLREDUCE(invec,outvec,tri_size,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      .uncompress_from_triangle(outvec,mat)
#else
      DIE("wtf?")
#endif
      deallocate(outvec)
      deallocate(invec)

   end


   parallel_sum(mat) ::: template
   ! This routine adds the versions of a 3d "mat" from all processors, and gives
   ! the result to all processors.
      mat :: MAT3{X}, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: MAT3{X}*

      allocate(tmp(mat.dim1,mat.dim2,mat.dim3))
#ifdef MPI
      call MPI_ALLREDUCE(mat,tmp,mat.dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      mat = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 3d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 3d "mat" from all processors, and gives
   ! the result to all processors.
   end


   parallel_sum(mat) ::: template
   ! This routine adds the versions of a 4d "mat" from all processors, and gives
   ! the result to all processors.
      mat :: MAT4{X}, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: MAT4{X}*

      allocate(tmp(mat.dim1,mat.dim2,mat.dim3,mat.dim4))
#ifdef MPI
      call MPI_ALLREDUCE(mat,tmp,mat.dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      mat = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 4d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 4d "mat" from all processors, and gives
   ! the result to all processors.
   end


   parallel_sum(mat) ::: template
   ! This routine adds the versions of a 5d "mat" from all processors, and gives
   ! the result to all processors.
      mat :: MAT5{X}, INOUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      tmp :: MAT5{X}*

      allocate(tmp(mat.dim1,mat.dim2,mat.dim3,mat.dim4,mat.dim5))
#ifdef MPI
      call MPI_ALLREDUCE(mat,tmp,mat.dim,Y,MPI_SUM,MPI_COMM_WORLD,.mpi_error)
      mat = tmp
#else
      DIE("wtf?")
#endif
      deallocate(tmp)

   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 5d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 5d "mat" from all processors, and gives
   ! the result to all processors.
   end


! These should be inherited, but aren't, to decouple this
! module from any others.

   compress_to_triangle(tr,mat)
   ! Converts the lower triangle of matrix "mat" to the triangle "tr".
   ! using row order.
      mat :: MAT{REAL}
      tr :: VEC{REAL}

   ENSURE(mat.dim1==mat.dim2,"mat must be square")

      ij,i,j :: INT

      if (FALSE) self = self

      ij = 0
      do i = 1,mat.dim1
        do j = 1,i
           tr(ij+j) = mat(i,j)
        end
        ij = ij+i
      end

   end

   uncompress_from_triangle(tr,mat)
   ! Converts the triangle "tr" into the symmetric matrix "mat".
      tr :: VEC{REAL}
      mat :: MAT{REAL}

   ENSURE(mat.dim1==mat.dim2,"mat must be square")

      tmp :: REAL
      ij,i,j :: INT

      if (FALSE) self = self

      ij = 0
      do i = 1,size(mat,1)
        do j = 1,i
           tmp = tr(ij+j)
           mat(j,i) = tmp
           mat(i,j) = tmp
        end
        ij = ij+i
      end

   end

! ==============
! Logical Reduce
! ==============
   
   parallel_or(bin) ::: template
   ! This routine collects binary statements, and returns true if one
   ! binary statements is true.
      bin :: BIN, INOUT

   ENSURE(.is_parallel,"must be in a parallel environment")

      tmp :: BIN

#ifdef MPI
      call MPI_ALLREDUCE(bin,tmp,1,MPI_LOGICAL,MPI_LOR,MPI_COMM_WORLD,.mpi_error)
      bin = tmp
#else
      bin = bin
      tmp = bin
      self = self
      DIE("wtf?")
#endif

   end

! =======
! Barrier
! =======

   barrier ::: template 
   ! Blocks until all processes in the communicator have reached this routine.
   self :: INOUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
      
#ifdef MPI
      call MPI_BARRIER(MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
      self = self
#endif

   end

   ibarrier(request) ::: template 
   ! Notifies the process that it has reached the barrier and returns immediately
   self :: INOUT
   request :: INT, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")
   req :: INT   
      
#ifdef MPI
      call MPI_IBARRIER(MPI_COMM_WORLD,req,.mpi_error)
      if(present(request)) request = req
#else
      req = 0
      if(present(request)) request = req
      DIE("wtf?")
      self = self
#endif

   end

! ====
! Wait
! ====

   parallel_wait(request) ::: template 
   ! Waits for an MPI request to complete
      request :: INT, INOUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      status :: VEC{INT}(6)
#endif      
      
#ifdef MPI
      call MPI_WAIT(request,status,.mpi_error)
#else
      DIE("wtf?")
      self = self
      request = request
#endif
   end

   waitall(count,array_of_requests) ::: template 
   ! Waits for all given MPI Requests to complete
      count :: INT, INOUT
      array_of_requests :: VEC{INT}*, INOUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
     ! status :: VEC{INT}*
      statuses :: VEC{INT}(6*array_of_requests.dim)
     ! do i = 1,array_of_requests.dim
     !    ALLOCATE(statuses(i).element(MPI_STATUS_SIZE))
     ! end do
    !  ALLOCATE(statuses(:).element(MPI_STATUS_SIZE))

#endif      
      
#ifdef MPI
      call MPI_WAITALL(count,array_of_requests,statuses,.mpi_error)
      !DEALLOCATE(statuses)
#else
      array_of_requests(1) = count
      DIE("wtf?")
      self = self
#endif

   end

   waitany(count,array_of_requests,indx) ::: template 
   ! Waits for any specified MPI Request to complete
      count :: INT, IN
      array_of_requests :: VEC{INT}, INOUT
      indx :: INT, OUT, optional
   ENSURE(.is_parallel,"must be in a parallel environment")
#ifdef MPI
      index :: INT
      status :: VEC{INT}*
      ALLOCATE(status(MPI_STATUS_SIZE))
#endif      
      
#ifdef MPI
      call MPI_WAITANY(count,array_of_requests,index,status,.mpi_error)
      if(present(indx)) indx = index
      DEALLOCATE(status)
#else
      array_of_requests(1) = count
      if(present(indx)) indx = count
      DIE("wtf?")
      self = self
#endif

   end

! =====
! Probe
! =====

   probe(source,tag,status) ::: template 
   ! Blocking test for a message
      source :: INT, IN
      tag :: INT, IN, optional
#ifdef MPI
      status :: VEC{INT}(MPI_STATUS_SIZE), OUT
#else
      status :: VEC{INT}(6), OUT
#endif
   ENSURE(.is_parallel,"must be in a parallel environment")
      mtag :: INT
      mtag = 1
      if(present(tag)) mtag = tag
#ifdef MPI
      call MPI_PROBE(source,mtag,MPI_COMM_WORLD,status,.mpi_error)
#else
      status(1)=source
#endif
   end

   iprobe(source,tag,flag,status) ::: template 
   ! Nonblocking test for a message
      source :: INT, IN
      tag :: INT, IN, optional
      flag :: BIN, OUT
#ifdef MPI
      status :: VEC{INT}(MPI_STATUS_SIZE), OUT, optional
#else
      status :: VEC{INT}(6), OUT
#endif
   ENSURE(.is_parallel,"must be in a parallel environment")
      mtag :: INT
      mtag = 1
      if(present(tag)) mtag = tag
#ifdef MPI
      call MPI_IPROBE(source,mtag,MPI_COMM_WORLD,flag,status,.mpi_error)
#else
      flag = TRUE
      status(1)=source
#endif
   end

! ====
! Test
! ====

   test(request,flag) ::: template 
   ! Tests for the completion of a request
      request :: INT, INOUT
      flag :: BIN, OUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      status :: VEC{INT}(MPI_STATUS_SIZE)
#endif      
      
#ifdef MPI
      call MPI_TEST(request,flag,status,.mpi_error)
#else
      DIE("wtf?")
      flag = TRUE
      self = self
      request = request
#endif
   end

   testany(count,array_of_requests,indx,flag) ::: template 
   ! Tests for the completion of any previously initiated array_of_requests
      count :: INT, IN
      array_of_requests :: VEC{INT}, INOUT
      indx :: INT, OUT
      flag :: BIN, OUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      status :: VEC{INT}(MPI_STATUS_SIZE)
#endif      
      
#ifdef MPI
      call MPI_TESTANY(count,array_of_requests,indx,flag,status,.mpi_error)
#else
      DIE("wtf?")
      flag = TRUE
      indx = count
      self = self
      array_of_requests = array_of_requests
#endif
   end

   test_cancelled(status,flag) ::: template 
   ! Tests to see if a request was cancelled
      flag :: BIN, OUT
   ENSURE(.is_parallel,"must be in a parallel environment")
#ifdef MPI
      status :: VEC{INT}(MPI_STATUS_SIZE), IN
#else
      status :: VEC{INT}(6), IN
      i :: INT
#endif      
      
#ifdef MPI
      call MPI_TEST_CANCELLED(status,flag,.mpi_error)
#else
      DIE("wtf?")
      flag = TRUE
      self = self
      i = status(1)
#endif
   end

! ======
! Cancel
! ======

   cancel(request) ::: template 
   ! Cancels a communication request
      request :: INT, INOUT
   ENSURE(.is_parallel,"must be in a parallel environment")
      
#ifdef MPI
      call MPI_CANCEL(request,.mpi_error)
      !DEALLOCATE(status)
#else
      DIE("wtf?")
      self = self
      request = request
#endif
   end

! =============
! Blocking send
! =============

   send(buf,dest,tag) ::: template
   ! Performs a blocking send
      buf :: VAR_TYPE, INOUT
      dest :: INT, IN
      tag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc,mtag :: INT

      if (FALSE) self = self

      mtag = 1
      proc = dest
      

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_SEND(buf,LEN,MPI_TYPE,proc,mtag,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
      proc = proc
      self = self
      mtag = mtag
      if(present(tag)) mtag = tag
      buf = buf
#endif

   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end

! ================
! Blocking receive
! ================

   recv(buf,source,tag) ::: template
   ! Blocking receive for a message
      self :: INOUT
      buf :: VAR_TYPE, OUT
      source :: INT, IN
      tag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc,mtag :: INT
#ifdef MPI
      status :: VEC{INT}*
      ALLOCATE(status(MPI_STATUS_SIZE))
#endif      
      if (FALSE) self = self

      mtag = 1
      proc = source

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_RECV(buf,LEN,MPI_TYPE,proc,mtag, &
      MPI_COMM_WORLD,status,.mpi_error)
      DEALLOCATE(status)
#else
      DIE("wtf?")
      buf = buf
      proc = proc
      self = self
      mtag = mtag
      if(present(tag)) mtag = tag
#endif

   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end

! =================
! Non-blocking send
! =================

   isend(buf,dest,tag,request) ::: template
   ! Begins a nonblocking send
      buf :: VAR_TYPE, INOUT
      dest :: INT, IN
      tag :: INT, IN, optional
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      req,proc,mtag :: INT


      mtag = dest
      proc = dest

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_ISEND(buf,LEN,MPI_TYPE,proc,mtag, &
      MPI_COMM_WORLD,req,.mpi_error)
      if (present(request)) request = req
#else
      if (FALSE) self = self
      DIE("wtf?")
      proc = proc
      self = self
      req =1 
      buf = buf
      if(present(tag)) mtag = tag
      if (present(request)) request = req
#endif

   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end

! ====================
! Non-blocking receive
! ====================

   irecv(buf,source,tag,request) ::: template
   ! Begins a nonblocking receive
      buf :: VAR_TYPE, INOUT
      source :: INT, IN
      tag :: INT, IN, optional
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      requestuest,proc,mtag :: INT

      if (FALSE) self = self

      mtag = source
      proc = source

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_IRECV(buf,LEN,MPI_TYPE,proc,mtag,MPI_COMM_WORLD, &
      requestuest,.mpi_error)
      if(present(request)) request = requestuest
#else
      DIE("wtf?")
      buf = buf
      proc = proc
      request = request
      self = self
      requestuest = 1
      mtag = mtag
      if(present(tag)) mtag = tag
      if (present(request)) request = requestuest
#endif

   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
!   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

! =========================
! Blocking synchronous send
! =========================

   ssend(buf,dest,tag) ::: template
   ! Blocking synchronous send
      buf :: VAR_TYPE, INOUT
      dest :: INT, IN
      tag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc,mtag :: INT

      if (FALSE) self = self

      mtag = 1
      proc = dest
      

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_SSEND(buf,LEN,MPI_TYPE,proc,mtag,MPI_COMM_WORLD,.mpi_error)
#else
      DIE("wtf?")
      proc = proc
      self = self
      mtag = mtag
      if(present(tag)) mtag = tag
      buf = buf
#endif

   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end

! =============================
! Non-blocking synchronous send
! =============================

   issend(buf,dest,tag,request) ::: template
   ! Starts a nonblocking synchronous send
      buf :: VAR_TYPE, INOUT
      dest :: INT, IN
      tag :: INT, IN, optional
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel environment")

      requestuest,proc,mtag :: INT


      mtag = dest
      proc = dest

#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_ISEND(buf,LEN,MPI_TYPE,proc,mtag, &
      MPI_COMM_WORLD,requestuest,.mpi_error)
      if (present(request)) request = requestuest
#else
      if (FALSE) self = self
      DIE("wtf?")
      proc = proc
      request = request
      self = self
      requestuest =1 
      buf = buf
      if(present(tag)) mtag = tag
      if (present(request)) request = requestuest
#endif

   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

! =====================
! Blocking send/receive
! =====================

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: template
   ! Sends and receives a message
      self :: INOUT
      sendbuf :: VAR_TYPE, IN
      recvbuf :: VAR_TYPE, INOUT
      dest :: INT, IN
      source :: INT, IN, optional
      sendtag,recvtag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc1,proc2,mtag1,mtag2 :: INT
#ifdef MPI
      status :: VEC{INT}*
      ALLOCATE(status(MPI_STATUS_SIZE))
#endif      
      if (FALSE) self = self

      mtag1 = 1
      mtag2 = 1
      proc1 = dest
      proc2 = dest

#ifdef MPI
      if(present(sendtag)) mtag1 = sendtag
      if(present(recvtag)) mtag2 = recvtag
      if(present(source)) proc2 = source
      call MPI_SENDRECV(sendbuf,LEN1,MPI_TYPE,proc1,mtag1,recvbuf,LEN2,MPI_TYPE,proc2,mtag2,MPI_COMM_WORLD,status,.mpi_error)
      DEALLOCATE(status)
#else
      DIE("wtf?")
      self = self
      recvbuf = sendbuf
      if(present(sendtag)) mtag1 = sendtag
      if(present(recvtag)) mtag2 = recvtag
      if(present(source)) proc2 = source
#endif

   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN1=>len(sendbuf)), LEN2=>len(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

! ======
! Gather
! ======

   gather(sendbuf,sendcount,recvbuf,root) ::: template
   ! Gather together values from a group of processes
      sendbuf :: VAR_TYPE, INOUT
      sendcount :: INT, IN
      recvbuf :: VAR_TYPE, OUT, optional
      root :: INT, IN, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc :: INT

#ifdef MPI
      call MPI_GATHER(sendbuf,sendcount,MPI_TYPE,recvbuf,LEN,MPI_TYPE,root,MPI_COMM_WORLD,.mpi_error)
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      proc = sendcount

      proc = 0
      if (present(recvbuf)) recvbuf = sendbuf
      if (present(root)) proc = root

      DIE("wtf?")
      proc = proc
#endif

   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gather together values from a group of processes
   end


   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end


   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end


   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end


   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end


   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

! =======
! Scatter
! =======

   scatter(sendbuf,recvbuf,root) ::: template
   ! Sends data from one process to all other processes in a communicator
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VAR_TYPE, OUT
      root :: INT, IN

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc :: INT

#ifdef MPI
      call MPI_SCATTER(sendbuf,LEN1,MPI_TYPE,recvbuf,LEN2,MPI_TYPE,root,MPI_COMM_WORLD,.mpi_error)
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      proc = root

      proc = 0
      recvbuf = sendbuf

      DIE("wtf?")
      proc = proc
#endif

   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN1=>len(sendbuf)), LEN2=>len(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

! ====================
! Non-blocking scatter
! ====================

   iscatter(sendbuf,recvbuf,root,request) ::: template
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
      sendbuf :: VAR_TYPE, INOUT
      recvbuf :: VAR_TYPE, OUT
      root :: INT, IN
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      req,proc :: INT

#ifdef MPI
      call MPI_ISCATTER(sendbuf,LEN1,MPI_TYPE,recvbuf,LEN2,MPI_TYPE,root,MPI_COMM_WORLD,req,.mpi_error)
      if (present(request)) request = req
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      proc = root
      req = root

      if (present(request)) request = req
      proc = 0
      recvbuf = sendbuf

      DIE("wtf?")
      proc = proc
#endif

   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN1=>len(sendbuf)), LEN2=>len(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

! ===================
! Non-blocking gather
! ===================

   igather(sendbuf,sendcount,recvbuf,root,request) ::: template
   ! Gather together values from a group of processes in a nonblocking way
      sendbuf :: VAR_TYPE, INOUT
      sendcount :: INT, IN
      recvbuf :: VAR_TYPE, OUT, optional
      root :: INT, IN, optional
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc,req :: INT

#ifdef MPI
      call MPI_IGATHER(sendbuf,sendcount,MPI_TYPE,recvbuf,LEN,MPI_TYPE,root,MPI_COMM_WORLD,req,.mpi_error)
      if (present(request)) request = req
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      proc = sendcount

      proc = 0
      req = 0
      if (present(request)) request = req
      if (present(recvbuf)) recvbuf = sendbuf
      if (present(root)) proc = root

      DIE("wtf?")
      proc = proc
#endif

   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

! =========
! Allgather
! =========

   allgather(sendbuf,sendcount,recvbuf) ::: template
   ! Gathers data from all tasks and distribute the combined data to all tasks
      sendbuf :: VAR_TYPE, INOUT
      sendcount :: INT, IN
      recvbuf :: VAR_TYPE, OUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc :: INT

#ifdef MPI
      call MPI_ALLGATHER(sendbuf,sendcount,MPI_TYPE,recvbuf,LEN,MPI_TYPE,MPI_COMM_WORLD,.mpi_error)
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      proc = sendcount

      proc = 0
      recvbuf = sendbuf

      DIE("wtf?")
      proc = proc
#endif

   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

! ======================
! Non-blocking Allgather
! ======================

   iallgather(sendbuf,sendcount,recvbuf,request) ::: template
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
      sendbuf :: VAR_TYPE, INOUT
      sendcount :: INT, IN
      recvbuf :: VAR_TYPE, OUT
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      req,proc :: INT

#ifdef MPI
      call MPI_IALLGATHER(sendbuf,sendcount,MPI_TYPE,recvbuf,LEN,MPI_TYPE,MPI_COMM_WORLD,req,.mpi_error)
      if(present(request)) request = req
#else
      if (FALSE) self = self
      req = 1
      if(present(request)) request = req
      sendbuf = sendbuf
      proc = sendcount
      request = proc

      proc = 0
      recvbuf = sendbuf

      DIE("wtf?")
      proc = proc
#endif

   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end


   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end


   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end


   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end


   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end


   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

! ==========
! All-to-all
! ==========

   alltoall(sendbuf,sendcount,recvbuf) ::: template
   ! Sends data from all to all processes
      sendbuf :: VAR_TYPE, INOUT
      sendcount :: INT, IN
      recvbuf :: VAR_TYPE, OUT

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc :: INT

#ifdef MPI
      call MPI_ALLTOALL(sendbuf,sendcount,MPI_TYPE,recvbuf,LEN,MPI_TYPE,MPI_COMM_WORLD,.mpi_error)
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      proc = sendcount

      proc = 0
      recvbuf = sendbuf

      DIE("wtf?")
      proc = proc
#endif

   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

! =======================
! Non-blocking all-to-all
! =======================

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: template
   ! Sends data from all to all processes in a non-blocking way
      sendbuf :: VAR_TYPE, INOUT
      sendcount :: INT, IN
      recvbuf :: VAR_TYPE, OUT
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      req :: INT

#ifdef MPI
      call MPI_IALLTOALL(sendbuf,sendcount,MPI_TYPE,recvbuf,LEN,MPI_TYPE,MPI_COMM_WORLD,req,.mpi_error)
      if(present(request)) request = req
#else
      if (FALSE) self = self
      sendbuf = sendbuf
      req = sendcount
      if(present(request)) request = req

      recvbuf = sendbuf

      DIE("wtf?")
#endif

   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

! =========
! Broadcast
! =========

   broadcast(buffer,root) ::: template
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
      buffer :: VAR_TYPE
      root :: INT, IN, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      proc :: INT


#ifdef MPI
      proc = 0
      if (present(root)) proc = root
      call MPI_BCAST(buffer,LEN,MPI_TYPE,proc,MPI_COMM_WORLD,.mpi_error)
#else
      if (FALSE) self = self
      buffer = buffer

      proc = 0
      if (present(root)) proc = root
      DIE("wtf?")
      proc = proc
#endif

   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

! ======================
! Non-blocking Broadcast
! ======================

   ibroadcast(buffer,root,request) ::: template
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
      buffer :: VAR_TYPE
      root :: INT, IN, optional
      request :: INT, OUT, optional

   ENSURE(.is_parallel,"must be in a parallel enviornoment")

      req,proc :: INT


#ifdef MPI
      proc = 0
      if (present(root)) proc = root
      call MPI_IBCAST(buffer,LEN,MPI_TYPE,proc,MPI_COMM_WORLD,req,.mpi_error)
      if (present(request)) request = req
#else
      if (FALSE) self = self
      buffer = buffer

      proc = 0
      if (present(root)) proc = root
      req = 0
      if (present(request)) request = req
      DIE("wtf?")
      proc = proc
#endif

   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

! Comment: there may be problems implementing higher order array
! broadcasts, because the interfaces may be missing from the MPI
! module implementation; we could work around this by using temporary
! one dimensional arrays, but that would be inefficient.

   IO_is_allowed result (res)
   ! Return whether or not this processor is allowed to do the I/O operation.
   ! Here, only the master processor does I/O in a parallel job.
      res :: BIN
      res = .is_master_processor OR (NOT .is_parallel)
   end

   set_max_n_skip_proc(n)
   ! Set the maximum number of processors that can be skipped when assigning a
   ! processor grid.
      self :: INOUT
      n :: INT, IN
      .max_n_skip_proc = n
   end

   get_2d_processor_grid(nprocx,nprocy,nproc)
   ! This routine returns how to divide up the processors into a two-dimensional
   ! grid so that the number of processors in each dimension is roughly the same.
   ! If the number of processors cannot be factored into two dimensions, then it
   ! starts ignoring cpus.
      self :: IN
      nprocx, nprocy :: INT, OUT
      nproc :: INT, IN, optional
      n_skip,i,nprocs :: INT
      found :: BIN

      if (present(nproc)) then
       nprocs = nproc
      else
       nprocs = .n_processors
      end

      ! Quick case
      if (nprocs < 4) then
       nprocx = nprocs
       nprocy = 1
       return
      end

      ENSURE(.max_n_skip_proc>=0,"max_n_skip_proc must be positive")
      ! How to break up grid of processors.
      ! Must have nprocx * nprocy = nprocs
      ! If they can't be factored nicely then try factoring less cpus.
      nprocs = nprocs + 1
      do n_skip = 0,min(.max_n_skip_proc,nprocs)
       nprocs = nprocs - 1
       nprocy = floor(sqrt(real(nproc,kind=REAL_KIND))) ! Start from the sqrt in
       ! case we have some nice big factors as well as small ones.
       found = FALSE
       do i=nprocy,1,-1
         nprocx = nproc/nprocy
         if (mod(nproc,nprocy)==0) then
           found = TRUE
           exit
         end
         if (nprocy<nprocx) exit ! Otherwise same test with x,y reversed.
       end
       if (found) exit
      end
   end

!  =====================
!  Scalapack/blacs stuff
!  =====================

   init_2d_proc_grid
   ! This routine initializes blacs for a two-dimensional processor grid.
      self :: INOUT

      ! Initialise Scalapack with 2d processor grid.
      .get_2d_processor_grid(.proc_grid_nrow,.proc_grid_ncol)
#ifdef SCALAPACK
      call sl_init(.blacs_2d_context, .proc_grid_nrow, .proc_grid_ncol)
#else
      .blacs_2d_context = 0
      .proc_grid_nrow = 1
      .proc_grid_ncol = 1
#endif

      ! Get my position in the global matrix.
#ifdef SCALAPACK
      call blacs_gridinfo(.blacs_2d_context, .proc_grid_nprow, &
             .proc_grid_npcol, .proc_grid_myrow, .proc_grid_mycol)
#else
      .proc_grid_myrow = 1
      .proc_grid_mycol = 1
#endif
   end

   done_proc_grid
   ! Uninitialize blacs.
      self :: INOUT
#ifdef SCALAPACK
      call blacs_gridexit( .blacs_2d_context )
      call blacs_exit( 1 ) ! 0 if you want it to shut down MPI
#else
      .blacs_2d_context = 0 ! so that the routine is not empty
#endif
   end

   n_this_row(n) result (res)
   ! Number of rows of a matrix of n rows assigned to this processor.
      self :: IN
      n :: INT, IN
      res :: INT
#ifdef SCALAPACK
      res = numroc(n, .proc_grid_bs, .proc_grid_myrow, 0, .proc_grid_nrow)
#else
      res = n
#endif
   end

   n_this_col(n)
   ! Number of columns of a matrix of n columns assigned to this processor.
      self :: IN
      n :: INT, IN
      res :: INT
#ifdef SCALAPACK
      res = numroc(n, .proc_grid_bs, .proc_grid_mycol, 0, .proc_grid_ncol)
#else
      res = n
#endif
   end

end
