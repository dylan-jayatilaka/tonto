!==================================================================
!
! L_BFGS: Low memory BFGS minimiser routine.
!
! Copyright (C) Max Davidson, 2017
! Copyright (C) Dylan Jayatilaka, 2021
!
!
! This library is free software; you can redistribute it and/or
! modify it under the terms of the GNU Library General Public
! License as published by the Free Software Foundation; either
! version 2 of the License, or (at your option) any later version.
!
! This library is distributed in the hope that it will be useful,
! but WITHOUT ANY WARRANTY; without even the implied warranty of
! MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
! Library General Public License for more details.
!
! You should have received a copy of the GNU Library General Public
! License along with this library; if not, write to the
! Free Software Foundation, Inc., 59 Temple Place - Suite 330,
! Boston, MA  02111-1307, USA.
!==================================================================

module L_BFGS

   implicit none

contains

! ==========
! Allocation
! ==========

   create ::: get_from(OBJECT), leaky, PURE
   ! Allocate an object
   end

   destroy ::: get_from(OBJECT), leaky, PURE
   ! Deallocate "self"
   end

   destroy_ptr_part ::: leaky, PURE
   ! Destroy the pointer parts of self
      self :: INOUT
      .par.destroy
      .grd.destroy
   end

!  ===========
!  Set methods
!  ===========

   set_defaults ::: leaky, PURE
   ! Set up the default settings
      self :: INOUT

      .iteration             = 0 
      .start_iteration       = DIIS_START_ITERATION
      .save_iteration        = DIIS_SAVE_ITERATION
      .keep                  = DIIS_KEEP

      .grd_norm              = 1000d0
      .convergence_tolerance = DIIS_CONVERGENCE_TOLERANCE
      .start_tolerance       = DIIS_START_TOLERANCE

      ! Clean up (leaky)
      .destroy_ptr_part

   end

   reset_iteration_defaults ::: leaky, PURE
   ! Reset defaults without changing user settings
      self :: INOUT

      ! The first time called it is iteration 0
      .iteration  = -1 

      ! Clean up old stuff (leaky)
      .destroy_ptr_part

   end

   set_convergence_tolerance(tolerance) ::: PURE
   ! Read the convergence tolerance
      self :: INOUT
      tolerance :: REAL, IN

   ENSURE(tolerance>ZERO,"must have positive convergence tolerance")
 ! WARN_IF(tolerance<TOL(6),"convergence tolerance may be too small")

      .convergence_tolerance = tolerance

   end

   set_start_tolerance(tolerance) ::: PURE
   ! Read the start tolerance when extrapolation is supposed to start
      self :: INOUT
      tolerance :: REAL, IN

   ENSURE(tolerance>ZERO,"must have positive start tolerance")
   ENSURE(tolerance>.convergence_tolerance,"start tolerance is <= convergence tolerance")
 ! WARN_IF(tolerance<TOL(6),"start tolerance may be too small")

      .start_tolerance = tolerance

   end

   set_keep(keep) ::: PURE
   ! Set the number of parameter/error vectors to keep 
      self :: INOUT
      keep :: INT, IN

   ENSURE(keep>=4,"no. to keep must be more than 4")

      .keep = keep

   end

   set_start_iteration(start) ::: pure
   ! Set at what iteration the procedure should start
      self :: INOUT
      start :: INT, IN

      .start_iteration = start

   end

   set_save_iteration(save_it) ::: pure
   ! Set at what iteration the procedure should start saving vectors
      self :: INOUT
      save_it :: INT, IN

      .save_iteration = save_it

   end

!  =================
!  Read the keywords
!  =================

   read_keywords ::: get_from(OBJECT)
   ! Read data from "stdin" using keyword style input.
   end

   process_keyword(keyword) ::: leaky
   ! Process a command "keyword". Data is inputted from "stdin", unless
   ! "word" is a sequence of blank separated strings. In this case,
   ! the sequence is processed as if it were a separate file.
      self :: INOUT
      keyword :: STR, IN

      word :: STR

      word = keyword
      word.to_lower_case

      select case (word)
      case ("}                       ")  ! exit case
      case ("convergence_tolerance=  "); .read_convergence_tolerance
      case ("keep=                   "); .read_keep
      case ("save_iteration=         "); .read_save_iteration
      case ("start_iteration=        "); .read_start_iteration
      case ("start_tolerance=        "); .read_start_tolerance
      case default;           UNKNOWN(word)
      end

   end

   read_convergence_tolerance ::: get_from(OBJECT:read_and_set, VAL?=>REAL, SET?=>.set_convergence_tolerance)
   ! Generic read and set a quantity with units
   end

   read_start_tolerance ::: get_from(OBJECT:read_and_set, VAL?=>REAL, SET?=>.set_start_tolerance)
   ! Generic read and set a quantity with units
   end

   read_keep ::: get_from(OBJECT:read_and_set, VAL?=>INT, SET?=>.set_keep)
   ! Generic read and set a quantity with units
   end

   read_start_iteration ::: get_from(OBJECT:read_and_set, VAL?=>INT, SET?=>.set_start_iteration)
   ! Generic read and set a quantity with units
   end

   read_save_iteration ::: get_from(OBJECT:read_and_set, VAL?=>INT, SET?=>.set_save_iteration)
   ! Generic read and set a quantity with units
   end

!  ======
!  L-BFGS
!  ======

   minimize_L_BFGS(func,par)
   ! Use the limited memory Broyden-Fletcher-Goldfarb-Shanno method to
   ! minimise the molecular orbitals with gradient function from
   ! Stoll(1980). This code is all per Nocedal & Wright
   ! NOTE: set saved_self before calling self & dself
      self :: INOUT
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN  ! MO's as vector
            res  :: REAL, OUT   ! E(MO)
            g :: VEC{REAL}, OUT ! gradient
         end
      end
      par :: VEC{REAL}, INOUT
      
      j,n,m :: INT
      
      ! alpha, c1 and c2 are used in the line search algorithm and
      ! beta, gamma and rho are used in the Hessian approximation
      alpha,beta,gamma,rho,c1,c2 :: REAL
      
      ! a is used to store the various alpha values in the two loop
      ! recursion, p,q and r are used as generic vectors, and as part
      ! of the two loop recursion process
      a,q,s,y :: VEC{REAL}*
      
      .iteration = .iteration + 1
      n =  par.dim1

      ! Gradient 
      q.create(n)
      call func(par,c1,q)

      ! Store the parameter & gradient
      ! NOTE: you should reset if you change
      ! any of the par vectors outside of here ...
      .par.append_fifo(par,.keep)
      .grd.append_fifo(q  ,.keep)

      ! Size of the kept vectors
      m = .par.dim1 ! Note self

      ! Converged? Cleanup
      .grd_norm = q.norm
      if(.is_converged) then
         q.destroy
         return
      end if

      ! Workspace
      s.create(n)
      y.create(n)
      a.create(m)
      a = ZERO

      ! The values used for line search algorithms. These values are
      ! the ones which usually work.
      c1 =   TOL(3)
      c2 = 9*TOL(1)

      
      ! Initialises the values used in the two-loop recursion.
      rho = 1
      if (m>1) then   
         s = .par(m).element - .par(m-1).element
         y = .grd(m).element - .grd(m-1).element
         gamma = dot_product(s,y)/dot_product(y,y)
      else
         gamma = 1 
      end if
      
      ! Will cause the program to exit if a suitable step can not be
      ! determined 
      !alpha = ZERO
     
      ! The two loop recursion process approximates the Hessian by
      ! using stored values of delta x and delta grad, as well as a
      ! pre-scaling, gamma, to allow for a stepping at alpha = 1, to
      ! be suitable 
        
      ! H^0 is approximated by I   
        
      if (m > 1) then
         do j = m,2,-1
            s = .par(j).element - .par(j-1).element
            y = .grd(j).element - .grd(j-1).element
            rho = 1/(dot_product(y,s))
            a(j) = rho*dot_product(s,q)
            q = q - a(j)*y
         end do
      end if
 
      q = gamma*q
 
      do j = 2,m
         s = .par(j).element - .par(j-1).element
         y = .grd(j).element - .grd(j-1).element
         rho = 1/dot_product(y,s)
         beta = rho*dot_product(y,q) 
         q = q + (a(j) - beta)*s
      end do
 
      q = -q

      call line_search(func,1d0,par,q,c1,c2,alpha)

      ! Performs the step (and stores the previous x value)
      par = par + alpha*q
         
      ! Cleanup
      q.destroy

   end
   
   minimize_parabolic_L_BFGS(func,par)
   ! Use the limited memory Broyden-Fletcher-Goldfarb-Shanno method to
   ! minimise the molecular orbitals with gradient function from
   ! Stoll(1980). This code is all per Nocedal & Wright. This function
   ! utilises a parabolic stepping protocol rather than a line-search
   ! to reduce memory usage.
   ! NOTE: set saved_self before calling self & dself
      self :: INOUT
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN  ! MO's as vector
            res  :: REAL, OUT   ! E(MO)
            g :: VEC{REAL}, OUT ! gradient
         end
      end
      par :: VEC{REAL}, INOUT
      
      j,n,m :: INT
      
      ! alpha, c1 and c2 are used in the line search algorithm and
      ! beta, gamma and rho are used in the Hessian approximation
      beta,gamma,rho,c1,c2,E :: REAL
      
      ! a is used to store the various alpha values in the two loop
      ! recursion, p,q and r are used as generic vectors, and as part
      ! of the two loop recursion process
      a,q,g,s,y :: VEC{REAL}*
      
      .iteration = .iteration + 1
      n =  par.dim1

      ! Gradient 
      q.create(n)
      call func(par,c1,q)

      ! Store the parameter & gradient
      ! NOTE: you should reset if you change
      ! any of the par vectors outside of here ...
      .par.append_fifo(par,.keep)
      .grd.append_fifo(q  ,.keep)

      ! Size of the kept vectors
      m = .par.dim1 ! Note self

      ! Converged?
      .grd_norm = q.norm
      if(.is_converged) then
         ! Cleanup
         q.destroy
         return
      end if

      ! Workspace
      s.create(n)
      y.create(n)
      a.create(m)
      a = ZERO

      ! The values used for line search algorithms. These values are
      ! the ones which usually work.
      c1 =   TOL(3)
      c2 = 9*TOL(1)

      
      ! Initialises the values used in the two-loop recursion.
      rho = 1
      if (m>1) then   
         s = .par(m).element - .par(m-1).element
         y = .grd(m).element - .grd(m-1).element
         gamma = dot_product(s,y)/dot_product(y,y)
      else
         gamma = 1 
      end if
      
     ! The two loop recursion process approximates the Hessian by
     ! using stored values of delta x and delta grad, as well as a
     ! pre-scaling, gamma, to allow for a stepping at alpha = 1, to
     ! be suitable 
        
     ! H^0 is approximated by I   
        
        if (m > 1) then
           do j = m,2,-1
              s = .par(j).element - .par(j-1).element
              y = .grd(j).element - .grd(j-1).element
              rho = 1/(dot_product(y,s))
              a(j) = rho*dot_product(s,q)
              q = q - a(j)*y
           end do
        end if
 
        q = gamma*q
 
        do j = 2,m
           s = .par(j).element - .par(j-1).element
           y = .grd(j).element - .grd(j-1).element
           rho = 1/dot_product(y,s)
           beta = rho*dot_product(y,q) 
           q = q + (a(j) - beta)*s
        end do
 
      ! This is the step toward the minimum
         q = -q
         if( .iteration >7) then

         else
            q = 0.1*q
         end if
         g.create(n)

         call func(par+q,E,g)

         par = par + dot_product(.grd(min(.iteration, .keep)).element,q/q.norm)&
         /(dot_product(g,q/q.norm)-&
         dot_product(.grd(min(.iteration,.keep)).element,q/q.norm))*q


      g.destroy
      q.destroy

   end
   
   minimize_nols_L_BFGS(func,par)
   ! Use the limited memory Broyden-Fletcher-Goldfarb-Shanno method to
   ! minimise the molecular orbitals with gradient function from
   ! Stoll(1980). This code is all per Nocedal & Wright. This function
   ! utilises an in-built stepping protocol rather than a line-search
   ! to reduce memory usage.
   ! NOTE: set saved_self before calling self & dself
      self :: INOUT
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN  ! MO's as vector
            res  :: REAL, OUT   ! E(MO)
            g :: VEC{REAL}, OUT ! gradient
         end
      end
      par :: VEC{REAL}, INOUT
      
      j,n,m :: INT
      
      ! alpha, c1 and c2 are used in the line search algorithm and
      ! beta, gamma and rho are used in the Hessian approximation
      alpha,beta,gamma,rho,c1,c2 :: REAL
      
      ! a is used to store the various alpha values in the two loop
      ! recursion, p,q and r are used as generic vectors, and as part
      ! of the two loop recursion process
      a,q,s,y :: VEC{REAL}*
      
      .iteration = .iteration + 1
      n =  par.dim1

      ! Gradient 
      q.create(n)
      call func(par,c1,q)

      ! Store the parameter & gradient
      ! NOTE: you should reset if you change
      ! any of the par vectors outside of here ...
      .par.append_fifo(par,.keep)
      .grd.append_fifo(q  ,.keep)

      ! Size of the kept vectors
      m = .par.dim1 ! Note self

      ! Converged?
      .grd_norm = q.norm
      if(.is_converged) then
         ! Cleanup
         q.destroy
         return
      end if

      ! Workspace
      s.create(n)
      y.create(n)
      a.create(m)
      a = ZERO

      ! The values used for line search algorithms. These values are
      ! the ones which usually work.
      c1 = 10d-4
      c2 = 9d-1

      
      ! Initialises the values used in the two-loop recursion.
      rho = 1
      if (m>1) then   
         s = .par(m).element - .par(m-1).element
         y = .grd(m).element - .grd(m-1).element
         gamma = dot_product(s,y)/dot_product(y,y)
      else
         gamma = 1 
      end if
      
      ! Will cause the program to exit if a suitable step can not be
      ! determined 
      !alpha = ZERO
     
     ! The two loop recursion process approximates the Hessian by
     ! using stored values of delta x and delta grad, as well as a
     ! pre-scaling, gamma, to allow for a stepping at alpha = 1, to
     ! be suitable 
        
     ! H^0 is approximated by I   
        
        if (m > 1) then
           do j = m,2,-1
              s = .par(j).element - .par(j-1).element
              y = .grd(j).element - .grd(j-1).element
              rho = 1/(dot_product(y,s))
              a(j) = rho*dot_product(s,q)
              q = q - a(j)*y
           end do
        end if
 
        q = gamma*q
 
        do j = 2,m
           s = .par(j).element - .par(j-1).element
           y = .grd(j).element - .grd(j-1).element
           rho = 1/dot_product(y,s)
           beta = rho*dot_product(y,q) 
           q = q + (a(j) - beta)*s
        end do
 
      ! This is the step toward the minimum
         q = -q
         if( .iteration >7) then

         else
            q = 0.1*q
         end if


         alpha = dot_product(.grd(min(.iteration,.keep)).element, &
         .grd(min(.iteration,.keep)).element)/(dot_product(q,q) + &
         dot_product(.grd(min(.iteration,.keep)).element, &
         .grd(min(.iteration, .keep)).element))
         
      par = par + alpha*q

      q.destroy

      !stdout.show("alpha =",alpha)
      !stdout.text("par:")
      !stdout.put(par)

   end
   
   minimize_L_BFGS_a(func,par)
   ! Use the limited memory Broyden-Fletcher-Goldfarb-Shanno method to
   ! minimise the molecular orbitals with gradient function from
   ! Stoll(1980). This code is all per Nocedal & Wright
   ! NOTE: set saved_self before calling self & dself
      self :: INOUT
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN
            res  :: REAL, OUT
            g :: VEC{REAL}, OUT
         end
      end
      par :: VEC{REAL}, INOUT
      
      j,n,m :: INT
      
      ! alpha, c1 and c2 are used in the line search algorithm and
      ! beta, gamma and rho are used in the Hessian approximation
      alpha,beta,gamma,rho,c1,c2 :: REAL
      
      ! a is used to store the various alpha values in the two loop
      ! recursion, p,q and r are used as generic vectors, and as part
      ! of the two loop recursion process
      a,q,s,y :: VEC{REAL}*
      
      .iteration = .iteration + 1
      n =  par.dim1

      ! Gradient 
      q.create(n)
      call func(par,c1,q)

      ! Store the parameter & gradient
      ! NOTE: you should reset if you change
      ! any of the par vectors outside of here ...
      .par_a.append_fifo(par,.keep)
      .grd_a.append_fifo(q  ,.keep)

      ! Size of the kept vectors
      m = .par_a.dim1 ! Note self

      ! Converged?
      .grd_norm_a = q.norm
      if(.is_converged) then
         ! Cleanup
         q.destroy
         return
      end if
   
      ! Workspace
      s.create(n)
      y.create(n)
      a.create(m)
      a = ZERO

      ! The values used for line search algorithms. These values are
      ! the ones which usually work.
      c1 = 10d-4
      c2 = 9d-1

      
      ! Initialises the values used in the two-loop recursion.
      rho = 1
      if (m>1) then   
         s = .par_a(m).element - .par_a(m-1).element
         y = .grd_a(m).element - .grd_a(m-1).element
         gamma = dot_product(s,y)/dot_product(y,y)
      else
         gamma = 1 
      end if
      
      ! Will cause the program to exit if a suitable step can not be
      ! determined 
      alpha = ZERO
      

      ! The two loop recursion process approximates the Hessian by
      ! using stored values of delta x and delta grad, as well as a
      ! pre-scaling, gamma, to allow for a stepping at alpha = 1, to
      ! be suitable 
         
      ! H^0 is approximated by I   
         
         if (m > 1) then
            do j = m,2,-1
               s = .par_a(j).element - .par_a(j-1).element
               y = .grd_a(j).element - .grd_a(j-1).element
               rho = 1/(dot_product(y,s))
               a(j) = rho*dot_product(s,q)
               q = q - a(j)*y
            end do
         end if

         q = gamma*q

         do j = 2,m
            s = .par_a(j).element - .par_a(j-1).element
            y = .grd_a(j).element - .grd_a(j-1).element
            rho = 1/dot_product(y,s)
            beta = rho*dot_product(y,q) 
            q = q + (a(j) - beta)*s
         end do

      ! This is the step toward the minimum
         q = -q

      ! Fines the ideal multiple of the step to take (usually 1)
      ! and stores that in alpha
      call line_search(func,10d1,par,q,c1,c2,alpha)
         
      ! Performs the step (and stores the previous x value)
      par = par + alpha*q
         
      ! Cleanup
      a.destroy     
      y.destroy
      s.destroy
      q.destroy

   end
   
   minimize_L_BFGS_b(func,par)
   ! Use the limited memory Broyden-Fletcher-Goldfarb-Shanno method to
   ! minimise the molecular orbitals with gradient function from
   ! Stoll(1980). This code is all per Nocedal & Wright
   ! NOTE: set saved_self before calling self & dself
      self :: INOUT
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN
            res  :: REAL, OUT
            g :: VEC{REAL}, OUT
         end
      end
      par :: VEC{REAL}, INOUT
      
      j,n,m :: INT
      
      ! alpha, c1 and c2 are used in the line search algorithm and
      ! beta, gamma and rho are used in the Hessian approximation
      alpha,beta,gamma,rho,c1,c2 :: REAL
      
      ! a is used to store the various alpha values in the two loop
      ! recursion, p,q and r are used as generic vectors, and as part
      ! of the two loop recursion process
      a,q,s,y :: VEC{REAL}*
      
      .iteration = .iteration + 1
      n =  par.dim1

      ! Gradient 
      q.create(n)
      call func(par,c1,q)

      ! Store the parameter & gradient
      ! NOTE: you should reset if you change
      ! any of the par vectors outside of here ...
      .par_b.append_fifo(par,.keep)
      .grd_b.append_fifo(q  ,.keep)

      ! Size of the kept vectors
      m = .par_b.dim1 ! Note self

      ! Converged?
      .grd_norm_b = q.norm
      if(.is_converged) then
         ! Cleanup
         q.destroy
         return
      end if
   
      ! Workspace
      s.create(n)
      y.create(n)
      a.create(m)
      a = ZERO

      ! The values used for line search algorithms. These values are
      ! the ones which usually work.
      c1 = 10d-4
      c2 = 9d-1

      
      ! Initialises the values used in the two-loop recursion.
      rho = 1
      if (m>1) then   
         s = .par_b(m).element - .par_b(m-1).element
         y = .grd_b(m).element - .grd_b(m-1).element
         gamma = dot_product(s,y)/dot_product(y,y)
      else
         gamma = 1 
      end if
      
      ! Will cause the program to exit if a suitable step can not be
      ! determined 
      alpha = ZERO
      

      ! The two loop recursion process approximates the Hessian by
      ! using stored values of delta x and delta grad, as well as a
      ! pre-scaling, gamma, to allow for a stepping at alpha = 1, to
      ! be suitable 
         
      ! H^0 is approximated by I   
         
         if (m > 1) then
            do j = m,2,-1
               s = .par_b(j).element - .par_b(j-1).element
               y = .grd_b(j).element - .grd_b(j-1).element
               rho = 1/(dot_product(y,s))
               a(j) = rho*dot_product(s,q)
               q = q - a(j)*y
            end do
         end if

         q = gamma*q

         do j = 2,m
            s = .par_b(j).element - .par_b(j-1).element
            y = .grd_b(j).element - .grd_b(j-1).element
            rho = 1/dot_product(y,s)
            beta = rho*dot_product(y,q) 
            q = q + (a(j) - beta)*s
         end do

      ! This is the step toward the minimum
         q = -q

      ! Fines the ideal multiple of the step to take (usually 1)
      ! and stores that in alpha
      call line_search(func,10d1,par,q,c1,c2,alpha)
         
      ! Performs the step (and stores the previous x value)
      par = par + alpha*q
         
      ! Cleanup
      a.destroy     
      y.destroy
      s.destroy
      q.destroy

   end

!  ===========
!  Line Search
!  ===========

   line_search(func,alphamax,x,p,c1,c2,b) ::: selfless
   ! Given a real vector, x, function f, gradient function
   ! df, calculaes the ideal stepping scale alpha, given
   ! stepping p and constants c1, c2.
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN
            res  :: REAL, OUT
            g :: VEC{REAL}, OUT
         end
      end
      x :: VEC{REAL}, IN
      p :: VEC{REAL}, IN
      
      ! Wolfe constants c1 and c2 and a highest value of alpha
      c1,c2,alphamax :: REAL, IN
      
      ! The output alpha value
      b :: REAL, OUT

      ! alpha is the current value of alpha. alpha0 is the previous
      ! value. phi0 is the initial phi value, phi1 is the previous phi
      ! value and phi is thge current phi value. The same is true for
      ! dphi0, dphi1 and dphi.
      alpha,alpha0,phi0,phi1,phi,dphi,dphi0,dphi1 :: REAL
      
      ! temp is used in vector dot product calculations. Such as in
      ! phi' (dphi)
      temp :: VEC{REAL}*
      
      ! The maximum (iter) and current (j) number of iterations
      iter,j :: INT
      
      ! Logical for if the loop can conclude
      converged :: BIN

      ! The maximum number of iterations that may be performed
      iter = 5
      
      ! Allows temp to be used for calculations
      temp.create(size(x))

      ! Gives the first value of alpha
      call RANDOM_NUMBER(alpha0)
      alpha = alpha0*alphamax 
      
      ! Initialise alpha0, phi0, dphi0, phi1, j and converged
      alpha0 = 0
      call func(x,phi0,temp)

      dphi0 = dot_product(temp,p)
      phi1=0
      j = 1
      converged = FALSE 

      ! Performs the loop until it has converged or the loop has
      ! failed
      do while ((converged EQV FALSE) AND (j < iter)) 
         
         ! calculates phi for this iteration
         call func(x+alpha*p,phi,temp)

         ! The interval alpha0, alpha will contain ideal step lengths if
         ! any of the if conditionals below are violated
         if (phi > phi0 + c1*alpha*dphi0) then
      
            ! Sends interval off to find an ideal step and stores it
            ! in b
            call zoom(func,alpha0,alpha,x,p,c1,c2,b,converged)

         else if (phi >= phi1 AND j > 1) then
      
            ! Sends interval off to find an ideal step and stores it
            ! in b
            call zoom(func,alpha0,alpha,x,p,c1,c2,b,converged)

         end if
      
         ! Ensures that if alpha has converged, the value does not
         ! change   
         if (converged EQV FALSE) then

            ! calculates dphi for this iteration      
            dphi = dot_product(temp,p)

            ! If the second wolfe condition holds, this has converged      
            if (abs(dphi) <= -c2*dphi0) then

               ! Stores the ideal stepping in b         
               converged = TRUE
               b = alpha

            else if (dphi >= 0) then

               ! Sends the interval off to find an ideal step and stores it in
               ! b
               call zoom(func,alpha,alpha0,x,p,c1,c2,b,converged)

            end if

            ! Ensures that if alpha has converged, the value does not
            ! change   
            if (converged EQV FALSE) then
               ! Stores the values from this iteration, and proceeds to the
               ! next iteration         
               phi1 = phi
               dphi1 = dphi
               j = j + 1
               alpha0 = alpha
               call RANDOM_NUMBER(phi)
               alpha = alpha + phi*alphamax
            end if
         end if
     end
     
     ! Cleanup
     temp.destroy

   end

   zoom(func,low,high,x,p,c1,c2,alpha,converged) ::: selfless 
   ! Given a real vector, x, function f, gradient function
   ! df, calculaes the ideal stepping scale alpha, given
   ! stepping p and constants c1, c2. An ideal stepping must also
   ! reside between low and high.
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN
            res  :: REAL, OUT
            g :: VEC{REAL}, OUT
         end
      end
      ! Interval in which ideal step resides, low and high and Wolfe
      ! constants c1 and c2
      low,high,c1,c2 :: REAL, IN
      x,p :: VEC{REAL}, IN
      
      ! Ideal stepping scale
      alpha :: REAL, OUT

      ! Logical which describes whether or not it has converged to an
      ! ideal step
      converged :: BIN, OUT
      
      ! Vector used in various multiplications
      temp :: VEC{REAL}*
      
      ! Flexible interval alphalo and alphahi. Initial value of phi
      ! and dphi, phi0 and dphi0. Current values at phi and dphi. phi
      ! value at alpha lo is at philo.
      alphalo,alphahi,phi0,dphi,dphi0,phi,philo :: REAL
      
      ! The current (j) and maximum (iter) number of iterations
      j,iter :: INT
      iter = 10 
      
      ! creates temp
      temp.create(size(x))
      
      ! Initialise alphalo, alphahi, phi0, dphi0, converged and j
      alphalo = low
      alphahi = high
      call func(x,phi0,temp)
      dphi0 = dot_product(temp,p)
      converged = FALSE
      j = 0
 
      ! Does the loops until interations maximum is reached or the
      ! ideal stepping is found.
      do while ((converged EQV FALSE) AND (j < iter)) 
      
      ! New value of alpha is determined by bisection
         alpha = alphalo+alphahi/2
         
      ! Calculates current value of philo and phi   
         call func(x+alphalo*p,philo,temp)
         call func(x+alpha*p,phi,temp)

      ! If either of these condition are satisfied alphahi becomes
      ! alpha amd the cycle repeats   
         if (phi > phi0+c1*alpha*dphi0) then
            alphahi = alpha
         else if (phi >= philo) then
            alphahi = alpha
         else
     
      ! Calculates the current value of dphi
            dphi = dot_product(temp,p)

      ! Cycle is complete if strong Wolfe condition holds      
            if (abs(dphi) <= -c2*dphi0) then
               converged = TRUE
            end if

      ! Stops pointless calculations if ideal stepping has been found
            if (converged EQV FALSE) then

      ! Ensures alphahi and alphalo are the right way around         
               if (dphi*(alphahi-alphalo) >= ZERO) then
                  alphahi = alphalo
               end if
      ! alphalo becomes alpha         
               alphalo = alpha
            end if
         end if
         j = j + 1
      end
      temp.destroy
      converged = TRUE
   end


!  =======
!  Queries
!  =======

   subspace_saturated result (res) ::: pure
   ! Return TRUE if the subspace is saturated; no more new
   ! vectors can be stored.
      self :: IN
      res :: BIN

      res = .par.dim == .keep

   end

   apply_l_bfgs result (res) ::: pure
   ! Return TRUE if extrapolation must be applied this iteration
      self :: IN
      res :: BIN

      res = .iteration - .start_iteration >= 0

   end

   saved_iteration result (res) ::: pure
   ! Return the actual iteration number since starting to save vectors
      self :: IN
      res :: INT

      res = .iteration - .save_iteration

   end

!  =============
!  Extrapolation
!  =============

   is_converged result (res) ::: PURE
   ! Return TRUE if the procedure is converged
      self :: IN
      res :: BIN

      res = .grd_norm<.convergence_tolerance

   end

!  ======
!  Output
!  ======

   put
   ! Print out info
      self :: IN

      stdout.flush
      stdout.text("L_BFGS options: ")
      stdout.flush
      stdout.set_real_style("e")
      stdout.show("Convergence on max(abs(error)) =",.convergence_tolerance)
      stdout.set_real_style("f")
      stdout.show("Extrapolate  from iteration    =",.start_iteration)
      stdout.show("Save vectors from iteration    =",.save_iteration)
      stdout.show("No. of vectors to keep         =",.keep)

   end

end
