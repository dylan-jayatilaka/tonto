!---------------------------------------------------------------------------
!
! L_BFGS: Low memory BFGS minimiser routine.
!
! Copyright (C) Max Davidson, 2017
!
!
! This library is free software; you can redistribute it and/or
! modify it under the terms of the GNU Library General Public
! License as published by the Free Software Foundation; either
! version 2 of the License, or (at your option) any later version.
!
! This library is distributed in the hope that it will be useful,
! but WITHOUT ANY WARRANTY; without even the implied warranty of
! MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
! Library General Public License for more details.
!
! You should have received a copy of the GNU Library General Public
! License along with this library; if not, write to the
! Free Software Foundation, Inc., 59 Temple Place - Suite 330,
! Boston, MA  02111-1307, USA.
!
!---------------------------------------------------------------------------

module L_BFGS

   implicit none

contains

! =================
! Memory allocation
! =================

   create ::: get_from(OBJECT), leaky, PURE
   ! Create an object
   end

   destroy ::: get_from(OBJECT), leaky, PURE
   ! Destroy an object
   end

   nullify_ptr_part ::: pure
   ! Nullify the pointer parts of self
      self :: INOUT

      nullify(.par)
      nullify(.grd)

   end

   destroy_ptr_part ::: leaky, PURE
   ! Destroy the pointer parts of self
      self :: INOUT

      .par.destroy
      .grd.destroy

   end

!  ===========
!  Set methods
!  ===========

   set_defaults ::: leaky, PURE
   ! Set up the default settings
      self :: INOUT

      .iteration             = 0 
      .start_iteration       = DIIS_START_ITERATION
      .save_iteration        = DIIS_SAVE_ITERATION
      .keep                  = DIIS_KEEP

      .grd_norm              = 1000d0
      .convergence_tolerance = DIIS_CONVERGENCE_TOLERANCE
      .start_tolerance       = DIIS_START_TOLERANCE

      ! Clean up (leaky)
      .destroy_ptr_part

   end

   reset_iteration_defaults ::: leaky
   ! Reset defaults without changing user settings
      self :: INOUT

      .iteration  = -1 ! The first time called it is iteration 0

      ! Clean up old stuff (leaky)
      .destroy_ptr_part

   end

   set_convergence_tolerance(tolerance) ::: PURE
   ! Read the convergence tolerance
      self :: INOUT
      tolerance :: REAL, IN

   ENSURE(tolerance>ZERO,"must have positive convergence tolerance")
 ! WARN_IF(tolerance<TOL(6),"convergence tolerance may be too small")

      .convergence_tolerance = tolerance

   end

   set_start_tolerance(tolerance) ::: PURE
   ! Read the start tolerance when extrapolation is supposed to start
      self :: INOUT
      tolerance :: REAL, IN

   ENSURE(tolerance>ZERO,"must have positive start tolerance")
   ENSURE(tolerance>.convergence_tolerance,"start tolerance is <= convergence tolerance")
 ! WARN_IF(tolerance<TOL(6),"start tolerance may be too small")

      .start_tolerance = tolerance

   end

   set_keep(keep)
   ! Set the number of parameter/error vectors to keep 
      self :: INOUT
      keep :: INT, IN

   DIE_IF(keep<4,"no. to keep must be more than 4")

      .keep = keep

   end

   set_start_iteration(start) ::: pure
   ! Set at what iteration the procedure should start
      self :: INOUT
      start :: INT, IN

      .start_iteration = start

   end

   set_save_iteration(save_it) ::: pure
   ! Set at what iteration the procedure should start saving vectors
      self :: INOUT
      save_it :: INT, IN

      .save_iteration = save_it

   end

!  =================
!  Read the keywords
!  =================

   read_keywords ::: get_from(OBJECT)
   ! Read data from "stdin" using keyword style input.
   end

   process_keyword(keyword) ::: leaky
   ! Process a command "keyword". Data is inputted from "stdin", unless
   ! "word" is a sequence of blank separated strings. In this case,
   ! the sequence is processed as if it were a separate file.
      keyword :: STR

      word :: STR

      word = keyword
      word.to_lower_case

      select case (word)
      case ("}                       ")  ! exit case
      case ("convergence_tolerance=  "); .read_convergence_tolerance
      case ("keep=                   "); .read_keep
      case ("save_iteration=         "); .read_save_iteration
      case ("start_iteration=        "); .read_start_iteration
      case ("start_tolerance=        "); .read_start_tolerance
      case default;           UNKNOWN(word)
      end

   end

   read_convergence_tolerance ::: get_from(OBJECT:read_and_set, VAL?=>REAL, SET?=>.set_convergence_tolerance)
   ! Generic read and set a quantity with units
   end

   read_start_tolerance ::: get_from(OBJECT:read_and_set, VAL?=>REAL, SET?=>.set_start_tolerance)
   ! Generic read and set a quantity with units
   end

   read_keep ::: get_from(OBJECT:read_and_set, VAL?=>INT, SET?=>.set_keep)
   ! Generic read and set a quantity with units
   end

   read_start_iteration ::: get_from(OBJECT:read_and_set, VAL?=>INT, SET?=>.set_start_iteration)
   ! Generic read and set a quantity with units
   end

   read_save_iteration ::: get_from(OBJECT:read_and_set, VAL?=>INT, SET?=>.set_save_iteration)
   ! Generic read and set a quantity with units
   end

!  ======
!  L-BFGS
!  ======

   minimize_L_BFGS(func,par)
   ! Use the limited memory Broyden-Fletcher-Goldfarb-Shanno method to
   ! minimise the molecular orbitals with gradient function from
   ! Stoll(1980). This code is all per Nocedal & Wright
   ! NOTE: set saved_self before calling self & dself
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN  ! MO's as vector
            res  :: REAL, OUT   ! E(MO)
            g :: VEC{REAL}, OUT ! gradient
         end
      end
      par :: VEC{REAL}, INOUT
      
      j,n,m :: INT
      
      ! alpha, c1 and c2 are used in the line search algorithm and
      ! beta, gamma and rho are used in the Hessian approximation
      alpha,beta,gamma,rho,c1,c2 :: REAL
      
      ! a is used to store the various alpha values in the two loop
      ! recursion, p,q and r are used as generic vectors, and as part
      ! of the two loop recursion process
      a,q,s,y :: VEC{REAL}*
      
      .iteration = .iteration + 1
      n =  par.dim1

      ! Gradient 
      q.create(n)
      call func(par,c1,q)

      ! Store the parameter & gradient
      ! NOTE: you should reset if you change
      ! any of the par vectors outside of here ...
      .par.append_fifo(par,.keep)
      .grd.append_fifo(q  ,.keep)

      ! Size of the kept vectors
      m = .par.dim1 ! Note self

      ! Converged?
      .grd_norm = q.norm
      if(.is_converged) then
         ! Cleanup
         q.destroy
         return
      end if

      ! Workspace
      s.create(n)
      y.create(n)
      a.create(m)
      a = ZERO

      ! The values used for line search algorithms. These values are
      ! the ones which usually work.
      c1 = 10d-4
      c2 = 9d-1

      
      ! Initialises the values used in the two-loop recursion.
      rho = 1
      if (m>1) then   
         s = .par(m).element - .par(m-1).element
         y = .grd(m).element - .grd(m-1).element
         gamma = dot_product(s,y)/dot_product(y,y)
      else
         gamma = 1 
      end if
      
      ! Will cause the program to exit if a suitable step can not be
      ! determined 
      alpha = ZERO
     
     ! The two loop recursion process approximates the Hessian by
     ! using stored values of delta x and delta grad, as well as a
     ! pre-scaling, gamma, to allow for a stepping at alpha = 1, to
     ! be suitable 
        
     ! H^0 is approximated by I   
        
        if (m > 1) then
           do j = m,2,-1
              s = .par(j).element - .par(j-1).element
              y = .grd(j).element - .grd(j-1).element
              rho = 1/(dot_product(y,s))
              a(j) = rho*dot_product(s,q)
              q = q - a(j)*y
           end do
        end if
 
        q = gamma*q
 
        do j = 2,m
           s = .par(j).element - .par(j-1).element
           y = .grd(j).element - .grd(j-1).element
           rho = 1/dot_product(y,s)
           beta = rho*dot_product(y,q) 
           q = q + (a(j) - beta)*s
        end do
 
      ! This is the step toward the minimum
         q = -q

    ! ! Fines the ideal multiple of the step to take (usually 1)
    ! ! and stores that in alpha
    ! !if (.iteration == 1) then
    ! !   q = 0.0001*q
    ! !end if
    ! call line_search(func,1d0,par,q,c1,c2,alpha)
    ! !if (.iteration <= 2) then
    ! !   alpha = 0.1
    ! !else
    ! !    alpha = 1
    ! !end if

  ! DJ
  alpha = 0.005d0

      ! Performs the step (and stores the previous x value)
      par = par + alpha*q
         
      ! Cleanup
    ! a.destroy     
    ! y.destroy
    ! s.destroy
      q.destroy

      !stdout.show("alpha =",alpha)
      !stdout.text("par:")
      !stdout.put(par)

   end
   
   minimize_L_BFGS_a(func,par)
   ! Use the limited memory Broyden-Fletcher-Goldfarb-Shanno method to
   ! minimise the molecular orbitals with gradient function from
   ! Stoll(1980). This code is all per Nocedal & Wright
   ! NOTE: set saved_self before calling self & dself
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN
            res  :: REAL, OUT
            g :: VEC{REAL}, OUT
         end
      end
      par :: VEC{REAL}, INOUT
      
      j,n,m :: INT
      
      ! alpha, c1 and c2 are used in the line search algorithm and
      ! beta, gamma and rho are used in the Hessian approximation
      alpha,beta,gamma,rho,c1,c2 :: REAL
      
      ! a is used to store the various alpha values in the two loop
      ! recursion, p,q and r are used as generic vectors, and as part
      ! of the two loop recursion process
      a,q,s,y :: VEC{REAL}*
      
      .iteration = .iteration + 1
      n =  par.dim1

      ! Gradient 
      q.create(n)
      call func(par,c1,q)

      ! Store the parameter & gradient
      ! NOTE: you should reset if you change
      ! any of the par vectors outside of here ...
      .par_a.append_fifo(par,.keep)
      .grd_a.append_fifo(q  ,.keep)

      ! Size of the kept vectors
      m = .par_a.dim1 ! Note self

      ! Converged?
      .grd_norm_a = q.norm
      if(.is_converged) then
         ! Cleanup
         q.destroy
         return
      end if
   
      ! Workspace
      s.create(n)
      y.create(n)
      a.create(m)
      a = ZERO

      ! The values used for line search algorithms. These values are
      ! the ones which usually work.
      c1 = 10d-4
      c2 = 9d-1

      
      ! Initialises the values used in the two-loop recursion.
      rho = 1
      if (m>1) then   
         s = .par_a(m).element - .par_a(m-1).element
         y = .grd_a(m).element - .grd_a(m-1).element
         gamma = dot_product(s,y)/dot_product(y,y)
      else
         gamma = 1 
      end if
      
      ! Will cause the program to exit if a suitable step can not be
      ! determined 
      alpha = ZERO
      

      ! The two loop recursion process approximates the Hessian by
      ! using stored values of delta x and delta grad, as well as a
      ! pre-scaling, gamma, to allow for a stepping at alpha = 1, to
      ! be suitable 
         
      ! H^0 is approximated by I   
         
         if (m > 1) then
            do j = m,2,-1
               s = .par_a(j).element - .par_a(j-1).element
               y = .grd_a(j).element - .grd_a(j-1).element
               rho = 1/(dot_product(y,s))
               a(j) = rho*dot_product(s,q)
               q = q - a(j)*y
            end do
         end if

         q = gamma*q

         do j = 2,m
            s = .par_a(j).element - .par_a(j-1).element
            y = .grd_a(j).element - .grd_a(j-1).element
            rho = 1/dot_product(y,s)
            beta = rho*dot_product(y,q) 
            q = q + (a(j) - beta)*s
         end do

      ! This is the step toward the minimum
         q = -q

      ! Fines the ideal multiple of the step to take (usually 1)
      ! and stores that in alpha
      call line_search(func,10d1,par,q,c1,c2,alpha)
         
      ! Performs the step (and stores the previous x value)
      par = par + alpha*q
         
      ! Cleanup
      a.destroy     
      y.destroy
      s.destroy
      q.destroy

   end
   
   minimize_L_BFGS_b(func,par)
   ! Use the limited memory Broyden-Fletcher-Goldfarb-Shanno method to
   ! minimise the molecular orbitals with gradient function from
   ! Stoll(1980). This code is all per Nocedal & Wright
   ! NOTE: set saved_self before calling self & dself
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN
            res  :: REAL, OUT
            g :: VEC{REAL}, OUT
         end
      end
      par :: VEC{REAL}, INOUT
      
      j,n,m :: INT
      
      ! alpha, c1 and c2 are used in the line search algorithm and
      ! beta, gamma and rho are used in the Hessian approximation
      alpha,beta,gamma,rho,c1,c2 :: REAL
      
      ! a is used to store the various alpha values in the two loop
      ! recursion, p,q and r are used as generic vectors, and as part
      ! of the two loop recursion process
      a,q,s,y :: VEC{REAL}*
      
      .iteration = .iteration + 1
      n =  par.dim1

      ! Gradient 
      q.create(n)
      call func(par,c1,q)

      ! Store the parameter & gradient
      ! NOTE: you should reset if you change
      ! any of the par vectors outside of here ...
      .par_b.append_fifo(par,.keep)
      .grd_b.append_fifo(q  ,.keep)

      ! Size of the kept vectors
      m = .par_b.dim1 ! Note self

      ! Converged?
      .grd_norm_b = q.norm
      if(.is_converged) then
         ! Cleanup
         q.destroy
         return
      end if
   
      ! Workspace
      s.create(n)
      y.create(n)
      a.create(m)
      a = ZERO

      ! The values used for line search algorithms. These values are
      ! the ones which usually work.
      c1 = 10d-4
      c2 = 9d-1

      
      ! Initialises the values used in the two-loop recursion.
      rho = 1
      if (m>1) then   
         s = .par_b(m).element - .par_b(m-1).element
         y = .grd_b(m).element - .grd_b(m-1).element
         gamma = dot_product(s,y)/dot_product(y,y)
      else
         gamma = 1 
      end if
      
      ! Will cause the program to exit if a suitable step can not be
      ! determined 
      alpha = ZERO
      

      ! The two loop recursion process approximates the Hessian by
      ! using stored values of delta x and delta grad, as well as a
      ! pre-scaling, gamma, to allow for a stepping at alpha = 1, to
      ! be suitable 
         
      ! H^0 is approximated by I   
         
         if (m > 1) then
            do j = m,2,-1
               s = .par_b(j).element - .par_b(j-1).element
               y = .grd_b(j).element - .grd_b(j-1).element
               rho = 1/(dot_product(y,s))
               a(j) = rho*dot_product(s,q)
               q = q - a(j)*y
            end do
         end if

         q = gamma*q

         do j = 2,m
            s = .par_b(j).element - .par_b(j-1).element
            y = .grd_b(j).element - .grd_b(j-1).element
            rho = 1/dot_product(y,s)
            beta = rho*dot_product(y,q) 
            q = q + (a(j) - beta)*s
         end do

      ! This is the step toward the minimum
         q = -q

      ! Fines the ideal multiple of the step to take (usually 1)
      ! and stores that in alpha
      call line_search(func,10d1,par,q,c1,c2,alpha)
         
      ! Performs the step (and stores the previous x value)
      par = par + alpha*q
         
      ! Cleanup
      a.destroy     
      y.destroy
      s.destroy
      q.destroy

   end
   
!   minimize_L_BFGS(func,grad,par)
!   ! Use the limited memory Broyden-Fletcher-Goldfarb-Shanno method to
!   ! minimise the molecular orbitals with gradient function from
!   ! Stoll(1980). This code is all per Nocedal & Wright
!   ! NOTE: set saved_self before calling self & dself
!      interface
!         func(x,res)
!            x :: VEC{REAL}, IN
!            res :: REAL, OUT
!         end
!      end
!      interface
!         grad(x,res)
!            x :: VEC{REAL}, IN
!            res :: VEC{REAL}, OUT
!         end
!      end
!      par :: VEC{REAL}, INOUT
!      
!      j,n,m :: INT
!      
!      ! alpha, c1 and c2 are used in the line search algorithm and
!      ! beta, gamma and rho are used in the Hessian approximation
!      alpha,beta,gamma,rho,c1,c2 :: REAL
!      
!      ! a is used to store the various alpha values in the two loop
!      ! recursion, p,q and r are used as generic vectors, and as part
!      ! of the two loop recursion process
!      a,q,s,y :: VEC{REAL}*
!      
!      .iteration = .iteration + 1
!      n =  par.dim1
!      m = .par.dim1 ! Note self
!
!      ! Gradient 
!      q.create(n)
!      call grad(par,q)
!
!      ! Store the parameter & gradient
!      ! NOTE: you should reset if you change
!      ! any of the par vectors outside of here ...
!      .par.append_fifo(par,.keep)
!      .grd.append_fifo(q  ,.keep)
!
!      ! Converged?
!      .grd_norm = q.norm
!      if(.is_converged) then
!         ! Cleanup
!         q.destroy
!         return
!      end if
!   
!      ! Workspace
!      s.create(n)
!      y.create(n)
!      a.create(m)
!      a = ZERO
!
!      ! The values used for line search algorithms. These values are
!      ! the ones which usually work.
!      c1 = 10d-4
!      c2 = 9d-1
!
!      
!      ! Initialises the values used in the two-loop recursion.
!      rho = 1
!      if (m>1) then   
!         s = .par(m).element - .par(m-1).element
!         y = .grd(m).element - .grd(m-1).element
!         gamma = dot_product(s,y)/dot_product(y,y)
!      else
!         gamma = 1 
!      end if
!      
!      ! Will cause the program to exit if a suitable step can not be
!      ! determined 
!      alpha = ZERO
!      
!
!      ! The two loop recursion process approximates the Hessian by
!      ! using stored values of delta x and delta grad, as well as a
!      ! pre-scaling, gamma, to allow for a stepping at alpha = 1, to
!      ! be suitable 
!         
!      ! H^0 is approximated by I   
!         
!         if (m > 1) then
!            do j = m,2,-1
!               s = .par(j).element - .par(j-1).element
!               y = .grd(j).element - .grd(j-1).element
!               rho = 1/(dot_product(y,s))
!               a(j) = rho*dot_product(s,q)
!               q = q - a(j)*y
!            end do
!         end if
!
!         q = gamma*q
!
!         do j = 2,m
!            s = .par(j).element - .par(j-1).element
!            y = .grd(j).element - .grd(j-1).element
!            rho = 1/dot_product(y,s)
!            beta = rho*dot_product(y,q) 
!            q = q + (a(j) - beta)*s
!         end do
!
!      ! This is the step toward the minimum
!         q = -q
!
!      ! Fines the ideal multiple of the step to take (usually 1)
!      ! and stores that in alpha
!      VEC{REAL}::line_search(func,grad,10d0,par,q,c1,c2,alpha)
!         
!      ! Performs the step (and stores the previous x value)
!      par = par + alpha*q
!         
!      ! Cleanup
!      q.destroy
!      s.destroy
!      y.destroy
!      a.destroy     
!
!   end

!   gradient(g,F,D,MO)
   ! Make the real gradient "g" of the *occupied* MO's,
   ! g = F MO S^{-1} - (1/2)S D F MO S^{-1}
!      self :: IN
!      g :: MAT{REAL}, OUT
!      F,D,MO :: MAT{REAL}, IN
!
!   ENSURE(g.is_same_shape_as(MO),"gradient g and MO are not the same shape")
!   ENSURE(F.is_square,"F is not square")
!   ENSURE(F.dim1==.n_bf,"F is not the right size, n_bf = "//trim(.n_bf.to_str))
!   ENSURE(D.is_same_shape_as(F),"D is not the right size")
!   ENSURE( .overlap_matrix.created,"NO overlap_matrix")

!      S_inv, FcSm1,cSm1 :: MAT{REAL}*

      ! Make S, S^{-1} in the MO basis
 !     S_inv.create(.n_a,.n_a)
 !     .BASE:make_ELMO_S_inv_r(S_inv,MO)

      ! Make gradient
  !    FcSm1.create(.n_bf,.n_a)
   !   cSm1.create(.n_bf,.n_a)
!      cSm1.to_product_of(MO,S_inv)
!      FcSm1.to_product_of(F,cSm1)
!      cSm1.destroy
!      cSm1.create(.n_bf,.n_bf)
!      cSm1.to_scaled_product_of(.overlap_matrix,D,fac=HALF)

      ! Assemble 1st and 2nd terms of gradient
!      g = FcSm1
!      g.plus_scaled_product_of(cSm1,FcSm1,fac=-ONE)

      ! Clean up
!      cSm1.destroy
!      FcSm1.destroy
!      S_inv.destroy

!   end

!  ===========
!  Line Search
!  ===========

   line_search(func,alphamax,x,p,c1,c2,b) ::: selfless
   ! Given a real vector, x, function f, gradient function
   ! df, calculaes the ideal stepping scale alpha, given
   ! stepping p and constants c1, c2.
      
   ! Interface for vector functions   
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN
            res  :: REAL, OUT
            g :: VEC{REAL}, OUT
         end
      end
      x :: VEC{REAL}, IN
      p :: VEC{REAL}, IN
      
      ! Wolfe constants c1 and c2 and a highest value of alpha
      c1,c2,alphamax :: REAL, IN
      
      ! The output alpha value
      b :: REAL, OUT

      ! alpha is the current value of alpha. alpha0 is the previous
      ! value. phi0 is the initial phi value, phi1 is the previous phi
      ! value and phi is thge current phi value. The same is true for
      ! dphi0, dphi1 and dphi.
      alpha,alpha0,phi0,phi1,phi,dphi,dphi0,dphi1 :: REAL
      
      ! temp is used in vector dot product calculations. Such as in
      ! phi' (dphi)
      temp :: VEC{REAL}*
      
      ! The maximum (iter) and current (j) number of iterations
      iter,j :: INT
      
      ! Logical for if the loop can conclude
      converged :: BIN

      ! The maximum number of iterations that may be performed
      iter = 5
      
      ! Allows temp to be used for calculations
      temp.create(size(x))

      ! Gives the first value of alpha
      alpha = rand(1)*alphamax 
      
      ! Initialise alpha0, phi0, dphi0, phi1, j and converged
      alpha0 = 0
      call func(x,phi0,temp)
      dphi0 = dot_product(temp,p)
      phi1=0
      j = 1
      converged = FALSE 

      ! Performs the loop until it has converged or the loop has
      ! failed
      do while ((converged EQV FALSE) AND (j < iter)) 
         
      ! calculates phi for this iteration
         call func(x+alpha*p,phi,temp)

      ! The interval alpha0, alpha will contain ideal step lengths if
      ! any of the if conditionals below are violated
         if (phi > phi0 + c1*alpha*dphi0) then
      
      ! Sends the interval off to find an ideal step and stores it in
      ! b
            call zoom(func,alpha0,alpha,x,p,c1,c2,b,converged)
         else if (phi >= phi1 AND j > 1) then
      
      ! Sends the interval off to find an ideal step and stores it in
      ! b
            call zoom(func,alpha0,alpha,x,p,c1,c2,b,converged)
         end if
      
      ! Ensures that if alpha has converged, the value does not
      ! change   
         if (converged EQV FALSE) then
      ! calculates dphi for this iteration      
            dphi = dot_product(temp,p)
      ! If the second wolfe condition holds, this has converged      
            if (abs(dphi) <= -c2*dphi0) then
               converged = TRUE
      ! Stores the ideal stepping in b         
               b = alpha
            else if (dphi >= 0) then
      ! Sends the interval off to find an ideal step and stores it in
      ! b
               call zoom(func,alpha,alpha0,x,p,c1,c2,b,converged)
            end if
      ! Ensures that if alpha has converged, the value does not
      ! change   
            if (converged EQV FALSE) then
      ! Stores the values from this iteration, and proceeds to the
      ! next iteration         
               phi1 = phi
               dphi1 = dphi
               j = j + 1
               alpha0 = alpha
               alpha = alpha + rand(1)*alphamax
            end if
         end if
     end
     
     ! Cleanup
     temp.destroy
   end

   zoom(func,low,high,x,p,c1,c2,alpha,converged) ::: selfless 
   ! Given a real vector, x, function f, gradient function
   ! df, calculaes the ideal stepping scale alpha, given
   ! stepping p and constants c1, c2. An ideal stepping must also
   ! reside between low and high.
      
      ! Interface for vector functions   
      interface
         func(x,res,g)
            x :: VEC{REAL}, IN
            res  :: REAL, OUT
            g :: VEC{REAL}, OUT
         end
      end
      ! Interval in which ideal step resides, low and high and Wolfe
      ! constants c1 and c2
      low,high,c1,c2 :: REAL, IN
      x,p :: VEC{REAL}, IN
      
      ! Ideal stepping scale
      alpha :: REAL, OUT

      ! Logical which describes whether or not it has converged to an
      ! ideal step
      converged :: BIN, OUT
      
      ! Vector used in various multiplications
      temp :: VEC{REAL}*
      
      ! Flexible interval alphalo and alphahi. Initial value of phi
      ! and dphi, phi0 and dphi0. Current values at phi and dphi. phi
      ! value at alpha lo is at philo.
      alphalo,alphahi,phi0,dphi,dphi0,phi,philo :: REAL
      
      ! The current (j) and maximum (iter) number of iterations
      j,iter :: INT
      iter = 10 
      
      ! creates temp
      temp.create(size(x))
      
      ! Initialise alphalo, alphahi, phi0, dphi0, converged and j
      alphalo = low
      alphahi = high
      call func(x,phi0,temp)
      dphi0 = dot_product(temp,p)
      converged = FALSE
      j = 0
 
      ! Does the loops until interations maximum is reached or the
      ! ideal stepping is found.
      do while ((converged EQV FALSE) AND (j < iter)) 
      
      ! New value of alpha is determined by bisection
         alpha = (alphalo+alphahi)/2
         
      ! Calculates current value of philo and phi   
         call func(x+alphalo*p,philo,temp)
         call func(x+alpha*p,phi,temp)

      ! If either of these condition are satisfied alphahi becomes
      ! alpha amd the cycle repeats   
         if (phi > phi0+c1*alpha*dphi0) then
            alphahi = alpha
         else if (phi >= philo) then
            alphahi = alpha
         else
     
      ! Calculates the current value of dphi
            dphi = dot_product(temp,p)

      ! Cycle is complete if strong Wolfe condition holds      
            if (abs(dphi) <= -c2*dphi0) then
               converged = TRUE
            end if

      ! Stops pointless calculations if ideal stepping has been found
            if (converged EQV FALSE) then

      ! Ensures alphahi and alphalo are the right way around         
               if (dphi*(alphahi-alphalo) >= ZERO) then
                  alphahi = alphalo
               end if
      ! alphalo becomes alpha         
               alphalo = alpha
            end if
         end if
         j = j + 1
      end
      temp.destroy
      converged = TRUE
   end


!  =======
!  Queries
!  =======

   subspace_saturated result (res) ::: pure
   ! Return TRUE if the subspace is saturated; no more new
   ! vectors can be stored.
      self :: IN
      res :: BIN

      res = .par.dim == .keep

   end

   apply_l_bfgs result (res) ::: pure
   ! Return TRUE if extrapolation must be applied this iteration
      self :: IN
      res :: BIN

      res = .iteration - .start_iteration >= 0

   end

   saved_iteration result (res) ::: pure
   ! Return the actual iteration number since starting to save vectors
      self :: IN
      res :: INT

      res = .iteration - .save_iteration

   end

!  =============
!  Extrapolation
!  =============

   is_converged result (res) ::: PURE
   ! Return TRUE if the procedure is converged
      self :: IN
      res :: BIN

      res = .grd_norm<.convergence_tolerance

   end

!  ======
!  Output
!  ======

   put
   ! Print out info
      self :: IN

      stdout.flush
      stdout.text("L_BFGS options: ")
      stdout.flush
      stdout.set_real_style("e")
      stdout.show("Convergence on max(abs(error)) =",.convergence_tolerance)
      stdout.set_real_style("f")
      stdout.show("Extrapolate  from iteration    =",.start_iteration)
      stdout.show("Save vectors from iteration    =",.save_iteration)
      stdout.show("No. of vectors to keep         =",.keep)

   end

end
