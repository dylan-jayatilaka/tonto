!-------------------------------------------------------------------------------
!
! SYSTEM:
!
! This is the second most basic module in the Tonto system. It depends only on
! the TYPES module.
!
! The SYSTEM module contains all functions which are deemed to provided by the
! "system" i.e. error messages, graceful error termination, memory management,
! call stack management, I/O buffer tracking, timing and profiling, and parallel
! message passing betwen processes -- including reduction, broadcast and
! synchronisation. The latter prallel operations are completely inherited from
! another stand-alone module, PARALLEL. Inheritance is implemented as text
! inclusion, so there is no fortran dependence betwen the SYSTEM and PARALLEL
! module.
!
! Most of the system functionalities are activated by C macros at compile-time.
! So, you can turn them off to obtain a lean executable for production. You can
! see all the macros, which call routines in this module, in the "macros" file.
! The normal "make" process produces pure Fortran where all the C macros have
! been expanded.
!
! An .error_status is simply an integer, the value is set to 1 if the program
! terminates, or -1 if a warning condition is encountered. Use the
! -DUSE_ERROR_MANAGEMENT macro at compile time. Use
! -DUSE_PRE_AND_POST_CONDITIONS to turn on stricted error checking at the start
! and sometimes at the end of every routine.
!
! The memory management part stores the total and maximum memory used (in
! bytes), and the the total and maximum number of blocks of memory allocated.  A
! memory limit is also stored.  It is a fatal error to use more than the
! allocated limit.
!
! The call stack management keeps track of the current call stack, which makes
! tracking down errors easier. Use the -DUSE_CALL_STACK_MANAGEMENT macro to
! activate memory management and call stack management together.
!
! The I/O part contains the file name and record number of the last acessed
! file, in the event that this file should cause an error, the exact position
! will be known. This feature is always on, but you get more detailed feedback
! if using call stack management.
!
! The timing part provides methods to profile and time routines in the program,
! and to print out the results acording the the call stack level. This part is
! currently limited by the clock size set by fortran, but it may be useful for
! rudimentary profiling. This is activated by -DUSE_TIME_PROFILING.
!
! The parallel part keeps track of the number of processors, the processor rank,
! and uses a model where only the rank 0 processor performs I/O. Currently the
! implementation is tied to MPI, so activate using -DMPI. Appropriate MPI
! libraries should be available, and supplied in the platform speicific file,
! and you should also provide MPI.mod file.
!
! A standard system object, "tonto", is provided to hold all this system
! information in the current program. It should not be neccesary to create any
! other system objects (except for the stdin, stdou, and stdtim objects).
!
! Copyright (C) Dylan Jayatilaka, 1998
!
! This library is free software; you can redistribute it and/or
! modify it under the terms of the GNU Library General Public
! License as published by the Free Software Foundation; either
! version 2 of the License, or (at your option) any later version.
!
! This library is distributed in the hope that it will be useful,
! but WITHOUT ANY WARRANTY; without even the implied warranty of
! MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
! Library General Public License for more details.
!
! You should have received a copy of the GNU Library General Public
! License along with this library; if not, write to the
! Free Software Foundation, Inc., 59 Temple Place - Suite 330,
! Boston, MA  02111-1307, USA.
!
!-------------------------------------------------------------------------------

module SYSTEM

! NOTE: Below, typing capital "USE" (rather than lower cased "use")
! means that the modules are not listed as dependencies in the Makefile.

#ifdef MPI
!  Note the capital USE, below which prevents the 
!  preprocessor from making the module a dependent

!  MPI datatypes  
   USE mpi, only: MPI_CHARACTER
   USE mpi, only: MPI_LOGICAL
   USE mpi, only: MPI_INTEGER
   USE mpi, only: MPI_ADDRESS_KIND
   USE mpi, only: MPI_DOUBLE_PRECISION
   USE mpi, only: MPI_DOUBLE_COMPLEX
! MPI communicator routines  
   USE mpi, only: MPI_COMM_WORLD
   USE mpi, only: MPI_COMM_DUP
   USE mpi, only: MPI_COMM_SIZE
   USE mpi, only: MPI_COMM_RANK
! MPI gather functions   
   USE mpi, only: MPI_SUM
   USE mpi, only: MPI_MAX
   USE mpi, only: MPI_MIN
   USE mpi, only: MPI_SUM
   USE mpi, only: MPI_PROD
   USE mpi, only: MPI_LAND
   USE mpi, only: MPI_LOR
   USE mpi, only: MPI_LXOR
   USE mpi, only: MPI_MAXLOC
   USE mpi, only: MPI_MINLOC
! MPI status   
   USE mpi, only: MPI_PROBE
   USE mpi, only: MPI_IPROBE
   USE mpi, only: MPI_TEST
   USE mpi, only: MPI_TESTANY
   USE mpi, only: MPI_TEST_CANCELLED
   USE mpi, only: MPI_STATUS_SIZE
! MPI initialise and finalise   
   USE mpi, only: MPI_INIT
! MPI Barrier   
   USE mpi, only: MPI_BARRIER
   USE mpi, only: MPI_IBARRIER
   USE mpi, only: MPI_WAIT
   USE mpi, only: MPI_WAITANY
!   USE mpi, only: MPI_WAITALL
#ifndef _WIN32
! MPI initialise and finalise   
   USE mpi, only: MPI_FINALIZE
! MPI gather functions   
   USE mpi, only: MPI_ALLREDUCE
! MPI gather functions   
   USE mpi, only: MPI_BCAST
   USE mpi, only: MPI_IBCAST
   USE mpi, only: MPI_GATHER
   USE mpi, only: MPI_IGATHER
   USE mpi, only: MPI_ALLGATHER
   USE mpi, only: MPI_IALLGATHER
   USE mpi, only: MPI_ALLTOALL
   USE mpi, only: MPI_IALLTOALL
! MPI scatter functions   
   USE mpi, only: MPI_SCATTER
   USE mpi, only: MPI_ISCATTER
! MPI Send and receives   
   USE mpi, only: MPI_SEND
   USE mpi, only: MPI_RECV
   USE mpi, only: MPI_ISEND
   USE mpi, only: MPI_IRECV
   USE mpi, only: MPI_SSEND
   USE mpi, only: MPI_ISSEND
   USE mpi, only: MPI_SENDRECV
   USE mpi, only: MPI_WIN_CREATE
   USE mpi, only: MPI_WIN_FREE
   USE mpi, only: MPI_PUT
   USE mpi, only: MPI_GET
   USE mpi, only: MPI_ACCUMULATE
#endif
#endif

#ifdef INTEL_ifort
   USE ifport, only: flush
#endif

#ifdef SERVICE_ROUTINES
   USE service_routines, only: flush
#endif

#ifdef F90_UNIX_IO
   USE f90_unix_io, only: flush
#endif

   implicit none

   ! Tonto system variable
   tonto :: SYSTEM*, public
   tonto_bug :: SYSTEM*  DEFAULT_NULL

contains

!  ==========
!  Allocation
!  ==========

   create ::: leaky
   ! Create the system object
      self :: PTR

      allocate(self)

      .initialize

   end

   destroy ::: leaky
   !  Destroy the memory manager object
      self :: PTR

      if (.disassociated) return

      .finalize

      deallocate(self)

   end

!  ===========
!  Set methods
!  ===========

   set_defaults ::: PURE
   !  Set defaults
      self :: INOUT

      .error_status    = 0
      .warnings_issued = FALSE
      .stderr_unit     = TEXTFILE_STDERR_UNIT
      .stdout_unit     = TEXTFILE_STDOUT_UNIT
      .keyword_echo    = SYSTEM_KEYWORD_ECHO

   end

   initialize
   ! Initialise the system object and set defaults
   ! Set the random seed from the date

      .set_defaults
      .initialize_cloned_random_seed
      .parallel_initialize

   end

   initialize_cloned_random_seed
   ! Initialise the random seed. Only for master and broadcast it
   ! so that all processors get the same random numbers.

      seed :: VEC{INT}*
      dt :: VEC{INT}(8)
      n,i,t :: INT

      ! Get the size "n"
      if (.is_master_processor) then
         call random_seed(size=n)
         PARALLEL_BROADCAST(n,0)
      end

      ! Allocate seed array
      allocate(seed(n))

      ! Get random number seed
      if (.is_master_processor) then

         ! Try system time for seed
         call system_clock(count=t)

         ! In case above does not work ...
         if (t==0) then
            call date_and_time(values=dt)
            t =  dt(3) * 24 * 60 * 60 * 60 * 1000 &
               + dt(5) * 60 * 60 * 1000 &
               + dt(6) * 60 * 1000 + dt(7) * 1000 &
               + dt(8)
         end

         ! Set seed array
         i = 0
         seed = t + 37 * [ (i, i=0,n-1) ]

         ! Broadcast seed
         PARALLEL_BROADCAST(seed,0)

      end

      ! Get the random numbers from same seed
      ! NOTE: cloned for each processor!
      call random_seed(put=seed)

      ! Clean
      deallocate(seed)

   end

   finalize
   !  Finalise the system object
      self :: INOUT

      .parallel_finalize

   end

!  ================
!  Error operations
!  ================

   set_stderr_unit(n) ::: pure
   ! Set the error unit number "n"
      self :: INOUT
      n :: INT, IN

      .stderr_unit = n

   end

   set_stderr_file(file) ::: pure
   ! Set the error output file to "file"
      self :: INOUT
      file :: TEXTFILE, IN

      .stderr_unit = file.unit

   end

   set_stdout_unit(n) ::: pure
   ! Set the output unit number "n"
      self :: INOUT
      n :: INT, IN

      .stdout_unit = n

   end

   set_stdout_file(file) ::: pure
   ! Set the output file to "file"
      self :: INOUT
      file :: TEXTFILE, IN

      .stdout_unit = file.unit

   end

   set_keyword_echo(echo) ::: pure
   ! Set whether to echo user-input keywords
      self :: INOUT
      echo :: BIN, IN

      .keyword_echo = echo

   end

!  ==============
!  Error messages
!  ==============

   die(message)
   ! Set the error flag to 1 and terminate the program with a message
      self :: INOUT
      message :: STR, IN

      stdout_open :: BIN

      .error_status = 1

      if (.IO_is_allowed) then
         write(.stderr_unit,*)
         write(.stderr_unit,"(a)") &
          "Error in "// trim(message) ! message should include the routine name via foo
         inquire(unit=.stdout_unit,opened=stdout_open)
         if (stdout_open) then
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,*)
         write(.stdout_unit,"(a)") &
          "Error in "// trim(message) ! message should include the routine name via foo
         end
         end
      end

      .report_io_file_info
      .report_keyword_info
#ifdef MPI
      .parallel_finalize
#endif

#ifdef DEBUG
      deallocate(tonto_bug)
#endif

      stop

   end

   die_if(condition,message) ::: public
   ! Set the error flag to 1 and terminate the program with a message
   ! provided "condition" is TRUE
      condition :: BIN
      message :: STR
      if (condition) .die(message)
   end

   warn(message,iostat) ::: public
   ! Set the error flag to -1 and issue a warning message.
      message :: STR
      iostat :: INT, IN, optional

      stdout_open :: BIN

      .error_status = -1

      if (.IO_is_allowed) then
         write(.stderr_unit,"(' ')")
         write(.stderr_unit,"(a)") &
         "Warning from "// trim(message) ! message should include the routine name
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,"(' ')")
         write(.stdout_unit,"(a)") &
         "Warning from "// trim(message) ! message should include the routine name
         end
      end

      if (present(iostat)) then

         if (.IO_is_allowed) then
            write(.stderr_unit,"(' ')")
            write(.stderr_unit,"(a,i4)") "Fortran error ",iostat
            inquire(unit=.stdout_unit,opened=stdout_open)
            if (stdout_open) then
            if (.stderr_unit/=.stdout_unit) then
            write(.stdout_unit,"(' ')")
            write(.stdout_unit,"(a,i4)") "Fortran error ",iostat
            end
            end
         end

      end

      .flush_buffer
      .warnings_issued = TRUE

   end

   warn_if(condition,message,iostat)
   ! If "condition" is true, issue a warning and continue, but set the error
   ! flag to -1 and
      condition :: BIN, IN
      message :: STR, IN
      iostat :: INT, IN, optional
      if (condition) .warn(message,iostat)
   end

   ensure(condition,message)
   ! Ensure "condition" is true, otherwise set the error flag to 1 and
   ! terminate the program with a "message"
      self :: INOUT
      condition :: BIN, IN
      message :: STR, IN

      if (NOT condition) .die(message)

   end

!  ===========================================================
!  Unknown keywords: the preprocessor can define the "options"
!  ===========================================================

   unknown(word,name,options)
   ! Set the error flag to 1 and terminate the program with a message
   ! "Unknown option". The list of known keywords is dumped.
      word :: STR, IN
      name :: STR, IN
      options :: VEC{STR}, IN

      stdout_open :: BIN

      .error_status = 1

      if (.IO_is_allowed) then
         write(.stderr_unit,*)
         write(.stderr_unit,"(a)") &
         "Error in "// trim(name) // " ... unknown option: " // trim(word)
         inquire(unit=.stdout_unit,opened=stdout_open)
         if (stdout_open) then
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,*)
         write(.stdout_unit,"(a)") &
         "Error in "// trim(name) // " ... unknown option: " // trim(word)
         end
         end
      end

      .report_io_file_info
      .report_keyword_info(options)
#ifdef MPI
      .parallel_finalize
#endif

#ifdef DEBUG
      deallocate(tonto_bug)
#endif

      stop

   end

   unknown(word,name)
   ! Set the error flag to 1 and terminate the program with a message
   ! "Unknown option". The list of known keywords is dumped.
      word :: STR, IN
      name :: STR, IN

      stdout_open :: BIN

      .error_status = 1

      if (.IO_is_allowed) then
         write(.stderr_unit,*)
         write(.stderr_unit,"(a)") &
         "Error in "// trim(name) // " ... unknown option: " // trim(word)
         inquire(unit=.stdout_unit,opened=stdout_open)
         if (stdout_open) then
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,*)
         write(.stdout_unit,"(a)") &
         "Error in "// trim(name) // " ... unknown option: " // trim(word)
         end
         end
      end

      .report_io_file_info

      if (.known_keywords.associated) .report_keyword_info(.known_keywords)
#ifdef MPI
      .parallel_finalize
    ! call MPI_ABORT(MPI_COMM_WORLD,.error_status,.mpi_status)
#endif

#ifdef DEBUG
      deallocate(tonto_bug)
#endif

      stop

   end

   report_io_file_info ::: private
   ! Report info about the most recent open file.

      cursor :: STR(len=BSTR_SIZE)
      item_end :: INT
      io_file_open,stdout_open :: BIN

      if (associated(.io_file)) then
      inquire(unit=.io_file.unit,opened=io_file_open)
      if (io_file_open) then

      item_end = max(1,.io_file.buffer.item_end)
      if (item_end>0) cursor = repeat("-",item_end-1)//"^"

      if (.IO_is_allowed) then
         write(.stderr_unit,*)
         write(.stderr_unit,"('File name   = ',a)")  trim(.io_file.name)
         write(.stderr_unit,"('Line number = ',i4)")      .io_file.record
         write(.stderr_unit,"('File buffer = ',a)")  trim(.io_file.buffer.string)
         inquire(unit=.stdout_unit,opened=stdout_open)
         if (stdout_open) then
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,*)
         write(.stdout_unit,"('File name   = ',a)")  trim(.io_file.name)
         write(.stdout_unit,"('Line number = ',i4)")      .io_file.record
         write(.stdout_unit,"('File buffer = ',a)")  trim(.io_file.buffer.string)
         end
         end

         if (item_end>0) then
         write(.stderr_unit,"('Cursor -------',a)")  trim(cursor)
         if (stdout_open) then
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,"('Cursor -------',a)")  trim(cursor)
         end
         end
         end
      end

      end
      end

      .flush_buffer

   end

   report_keyword_info(options) ::: private
   ! Report info about the most recent keywords used
      options :: VEC{STR}, optional

      n :: INT
      stdout_open :: BIN

      if (present(options)) then

         if (.IO_is_allowed) then

            write(.stderr_unit,*)
            write(.stderr_unit,"('Allowed keyword options:')")
            write(.stderr_unit,*)
            do n = 1,size(options)
            write(.stderr_unit,"('   ',a)") trim(options(n))
            end

            inquire(unit=.stdout_unit,opened=stdout_open)
            if (stdout_open) then
            if (.stderr_unit/=.stdout_unit) then
            write(.stdout_unit,*)
            write(.stdout_unit,"('Allowed keyword options:')")
            write(.stdout_unit,*)
            do n = 1,size(options)
            write(.stdout_unit,"('   ',a)") trim(options(n))
            end
            end
            end

         end

      end

      .flush_buffer

   end

   flush_buffer(unit)
   ! Flush the output
      unit :: INT, IN, optional

      f_unit :: INT

      if (present(unit)) then
         f_unit = unit
      else
         f_unit = tonto.stderr_unit
      end

      if (.IO_is_allowed) then
#ifdef FLUSH
       call flush(f_unit)
#endif
      end

   end

! =====================================
! These are all inherited from PARALLEL
! =====================================

   parallel_initialize ::: get_from(PARALLEL:initialize)
   ! Initialise the parallel environment.
   end

   parallel_finalize ::: get_from(PARALLEL:finalize)
   ! Finalise the parallel environment.
   end

! Inquiry routines.

   is_master_processor result (res) ::: get_from(PARALLEL), pure
   ! Return TRUE if this is the master processor. The index of the master
   ! processor is normally 0.
   end

   master_processor result (res) ::: get_from(PARALLEL), pure
   ! Return the index of the master processor, which is normally 0.
   end

   this_processor result (res) ::: get_from(PARALLEL), pure
   ! Return the index of this processor.  First index is zero!
   end

! Parallel do loops

   parallel_do_start(first,stride) result (res) ::: get_from(PARALLEL), pure
   ! Return the starting index to be used in a parallel do statement. The
   ! "first" index and the loop "stride" can optionally be supplied, if they are
   ! not equal to 1.  In this model, each processor skips through the whole
   ! length of the loop.  This should be load balanced --- assuming there is no
   ! systematic dependence in the work for each element of the loop which
   ! depends on a multiple of the number of processors. See the "do_stride"
   ! routine.
   end

   parallel_do_stride(stride) result (res) ::: get_from(PARALLEL), pure
   ! Return the stride to be used in a parallel do statement.  The "stride"
   ! length can be optionally supplied, if it is not 1.
   end

   do_in_parallel result (res) ::: get_from(PARALLEL), pure
   ! Return TRUE if we are allowed to start to implement a given do-loop in
   ! parallel. NOTE: Once you have parallelised a loop, remember to prevent
   ! further parallelisation, using the ".lock_parallel_do" method. This is
   ! typically used as the first statement after the parallel do loop is
   ! entered.
   end

   lock_parallel_do(name) ::: get_from(PARALLEL), pure
   ! Sets the parallel do-loop lock to the "name" of the locking routine.
   ! Only the routine with the same "name" may unlock the loop. WARNING: This
   ! assumes that the names of the routines are all distinct. NOTE: It is
   ! currently an error if the routine is recursive. Nested parallel loops are
   ! OK but disabled.
   end

   unlock_parallel_do(name) ::: get_from(PARALLEL), pure
   ! Allow parallelisation again, if the name matches the original.
   end

! Summation routines.

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>BIN, Y=>MPI_LOGICAL)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>INT, Y=>MPI_INTEGER)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>BIN, YY=>MPI_LOGICAL)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>BIN, Y=>MPI_LOGICAL)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>INT, Y=>MPI_INTEGER)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors.
   end

   parallel_symmetric_sum(mat) ::: get_from(PARALLEL)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors. The matrix "mat" is assumed to be symmetric, and
   ! only the lower half of "mat" is summed; the upper triangle is forced to be
   ! the same as the lower triangle.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 3d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 3d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 4d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 4d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 5d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 5d "mat" from all processors, and gives
   ! the result to all processors.
   end

   compress_to_triangle(tr,mat) ::: get_from(PARALLEL)
   ! Converts the lower triangle of matrix "mat" to the triangle "tr".
   ! using row order.
   end

   uncompress_from_triangle(tr,mat) ::: get_from(PARALLEL)
   ! Converts the triangle "tr" into the symmetric matrix "mat".
   end

! ==============
! Logical Reduce
! ==============
   
   parallel_or(bin) ::: get_from(PARALLEL)
   ! This routine collects binary statements, and returns true if one
   ! binary statements is true.
   end

! =======
! Barrier
! =======

   barrier ::: get_from(PARALLEL) 
   ! Blocks until all processes in the communicator have reached this routine.
   end

   ibarrier(request) ::: get_from(PARALLEL) 
   ! Notifies the process that it has reached the barrier and returns immediately
   end

! ====
! Wait
! ====

   parallel_wait(request) ::: get_from(PARALLEL) 
   ! Waits for an MPI request to complete
   end

   waitall(count,array_of_requests) ::: get_from(PARALLEL) 
   ! Waits for all given MPI Requests to complete
   end

   waitany(count,array_of_requests,indx) ::: get_from(PARALLEL) 
   ! Waits for any specified MPI Request to complete
   end

! =====
! Probe
! =====

   probe(source,tag,status) ::: get_from(PARALLEL) 
   ! Blocking test for a message
   end

   iprobe(source,tag,flag,status) ::: get_from(PARALLEL)
   ! Nonblocking test for a message
   end

! ====
! Test
! ====

   test(request,flag) ::: get_from(PARALLEL)
   ! Tests for the completion of a request
   end

   testany(count,array_of_requests,indx,flag) ::: get_from(PARALLEL)
   ! Tests for the completion of any previously initiated array_of_requests
   end

   test_cancelled(status,flag) ::: get_from(PARALLEL) 
   ! Tests to see if a request was cancelled
   end

! ======
! Cancel
! ======

   cancel(request) ::: get_from(PARALLEL) 
   ! Cancels a communication request
   end

! =============
! Blocking send
! =============

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end


   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Performs a blocking send
   end

   send(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Performs a blocking send
   end

! ================
! Blocking receive
! ================

   recv(buf,source,tag) ::: template
   ! Blocking receive for a message
      self :: INOUT
      buf :: VAR_TYPE, OUT
      source :: INT, IN
      tag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc,mtag :: INT

      if (FALSE) self = self

      mtag = 1
      proc = source
      .mpi_status = 0
#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_Recv(buf,LEN,MPI_TYPE,proc,mtag,.mpi_comm,.mpi_status)
#else
      DIE("wtf?")
      proc = proc
#endif

   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end


   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking receive for a message
   end

   recv(buf,source,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking receive for a message
   end

! =================
! Non-blocking send
! =================

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end


   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking send
   end

   isend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking send
   end

! ====================
! Non-blocking receive
! ====================

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end


   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

   irecv(buf,source,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Begins a nonblocking receive
   end

! =========================
! Blocking synchronous send
! =========================

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end


   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Blocking synchronous send
   end

   ssend(buf,dest,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Blocking synchronous send
   end

! =============================
! Non-blocking synchronous send
! =============================

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buf)*len(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end


   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

   issend(buf,dest,tag,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buf))
   ! Starts a nonblocking synchronous send
   end

! =====================
! Blocking send/receive
! =====================

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN1=>len(sendbuf)), LEN2=>len(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end


   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

   sendrecv(sendbuf,recvbuf,dest,sendtag,source,recvtag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends and receives a message
   end

! ======
! Gather
! ======

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gather together values from a group of processes
   end


   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end


   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end


   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end


   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end


   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

   gather(sendbuf,sendcount,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes
   end

! ===================
! Non-blocking gather
! ===================

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end


   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

   igather(sendbuf,sendcount,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gather together values from a group of processes in a nonblocking way
   end

! =======
! Scatter
! =======

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN1=>len(sendbuf)), LEN2=>len(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end


   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

   scatter(sendbuf,recvbuf,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a communicator
   end

! ====================
! Non-blocking scatter
! ====================

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN1=>len(sendbuf)), LEN2=>len(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(sendbuf)*len(sendbuf)), LEN2=>size(recvbuf)*len(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end


   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

   iscatter(sendbuf,recvbuf,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(sendbuf)), LEN2=>size(recvbuf))
   ! Sends data from one process to all other processes in a
   ! communicator in a nonblocking way
   end

! =========
! Allgather
! =========

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end


   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

   allgather(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to all tasks
   end

! ======================
! Non-blocking Allgather
! ======================

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end


   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end


   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end


   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end


   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end


   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

   iallgather(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Gathers data from all tasks and distribute the combined data to
   ! all tasks in a nonblocking way
   end

! ==========
! All-to-all
! ==========

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end


   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

   alltoall(sendbuf,sendcount,recvbuf) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes
   end

! =======================
! Non-blocking all-to-all
! =======================

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(recvbuf)*len(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end


   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

   ialltoall(sendbuf,sendcount,recvbuf,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(recvbuf))
   ! Sends data from all to all processes in a non-blocking way
   end

! =========
! Broadcast
! =========

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


!   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
!   ! Broadcasts a message from the process with rank "root" to all
!   ! other processes of the communicator
!   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


!   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
!   ! Broadcasts a message from the process with rank "root" to all
!   ! other processes of the communicator
!   end

!   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
!   ! Broadcasts a message from the process with rank "root" to all
!   ! other processes of the communicator
!   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end


!   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
!   ! Broadcasts a message from the process with rank "root" to all
!   ! other processes of the communicator
!   end

!   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
!   ! Broadcasts a message from the process with rank "root" to all
!   ! other processes of the communicator
!   end

!   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
!   ! Broadcasts a message from the process with rank "root" to all
!   ! other processes of the communicator
!   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

   broadcast(buffer,root) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator
   end

! ======================
! Non-blocking Broadcast
! ======================

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(buffer)*len(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end


   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

   ibroadcast(buffer,root,request) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(buffer))
   ! Broadcasts a message from the process with rank "root" to all
   ! other processes of the communicator in a nonblocking way
   end

! =================
! Window Operations
! =================

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>STR, LEN=>len(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, LEN=>1)
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, LEN=>1)
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, LEN=>1)
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, LEN=>1)
   ! Create an MPI Window object for one-sided communication
   end


   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, LEN=>size(base)*len(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end


   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, LEN=>size(base)*len(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end


   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end


   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end


   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

   win_create(base,siz,disp_unit,info,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, LEN=>size(base))
   ! Create an MPI Window object for one-sided communication
   end

! ===========
! Window Free
! ===========

   win_free(win) ::: get_from(PARALLEL)
   ! Free an MPI RMA window
   end

! ==========
! Window Put
! ==========

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Put data into a memory window on a remote process
   end


   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(origin_addr)*len(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end


   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(origin_addr)*len(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end


   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end


   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end


   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

   put(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Put data into a memory window on a remote process
   end

! ==========
! Window Get
! ==========

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Get data from a memory window on a remote process
   end


   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(origin_addr)*len(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end


   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(origin_addr)*len(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end


   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end


   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end


   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

   get(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Get data from a memory window on a remote process
   end

! =================
! Window Accumulate
! =================

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end


   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   accumulate_max(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum into the target process using remote memory access
   end

   
   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end

   accumulate_min(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end


   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   accumulate_sum(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a sum into the target process using remote memory access
   end

   
   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end


   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   accumulate_prod(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a product into the target process using remote memory access
   end

   
   accumulate_land(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Accumulate data using a logical and into the target process using remote memory access
   end


   accumulate_land(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical and into the target process using remote memory access
   end


   accumulate_land(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical and into the target process using remote memory access
   end


   accumulate_land(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical and into the target process using remote memory access
   end


   accumulate_lor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Accumulate data using a logical or into the target process using remote memory access
   end


   accumulate_lor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical or into the target process using remote memory access
   end


   accumulate_lor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical or into the target process using remote memory access
   end


   accumulate_lor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical or into the target process using remote memory access
   end


   accumulate_lxor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Accumulate data using a logical xor into the target process using remote memory access
   end


   accumulate_lxor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical xor into the target process using remote memory access
   end


   accumulate_lxor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical xor into the target process using remote memory access
   end


   accumulate_lxor(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(origin_addr))
   ! Accumulate data using a logical xor into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end

   accumulate_maxloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a maximum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end


   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   accumulate_minloc(origin_addr,target_rank,target_disp,win) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(origin_addr))
   ! Accumulate data using a minimum with location into the target process using remote memory access
   end

   IO_is_allowed result (res)
   ! Return whether or not this processor is allowed to do the I/O operation.
   ! Here, only the master processor does I/O in a parallel job.
      res :: BIN
      res = .is_master_processor OR (NOT .is_parallel)
   end

end
