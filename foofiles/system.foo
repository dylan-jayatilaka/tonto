!-------------------------------------------------------------------------------
!
! SYSTEM:
!
! This is the second most basic module in the Tonto system. It depends only on
! the TYPES module.
!
! The SYSTEM module contains all functions which are deemed to provided by the
! "system" i.e. error messages, graceful error termination, memory management,
! call stack management, I/O buffer tracking, timing and profiling, and parallel
! message passing betwen processes -- including reduction, broadcast and
! synchronisation. The latter prallel operations are completely inherited from
! another stand-alone module, PARALLEL. Inheritance is implemented as text
! inclusion, so there is no fortran dependence betwen the SYSTEM and PARALLEL
! module.
!
! Most of the system functionalities are activated by C macros at compile-time.
! So, you can turn them off to obtain a lean executable for production. You can
! see all the macros, which call routines in this module, in the "macros" file.
! The normal "make" process produces pure Fortran where all the C macros have
! been expanded.
!
! An .error_status is simply an integer, the value is set to 1 if the program
! terminates, or -1 if a warning condition is encountered. Use the
! -DUSE_ERROR_MANAGEMENT macro at compile time. Use
! -DUSE_PRE_AND_POST_CONDITIONS to turn on stricted error checking at the start
! and sometimes at the end of every routine.
!
! The memory management part stores the total and maximum memory used (in
! bytes), and the the total and maximum number of blocks of memory allocated.  A
! memory limit is also stored.  It is a fatal error to use more than the
! allocated limit.
!
! The call stack management keeps track of the current call stack, which makes
! tracking down errors easier. Use the -DUSE_CALL_STACK_MANAGEMENT macro to
! activate memory management and call stack management together.
!
! The I/O part contains the file name and record number of the last acessed
! file, in the event that this file should cause an error, the exact position
! will be known. This feature is always on, but you get more detailed feedback
! if using call stack management.
!
! The timing part provides methods to profile and time routines in the program,
! and to print out the results acording the the call stack level. This part is
! currently limited by the clock size set by fortran, but it may be useful for
! rudimentary profiling. This is activated by -DUSE_TIME_PROFILING.
!
! The parallel part keeps track of the number of processors, the processor rank,
! and uses a model where only the rank 0 processor performs I/O. Currently the
! implementation is tied to MPI, so activate using -DMPI. Appropriate MPI
! libraries should be available, and supplied in the platform speicific file,
! and you should also provide MPI.mod file.
!
! A standard system object, "tonto", is provided to hold all this system
! information in the current program. It should not be neccesary to create any
! other system objects (except for the stdin, stdou, and stdtim objects).
!
! Copyright (C) Dylan Jayatilaka, 1998
!
! This library is free software; you can redistribute it and/or
! modify it under the terms of the GNU Library General Public
! License as published by the Free Software Foundation; either
! version 2 of the License, or (at your option) any later version.
!
! This library is distributed in the hope that it will be useful,
! but WITHOUT ANY WARRANTY; without even the implied warranty of
! MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
! Library General Public License for more details.
!
! You should have received a copy of the GNU Library General Public
! License along with this library; if not, write to the
! Free Software Foundation, Inc., 59 Temple Place - Suite 330,
! Boston, MA  02111-1307, USA.
!
!-------------------------------------------------------------------------------

module SYSTEM

! NOTE: Below, typing capital "USE" (rather than lower cased "use")
! means that the modules are not listed as dependencies in the Makefile.

#ifdef MPI
!  Note the capital USE, below which prevents the 
!  preprocessor from making the module a dependent
   USE mpi, only: MPI_CHARACTER
   USE mpi, only: MPI_LOGICAL
   USE mpi, only: MPI_INTEGER
   USE mpi, only: MPI_DOUBLE_PRECISION
   USE mpi, only: MPI_DOUBLE_COMPLEX
   USE mpi, only: MPI_COMM_WORLD
   USE mpi, only: MPI_SUM
   USE mpi, only: MPI_STATUS_SIZE
   USE mpi, only: MPI_INIT
   USE mpi, only: MPI_COMM_DUP
   USE mpi, only: MPI_COMM_SIZE
   USE mpi, only: MPI_COMM_RANK
   USE mpi, only: MPI_BARRIER
#ifndef _WIN32
   USE mpi, only: MPI_FINALIZE
   USE mpi, only: MPI_ALLREDUCE
   USE mpi, only: MPI_BCAST
   USE mpi, only: MPI_SEND
   USE mpi, only: MPI_RECV
   USE mpi, only: MPI_ISEND
   USE mpi, only: MPI_IRECV
#endif
#endif

#ifdef INTEL_ifort
   USE ifport, only: flush
#endif

#ifdef SERVICE_ROUTINES
   USE service_routines, only: flush
#endif

#ifdef F90_UNIX_IO
   USE f90_unix_io, only: flush
#endif

   implicit none

   ! Tonto system variable
   tonto :: SYSTEM*, public
   tonto_bug :: SYSTEM*  DEFAULT_NULL

contains

!  ==========
!  Allocation
!  ==========

   create ::: leaky
   ! Create the system object
      self :: PTR

      allocate(self)

      .initialize

   end

   destroy ::: leaky
   !  Destroy the memory manager object
      self :: PTR

      if (.disassociated) return

      .finalize

      deallocate(self)

   end

!  ===========
!  Set methods
!  ===========

   set_defaults ::: PURE
   !  Set defaults
      self :: INOUT

      .error_status    = 0
      .warnings_issued = FALSE
      .stderr_unit     = TEXTFILE_STDERR_UNIT
      .stdout_unit     = TEXTFILE_STDOUT_UNIT
      .keyword_echo    = SYSTEM_KEYWORD_ECHO

   end

   initialize
   ! Initialise the system object and set defaults
   ! Set the random seed from the date

      .set_defaults
      .initialize_cloned_random_seed
      .parallel_initialize

   end

   initialize_cloned_random_seed
   ! Initialise the random seed. Only for master and broadcast it
   ! so that all processors get the same random numbers.

      seed :: VEC{INT}*
      dt :: VEC{INT}(8)
      n,i,t :: INT

      ! Get the size "n"
      if (.is_master_processor) then
         call random_seed(size=n)
         PARALLEL_BROADCAST(n,0)
      end

      ! Allocate seed array
      allocate(seed(n))

      ! Get random number seed
      if (.is_master_processor) then

         ! Try system time for seed
         call system_clock(count=t)

         ! In case above does not work ...
         if (t==0) then
            call date_and_time(values=dt)
            t =  dt(3) * 24 * 60 * 60 * 60 * 1000 &
               + dt(5) * 60 * 60 * 1000 &
               + dt(6) * 60 * 1000 + dt(7) * 1000 &
               + dt(8)
         end

         ! Set seed array
         i = 0
         seed = t + 37 * [ (i, i=0,n-1) ]

         ! Broadcast seed
         PARALLEL_BROADCAST(seed,0)

      end

      ! Get the random numbers from same seed
      ! NOTE: cloned for each processor!
      call random_seed(put=seed)

      ! Clean
      deallocate(seed)

   end

   finalize
   !  Finalise the system object
      self :: INOUT

      .parallel_finalize

   end

!  ================
!  Error operations
!  ================

   set_stderr_unit(n) ::: pure
   ! Set the error unit number "n"
      self :: INOUT
      n :: INT, IN

      .stderr_unit = n

   end

   set_stderr_file(file) ::: pure
   ! Set the error output file to "file"
      self :: INOUT
      file :: TEXTFILE, IN

      .stderr_unit = file.unit

   end

   set_stdout_unit(n) ::: pure
   ! Set the output unit number "n"
      self :: INOUT
      n :: INT, IN

      .stdout_unit = n

   end

   set_stdout_file(file) ::: pure
   ! Set the output file to "file"
      self :: INOUT
      file :: TEXTFILE, IN

      .stdout_unit = file.unit

   end

   set_keyword_echo(echo) ::: pure
   ! Set whether to echo user-input keywords
      self :: INOUT
      echo :: BIN, IN

      .keyword_echo = echo

   end

!  ==============
!  Error messages
!  ==============

   die(message)
   ! Set the error flag to 1 and terminate the program with a message
      self :: INOUT
      message :: STR, IN

      stdout_open :: BIN

      .error_status = 1

      if (.IO_is_allowed) then
         write(.stderr_unit,*)
         write(.stderr_unit,"(a)") &
          "Error in "// trim(message) ! message should include the routine name via foo
         inquire(unit=.stdout_unit,opened=stdout_open)
         if (stdout_open) then
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,*)
         write(.stdout_unit,"(a)") &
          "Error in "// trim(message) ! message should include the routine name via foo
         end
         end
      end

      .report_io_file_info
      .report_keyword_info
#ifdef MPI
      .parallel_finalize
#endif

#ifdef DEBUG
      deallocate(tonto_bug)
#endif

      stop

   end

   die_if(condition,message) ::: public
   ! Set the error flag to 1 and terminate the program with a message
   ! provided "condition" is TRUE
      condition :: BIN
      message :: STR
      if (condition) .die(message)
   end

   warn(message,iostat) ::: public
   ! Set the error flag to -1 and issue a warning message.
      message :: STR
      iostat :: INT, IN, optional

      stdout_open :: BIN

      .error_status = -1

      if (.IO_is_allowed) then
         write(.stderr_unit,"(' ')")
         write(.stderr_unit,"(a)") &
         "Warning from "// trim(message) ! message should include the routine name
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,"(' ')")
         write(.stdout_unit,"(a)") &
         "Warning from "// trim(message) ! message should include the routine name
         end
      end

      if (present(iostat)) then

         if (.IO_is_allowed) then
            write(.stderr_unit,"(' ')")
            write(.stderr_unit,"(a,i4)") "Fortran error ",iostat
            inquire(unit=.stdout_unit,opened=stdout_open)
            if (stdout_open) then
            if (.stderr_unit/=.stdout_unit) then
            write(.stdout_unit,"(' ')")
            write(.stdout_unit,"(a,i4)") "Fortran error ",iostat
            end
            end
         end

      end

      .flush_buffer
      .warnings_issued = TRUE

   end

   warn_if(condition,message,iostat)
   ! If "condition" is true, issue a warning and continue, but set the error
   ! flag to -1 and
      condition :: BIN, IN
      message :: STR, IN
      iostat :: INT, IN, optional
      if (condition) .warn(message,iostat)
   end

   ensure(condition,message)
   ! Ensure "condition" is true, otherwise set the error flag to 1 and
   ! terminate the program with a "message"
      self :: INOUT
      condition :: BIN, IN
      message :: STR, IN

      if (NOT condition) .die(message)

   end

!  ===========================================================
!  Unknown keywords: the preprocessor can define the "options"
!  ===========================================================

   unknown(word,name,options)
   ! Set the error flag to 1 and terminate the program with a message
   ! "Unknown option". The list of known keywords is dumped.
      word :: STR, IN
      name :: STR, IN
      options :: VEC{STR}, IN

      stdout_open :: BIN

      .error_status = 1

      if (.IO_is_allowed) then
         write(.stderr_unit,*)
         write(.stderr_unit,"(a)") &
         "Error in "// trim(name) // " ... unknown option: " // trim(word)
         inquire(unit=.stdout_unit,opened=stdout_open)
         if (stdout_open) then
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,*)
         write(.stdout_unit,"(a)") &
         "Error in "// trim(name) // " ... unknown option: " // trim(word)
         end
         end
      end

      .report_io_file_info
      .report_keyword_info(options)
#ifdef MPI
      .parallel_finalize
#endif

#ifdef DEBUG
      deallocate(tonto_bug)
#endif

      stop

   end

   unknown(word,name)
   ! Set the error flag to 1 and terminate the program with a message
   ! "Unknown option". The list of known keywords is dumped.
      word :: STR, IN
      name :: STR, IN

      stdout_open :: BIN

      .error_status = 1

      if (.IO_is_allowed) then
         write(.stderr_unit,*)
         write(.stderr_unit,"(a)") &
         "Error in "// trim(name) // " ... unknown option: " // trim(word)
         inquire(unit=.stdout_unit,opened=stdout_open)
         if (stdout_open) then
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,*)
         write(.stdout_unit,"(a)") &
         "Error in "// trim(name) // " ... unknown option: " // trim(word)
         end
         end
      end

      .report_io_file_info

      if (.known_keywords.associated) .report_keyword_info(.known_keywords)
#ifdef MPI
      .parallel_finalize
    ! call MPI_ABORT(MPI_COMM_WORLD,.error_status,.mpi_status)
#endif

#ifdef DEBUG
      deallocate(tonto_bug)
#endif

      stop

   end

   report_io_file_info ::: private
   ! Report info about the most recent open file.

      cursor :: STR(len=BSTR_SIZE)
      item_end :: INT
      io_file_open,stdout_open :: BIN

      if (associated(.io_file)) then
      inquire(unit=.io_file.unit,opened=io_file_open)
      if (io_file_open) then

      item_end = max(1,.io_file.buffer.item_end)
      if (item_end>0) cursor = repeat("-",item_end-1)//"^"

      if (.IO_is_allowed) then
         write(.stderr_unit,*)
         write(.stderr_unit,"('File name   = ',a)")  trim(.io_file.name)
         write(.stderr_unit,"('Line number = ',i4)")      .io_file.record
         write(.stderr_unit,"('File buffer = ',a)")  trim(.io_file.buffer.string)
         inquire(unit=.stdout_unit,opened=stdout_open)
         if (stdout_open) then
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,*)
         write(.stdout_unit,"('File name   = ',a)")  trim(.io_file.name)
         write(.stdout_unit,"('Line number = ',i4)")      .io_file.record
         write(.stdout_unit,"('File buffer = ',a)")  trim(.io_file.buffer.string)
         end
         end

         if (item_end>0) then
         write(.stderr_unit,"('Cursor -------',a)")  trim(cursor)
         if (stdout_open) then
         if (.stderr_unit/=.stdout_unit) then
         write(.stdout_unit,"('Cursor -------',a)")  trim(cursor)
         end
         end
         end
      end

      end
      end

      .flush_buffer

   end

   report_keyword_info(options) ::: private
   ! Report info about the most recent keywords used
      options :: VEC{STR}, optional

      n :: INT
      stdout_open :: BIN

      if (present(options)) then

         if (.IO_is_allowed) then

            write(.stderr_unit,*)
            write(.stderr_unit,"('Allowed keyword options:')")
            write(.stderr_unit,*)
            do n = 1,size(options)
            write(.stderr_unit,"('   ',a)") trim(options(n))
            end

            inquire(unit=.stdout_unit,opened=stdout_open)
            if (stdout_open) then
            if (.stderr_unit/=.stdout_unit) then
            write(.stdout_unit,*)
            write(.stdout_unit,"('Allowed keyword options:')")
            write(.stdout_unit,*)
            do n = 1,size(options)
            write(.stdout_unit,"('   ',a)") trim(options(n))
            end
            end
            end

         end

      end

      .flush_buffer

   end

   flush_buffer(unit)
   ! Flush the output
      unit :: INT, IN, optional

      f_unit :: INT

      if (present(unit)) then
         f_unit = unit
      else
         f_unit = tonto.stderr_unit
      end

      if (.IO_is_allowed) then
#ifdef FLUSH
       call flush(f_unit)
#endif
      end

   end

! =====================================
! These are all inherited from PARALLEL
! =====================================

   parallel_initialize ::: get_from(PARALLEL:initialize)
   ! Initialise the parallel environment.
   end

   parallel_finalize ::: get_from(PARALLEL:finalize)
   ! Finalise the parallel environment.
   end

! Inquiry routines.

   is_master_processor result (res) ::: get_from(PARALLEL), pure
   ! Return TRUE if this is the master processor. The index of the master
   ! processor is normally 0.
   end

   master_processor result (res) ::: get_from(PARALLEL), pure
   ! Return the index of the master processor, which is normally 0.
   end

   this_processor result (res) ::: get_from(PARALLEL), pure
   ! Return the index of this processor.  First index is zero!
   end

! Parallel do loops

   parallel_do_start(first,stride) result (res) ::: get_from(PARALLEL), pure
   ! Return the starting index to be used in a parallel do statement. The
   ! "first" index and the loop "stride" can optionally be supplied, if they are
   ! not equal to 1.  In this model, each processor skips through the whole
   ! length of the loop.  This should be load balanced --- assuming there is no
   ! systematic dependence in the work for each element of the loop which
   ! depends on a multiple of the number of processors. See the "do_stride"
   ! routine.
   end

   parallel_do_stride(stride) result (res) ::: get_from(PARALLEL), pure
   ! Return the stride to be used in a parallel do statement.  The "stride"
   ! length can be optionally supplied, if it is not 1.
   end

   do_in_parallel result (res) ::: get_from(PARALLEL), pure
   ! Return TRUE if we are allowed to start to implement a given do-loop in
   ! parallel. NOTE: Once you have parallelised a loop, remember to prevent
   ! further parallelisation, using the ".lock_parallel_do" method. This is
   ! typically used as the first statement after the parallel do loop is
   ! entered.
   end

   lock_parallel_do(name) ::: get_from(PARALLEL), pure
   ! Sets the parallel do-loop lock to the "name" of the locking routine.
   ! Only the routine with the same "name" may unlock the loop. WARNING: This
   ! assumes that the names of the routines are all distinct. NOTE: It is
   ! currently an error if the routine is recursive. Nested parallel loops are
   ! OK but disabled.
   end

   unlock_parallel_do(name) ::: get_from(PARALLEL), pure
   ! Allow parallelisation again, if the name matches the original.
   end

! Summation routines.

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>BIN, Y=>MPI_LOGICAL)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>INT, Y=>MPI_INTEGER)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_vector_sum(vec,dim) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "vec" with dimension "dim" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>BIN, YY=>MPI_LOGICAL)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>INT, YY=>MPI_INTEGER)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>REAL, YY=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(val) ::: get_from(PARALLEL, XX=>CPX, YY=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of scalar "val" from all
   ! processors, and gives the result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>BIN, Y=>MPI_LOGICAL)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>INT, Y=>MPI_INTEGER)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(vec) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "vec" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors.
   end

   parallel_symmetric_sum(mat) ::: get_from(PARALLEL)
   ! This routine adds the versions of "mat" from all processors, and gives the
   ! result to all processors. The matrix "mat" is assumed to be symmetric, and
   ! only the lower half of "mat" is summed; the upper triangle is forced to be
   ! the same as the lower triangle.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 3d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 3d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 4d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 4d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>REAL, Y=>MPI_DOUBLE_PRECISION)
   ! This routine adds the versions of a 5d "mat" from all processors, and gives
   ! the result to all processors.
   end

   parallel_sum(mat) ::: get_from(PARALLEL, X=>CPX, Y=>MPI_DOUBLE_COMPLEX)
   ! This routine adds the versions of a 5d "mat" from all processors, and gives
   ! the result to all processors.
   end

   compress_to_triangle(tr,mat) ::: get_from(PARALLEL)
   ! Converts the lower triangle of matrix "mat" to the triangle "tr".
   ! using row order.
   end

   uncompress_from_triangle(tr,mat) ::: get_from(PARALLEL)
   ! Converts the triangle "tr" into the symmetric matrix "mat".
   end

! =======================
! Barrier Synchronization
! =======================

   barrier ::: get_from(PARALLEL) 
   ! Sets up a barrier for some communicator.
   end

   parallel_wait(request) ::: get_from(PARALLEL) 
   ! Stalls a processor until a request is complete.
   end

   waitall(count,requests) ::: template 
   ! Stalls all processors until all requests are complete.
   end

! ==============================================
! Non-blocking send variable to other processors
! ==============================================

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end


   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end


   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end


   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end


   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end


   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

   isend(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronous send variable "var" from "processor" to some other processor.
   end

! ===================================================
! Non-blocking receive variable from other processors
! ===================================================

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Asynchronously receive variable "var" from "processor".
   end


   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end


   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end


   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end


   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end


   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

   irecv(var,processor,tag,req) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Asynchronously receive variable "var" from "processor".
   end

! ==========================================
! Blocking send variable to other processors
! ==========================================

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Send "var" from this processor to some other "processor".
   end


   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end


   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end


   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end


   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end


   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

   send(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Send "var" from this processor to some other "processor".
   end

! ===============================================
! Blocking receive variable from other processors
! ===============================================

   recv(var,processor,tag) ::: template
   ! Receive variable "var" from "processor".
      self :: INOUT
      var :: VAR_TYPE, OUT
      processor :: INT, IN
      tag :: INT, IN, optional


   ENSURE(.is_parallel,"must be in a parallel environment")

      proc,mtag :: INT

      if (FALSE) self = self

      mtag = 1
      proc = processor
      .mpi_status = 0
#ifdef MPI
      if(present(tag)) mtag = tag
      call MPI_Recv(var,LEN,MPI_TYPE,proc,mtag,.mpi_comm,.mpi_status)
#else
      DIE("wtf?")
      proc = proc
#endif

   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Receive variable "var" from "processor".
   end


   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end


   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end


   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end


   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end


   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

   recv(var,processor,tag) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Receive variable "var" from "processor".
   end

! =====================
! Blocking send/receive
! =====================

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN1=>len(vars)), LEN2=>len(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN1=>1, LEN2=>1)
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN1=>1, LEN2=>1)
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>1, LEN2=>1)
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>1, LEN2=>1)
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end


   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(vars)*len(vars)), LEN2=>size(varr)*len(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end


   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN1=>size(vars)*len(vars)), LEN2=>size(varr)*len(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end


   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end


   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end


   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

   sendrecv(vars,varr,processor,tags,processorr,tagr) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN1=>size(vars)), LEN2=>size(varr))
   ! Send a variable to a processor and receives a different variable
   ! form the same or different processor. Note: this only works for
   ! variables of the same type, as astronomical amounts of code 
   ! would be required to do this for the individual types.
   end

! ======================================
! Broadcast variables to all processors.
! ======================================

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>STR, MPI_TYPE=>MPI_CHARACTER, LEN=>len(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>BIN, MPI_TYPE=>MPI_LOGICAL, LEN=>1)
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>INT, MPI_TYPE=>MPI_INTEGER, LEN=>1)
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>REAL, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>1)
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>CPX, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>1)
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end


   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>VEC{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>VEC{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>VEC{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>VEC{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>VEC{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end


   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end


!   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
!   ! Broadcast variable "var" from "processor" to all the others.
!   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT3{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end


!   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
!   ! Broadcast variable "var" from "processor" to all the others.
!   end

!   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
!   ! Broadcast variable "var" from "processor" to all the others.
!   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT4{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end


!   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{STR}, MPI_TYPE=>MPI_CHARACTER, LEN=>size(var)*len(var))
!   ! Broadcast variable "var" from "processor" to all the others.
!   end

!   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{BIN}, MPI_TYPE=>MPI_LOGICAL, LEN=>size(var))
!   ! Broadcast variable "var" from "processor" to all the others.
!   end

!   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{INT}, MPI_TYPE=>MPI_INTEGER, LEN=>size(var))
!   ! Broadcast variable "var" from "processor" to all the others.
!   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{REAL}, MPI_TYPE=>MPI_DOUBLE_PRECISION, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   broadcast(var,processor) ::: get_from(PARALLEL, VAR_TYPE=>MAT5{CPX}, MPI_TYPE=>MPI_DOUBLE_COMPLEX, LEN=>size(var))
   ! Broadcast variable "var" from "processor" to all the others.
      var :: VAR_TYPE
      processor :: INT, IN, optional
   end

   IO_is_allowed result (res)
   ! Return whether or not this processor is allowed to do the I/O operation.
   ! Here, only the master processor does I/O in a parallel job.
      res :: BIN
      res = .is_master_processor OR (NOT .is_parallel)
   end

end
